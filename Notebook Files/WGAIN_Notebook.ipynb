{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "47jQTu_3xYbz",
      "metadata": {
        "id": "47jQTu_3xYbz"
      },
      "source": [
        "# Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dT3IOhs1Kj_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dT3IOhs1Kj_",
        "outputId": "b6e79013-bf0c-4806-ca9d-280dd3f7b831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gSbtK1xE1lFm",
      "metadata": {
        "id": "gSbtK1xE1lFm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models, layers, mixed_precision, regularizers, initializers\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dy2_1b_bqKZ-",
      "metadata": {
        "id": "dy2_1b_bqKZ-"
      },
      "outputs": [],
      "source": [
        "# Seeds for reproduciblity\n",
        "random_seed = 50701\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n",
        "\n",
        "tf.keras.utils.set_random_seed(random_seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "# Defining scaler and imputer to use throughout. Imputation is temporary so  scaling works properly.\n",
        "min_max_scaler = MinMaxScaler()\n",
        "temp_imputer = SimpleImputer(strategy=\"mean\")\n",
        "\n",
        "# Used when making predictions\n",
        "le = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DvZeVTAC1nnt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvZeVTAC1nnt",
        "outputId": "9d388012-c884-44c5-d50a-2afe2fbdea36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow is not using the GPU.\n"
          ]
        }
      ],
      "source": [
        "# Ensuring using best setup with colab\n",
        "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "\n",
        "if len(gpus) > 0:\n",
        "    policy = mixed_precision.Policy(\"mixed_float16\")\n",
        "    mixed_precision.set_global_policy(policy)\n",
        "    GPU = True\n",
        "\n",
        "    print(\"TensorFlow is using the GPU.\")\n",
        "else:\n",
        "    GPU = False\n",
        "    print(\"TensorFlow is not using the GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YMBGYg3E3jmA",
      "metadata": {
        "id": "YMBGYg3E3jmA"
      },
      "source": [
        "# Decide on the Data\n",
        "These can be modified to test 1 specific model which is broken down fully in the\n",
        "following sections.\n",
        "\n",
        "To test all models the section is towards the end under \"Train and Evaluate All\n",
        "Models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8pezPq803mUX",
      "metadata": {
        "id": "8pezPq803mUX"
      },
      "outputs": [],
      "source": [
        "# Select numerical features for imputation to impute\n",
        "numerical_features = [\"mean arterial pressure\", \"heart rate\", \"respiratory rate\", \"PCO2 (Arterial)\",\n",
        "                      \"PO2 (Arterial)\", \"FiO2\", \"arterial pH\", \"sodium\", \"postassium\", \"creatinine\",\n",
        "                      \"hematocrit\", \"white blood cell\", \"HCO3 (serum)\"]\n",
        "\n",
        "# Decide data specifics to test either raw or artificial with different missing mechanisms\n",
        "is_raw_missing = False  # Artificial otherwise\n",
        "missing_type = \"mcar\"  # Artificially only - mcar, mnar_central, mnar_upper, mnar_lower\n",
        "\n",
        "# Depends on missing limits\n",
        "# - Artificial is a percentage missing i.e. 0.2, 0.5 and 0.7\n",
        "# - Raw is number of values missing per row i.e. 2, 5, 10\n",
        "if is_raw_missing:\n",
        "    level_missing = 5\n",
        "else:\n",
        "    level_missing = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jzc8dFGx2Sz1",
      "metadata": {
        "id": "jzc8dFGx2Sz1"
      },
      "source": [
        "# Read and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UjyCeiq76_E1",
      "metadata": {
        "id": "UjyCeiq76_E1"
      },
      "outputs": [],
      "source": [
        "def get_data_and_reference(is_raw_missing=False, level_missing=0.5, missing_type=\"mcar\"):\n",
        "    \"\"\"\n",
        "    Given the specified flags return the specified original data, the numerical features to be\n",
        "    imputed and a reference specifying the data used.\n",
        "\n",
        "    :param is_raw_missing: Boolean flag specifying whether the data is artficially missing or raw.\n",
        "    :param level_missing: A float or integer representing either the percentage of missingness or\n",
        "                          the number of missing values per row.\n",
        "    :param missing_type: A string representing the type of missingness used - only applicable to\n",
        "                         artificially missing data.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if is_raw_missing:\n",
        "        reference = \"raw_{}\".format(level_missing)\n",
        "        df = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/missing/raw/measurements_{}.csv\".format(level_missing))\n",
        "    else:\n",
        "        reference = \"artificial_{}_{}\".format(level_missing, missing_type)\n",
        "        df = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/missing/artificial/measurements_{}_{}.csv\".format(level_missing, missing_type))\n",
        "\n",
        "    # Shuffling the data\n",
        "    df = df.sample(frac=1, random_state=507).reset_index(drop=True)\n",
        "\n",
        "    df_features = df[numerical_features]\n",
        "\n",
        "    return df, df_features, reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xY2wForNvnrx",
      "metadata": {
        "id": "xY2wForNvnrx"
      },
      "outputs": [],
      "source": [
        "def fit_scaler_and_imputer(df_to_fit_to, imputer, scaler):\n",
        "    \"\"\"\n",
        "    Given a dataset containing the training data, the temporary imputer and scaler this will fit\n",
        "    both the imputer and scale on the given data and return them.\n",
        "\n",
        "    :param df_to_fit_to: The dataframe for which the imputer and scaler are to be fitted to.\n",
        "    :param imputer: The intialised imputer.\n",
        "    :param scaler: The initialised scaler.\n",
        "    :return: The fitted imputer and scaler.\n",
        "    \"\"\"\n",
        "    # Fitting imputer on original data\n",
        "    imputer.fit(df_to_fit_to.values)\n",
        "\n",
        "    # Temporarily imputing the data\n",
        "    filled_train = imputer.transform(df_to_fit_to.values)\n",
        "\n",
        "    # Fitting the scaler on the temporary imputation\n",
        "    scaler.fit(filled_train)\n",
        "\n",
        "    return imputer, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pRAfyl_toVo-",
      "metadata": {
        "id": "pRAfyl_toVo-"
      },
      "outputs": [],
      "source": [
        "def scale_data(features_df, imputer, scaler):\n",
        "    \"\"\"\n",
        "    Given the data with missing features this will temporarily fill them through the given imputer\n",
        "    and scale the features.\n",
        "\n",
        "    :param features_df: The dataframe containing the missing features to be scaled.\n",
        "    :param imputer: The fitteed imputer.\n",
        "    :param scaler: The fitted scaler.\n",
        "    :return: The raw data and the scaled features.\n",
        "    \"\"\"\n",
        "    feature_values = features_df.values\n",
        "    missing_mask = features_df.isna()\n",
        "\n",
        "    # Using temporary imputation to fill nan's before scalling\n",
        "    features_temp_filled = imputer.transform(feature_values)\n",
        "\n",
        "    # Scaling the data\n",
        "    featured_scaled_filled = scaler.transform(features_temp_filled)\n",
        "\n",
        "    # Restoring the nans\n",
        "    featured_scaled_filled[missing_mask] = np.nan\n",
        "\n",
        "    scaled_features = pd.DataFrame(featured_scaled_filled, index=features_df.index,\n",
        "                                   columns=features_df.columns)\n",
        "\n",
        "    return feature_values, scaled_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pBseUuHUbSAZ",
      "metadata": {
        "id": "pBseUuHUbSAZ"
      },
      "outputs": [],
      "source": [
        "# Get the specified data\n",
        "shuffled_data, shuffled_features, data_reference = get_data_and_reference(is_raw_missing,\n",
        "                                                                          level_missing,\n",
        "                                                                          missing_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ScRTPYl9pNw-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ScRTPYl9pNw-",
        "outputId": "258146d0-d1a4-4f9c-8174-a11ca490556b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   mean arterial pressure  heart rate  respiratory rate  PCO2 (Arterial)  \\\n",
              "0                0.213720    0.287356          0.085271              NaN   \n",
              "1                     NaN    0.183908               NaN         0.188976   \n",
              "2                     NaN         NaN               NaN         0.173228   \n",
              "3                0.258575         NaN          0.093023              NaN   \n",
              "4                     NaN    0.551724               NaN         0.362205   \n",
              "5                0.284960         NaN          0.147287              NaN   \n",
              "6                     NaN         NaN          0.100775              NaN   \n",
              "7                0.224274         NaN               NaN         0.212598   \n",
              "8                     NaN    0.396552               NaN         0.141732   \n",
              "9                0.271768    0.339080               NaN         0.220472   \n",
              "\n",
              "   PO2 (Arterial)  FiO2  arterial pH    sodium  postassium  creatinine  \\\n",
              "0             NaN   1.0     0.525641  0.363636         NaN    0.041860   \n",
              "1             NaN   1.0          NaN       NaN         NaN    0.037209   \n",
              "2        0.393324   NaN     0.615385       NaN         NaN         NaN   \n",
              "3        0.571843   NaN          NaN       NaN         NaN    0.046512   \n",
              "4        0.044993   1.0          NaN       NaN    0.734177         NaN   \n",
              "5             NaN   1.0          NaN  0.530303    0.468354    0.069767   \n",
              "6        0.336720   NaN          NaN       NaN         NaN         NaN   \n",
              "7             NaN   NaN          NaN  0.469697    0.253165    0.027907   \n",
              "8        0.119013   NaN          NaN       NaN         NaN    0.027907   \n",
              "9             NaN   NaN          NaN  0.439394         NaN    0.027907   \n",
              "\n",
              "   hematocrit  white blood cell  HCO3 (serum)  \n",
              "0    0.453125          0.156395           NaN  \n",
              "1    0.687500               NaN           NaN  \n",
              "2    0.398438          0.069583      0.390244  \n",
              "3    0.367188          0.088138      0.463415  \n",
              "4         NaN          0.237906           NaN  \n",
              "5    0.390625          0.132538           NaN  \n",
              "6    0.406250               NaN           NaN  \n",
              "7         NaN               NaN      0.390244  \n",
              "8    0.445312               NaN           NaN  \n",
              "9         NaN          0.059642      0.463415  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa73db36-c0c6-4494-8b25-419c724311dd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean arterial pressure</th>\n",
              "      <th>heart rate</th>\n",
              "      <th>respiratory rate</th>\n",
              "      <th>PCO2 (Arterial)</th>\n",
              "      <th>PO2 (Arterial)</th>\n",
              "      <th>FiO2</th>\n",
              "      <th>arterial pH</th>\n",
              "      <th>sodium</th>\n",
              "      <th>postassium</th>\n",
              "      <th>creatinine</th>\n",
              "      <th>hematocrit</th>\n",
              "      <th>white blood cell</th>\n",
              "      <th>HCO3 (serum)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.213720</td>\n",
              "      <td>0.287356</td>\n",
              "      <td>0.085271</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.525641</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.041860</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>0.156395</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.183908</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.188976</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.037209</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.173228</td>\n",
              "      <td>0.393324</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.398438</td>\n",
              "      <td>0.069583</td>\n",
              "      <td>0.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.258575</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.093023</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.571843</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.046512</td>\n",
              "      <td>0.367188</td>\n",
              "      <td>0.088138</td>\n",
              "      <td>0.463415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.362205</td>\n",
              "      <td>0.044993</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.734177</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.237906</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.284960</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.147287</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.530303</td>\n",
              "      <td>0.468354</td>\n",
              "      <td>0.069767</td>\n",
              "      <td>0.390625</td>\n",
              "      <td>0.132538</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.100775</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.336720</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.406250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.224274</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.212598</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.469697</td>\n",
              "      <td>0.253165</td>\n",
              "      <td>0.027907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.396552</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.141732</td>\n",
              "      <td>0.119013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.027907</td>\n",
              "      <td>0.445312</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.271768</td>\n",
              "      <td>0.339080</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.220472</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.439394</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.027907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.059642</td>\n",
              "      <td>0.463415</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa73db36-c0c6-4494-8b25-419c724311dd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fa73db36-c0c6-4494-8b25-419c724311dd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fa73db36-c0c6-4494-8b25-419c724311dd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0cfdd160-c030-4c27-8ea3-a72802a3f851\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0cfdd160-c030-4c27-8ea3-a72802a3f851')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0cfdd160-c030-4c27-8ea3-a72802a3f851 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "scaled_featured",
              "summary": "{\n  \"name\": \"scaled_featured\",\n  \"rows\": 8223,\n  \"fields\": [\n    {\n      \"column\": \"mean arterial pressure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11734029718711267,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 244,\n        \"samples\": [\n          0.24010554089709762,\n          0.2453825857519789,\n          0.7651715039577837\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"heart rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11793169498869165,\n        \"min\": 0.0,\n        \"max\": 0.9999999999999999,\n        \"num_unique_values\": 131,\n        \"samples\": [\n          0.16091954022988503,\n          0.21264367816091956,\n          0.20114942528735635\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"respiratory rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.058349112382160985,\n        \"min\": 0.0,\n        \"max\": 0.9999999999999999,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          0.08527131782945736,\n          0.20155038759689925,\n          0.4806201550387597\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PCO2 (Arterial)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08699186516570281,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 92,\n        \"samples\": [\n          0.06299212598425197,\n          0.20472440944881892,\n          0.6692913385826771\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PO2 (Arterial)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18192770814693957,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 506,\n        \"samples\": [\n          0.6748911465892597,\n          0.4716981132075472,\n          0.02612481857764877\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FiO2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24262880046383262,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1.0,\n          0.39759036144578314,\n          0.3473895582329317\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arterial pH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0957208550299168,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 68,\n        \"samples\": [\n          0.2051282051282044,\n          0.5384615384615365,\n          0.6282051282051277\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sodium\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06883046087497355,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          0.3787878787878787,\n          0.18181818181818188,\n          0.6666666666666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"postassium\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09019675990340634,\n        \"min\": 0.0,\n        \"max\": 0.9999999999999999,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          0.7341772151898733,\n          0.3417721518987341,\n          0.6962025316455694\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"creatinine\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06104470391033187,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          0.4744186046511628,\n          0.2186046511627907,\n          0.4279069767441861\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hematocrit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14913276956661334,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 120,\n        \"samples\": [\n          0.203125,\n          0.6796875,\n          0.390625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"white blood cell\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05594082039384032,\n        \"min\": 0.0,\n        \"max\": 0.9999999999999999,\n        \"num_unique_values\": 402,\n        \"samples\": [\n          0.22001325381047046,\n          0.16898608349900596,\n          0.0874751491053678\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HCO3 (serum)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09387909188472057,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 37,\n        \"samples\": [\n          0.7073170731707318,\n          0.5609756097560976,\n          0.41463414634146345\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Scale the data, using placeholder imputationss\n",
        "imputer, scaler = fit_scaler_and_imputer(shuffled_features, temp_imputer, min_max_scaler)\n",
        "shuffled_features, scaled_featured = scale_data(shuffled_features, imputer, scaler)\n",
        "\n",
        "scaled_featured.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ELhaqHSV2VyY",
      "metadata": {
        "id": "ELhaqHSV2VyY"
      },
      "source": [
        "# Using the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UMGOurdoxlzq",
      "metadata": {
        "id": "UMGOurdoxlzq"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L9fqoKMV4b_W",
      "metadata": {
        "id": "L9fqoKMV4b_W"
      },
      "outputs": [],
      "source": [
        "def build_generator(n_features, l2_reg=1e-5, layer_sizes=None, noise_level=10, add_noise=False):\n",
        "    \"\"\"\n",
        "    Used to build a generator for the WGAIN. It takes the number of features that require imputation,\n",
        "    the regularisation rate, the sizes for the layers (A tuple i.e. (256, 128, 64, 32)) and\n",
        "    variables to add noise to the generator to help mitigate mean collapse.\n",
        "\n",
        "    :param n_features: The number of features to be imputed.\n",
        "    :param l2_reg: The regualirsation rate.\n",
        "    :param layer_sizes: The individual layer sizes for the generator.\n",
        "    :param noise_level: The level of noise to be added to the inputs.\n",
        "    :param add_noise: Flag to indicate if noise should be added to the inputs.\n",
        "    :return: The specified generator model.\n",
        "    \"\"\"\n",
        "    # Weight initialiser - need to look into why they used this\n",
        "    xavier_init = initializers.GlorotUniform()\n",
        "\n",
        "    # Setting default layers with most complex architecutre\n",
        "    if layer_sizes is None:\n",
        "        layer_sizes = (32, 64, 128)\n",
        "\n",
        "    # Separating inputs for the missing data and mask\n",
        "    data_input = layers.Input(shape=(n_features,), name=\"missing_data_input\")\n",
        "    mask_input = layers.Input(shape=(n_features,), name=\"missing_mask_input\")\n",
        "    noise_input = layers.Input(shape=(noise_level,), name=\"noise_input\")\n",
        "\n",
        "    # Combining input data\n",
        "    x = layers.Concatenate()([data_input, mask_input, noise_input])\n",
        "\n",
        "    # Building the architecture from the given layer sizes. Noise is optional and an activation is\n",
        "    # always used.\n",
        "    for size in layer_sizes:\n",
        "        x = layers.Dense(units=size, kernel_initializer=xavier_init,\n",
        "                         kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
        "\n",
        "        if add_noise:\n",
        "            x = layers.GaussianNoise(0.05)(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    # Outputting 13 features (imputations) with sigmoid so in range of 0-1 to match min-max scaling\n",
        "    imputation = layers.Dense(n_features, activation=\"sigmoid\", name=\"imputed_data\")(x)\n",
        "\n",
        "    # Built model on the given specification\n",
        "    generator = models.Model(inputs=[data_input, mask_input, noise_input], outputs=imputation, name=\"generator\")\n",
        "\n",
        "    return generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7EoO34qR4t6j",
      "metadata": {
        "id": "7EoO34qR4t6j"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(n_features, l2_reg, layer_sizes=None, dropout=0.2):\n",
        "    \"\"\"\n",
        "    Used to build a discriminator for the WGAIN. It takes the number of features that require imputation,\n",
        "    the regularisation rate, the sizes for the layers (A tuple i.e. (256, 128, 64, 32)) and\n",
        "    variables to add noise to configure dropout layers. They will be applied to all hidden layers\n",
        "    if dropout > 0.\n",
        "\n",
        "    :param: n_features: The number of features to be imputed.\n",
        "    :param: l2_reg: The regularisation rate.\n",
        "    :param layer_sizes: The individual layer sizes for the discrminator.\n",
        "    :param: dropout: The dropout rate (Float) to be applied after each dense layer.\n",
        "    :return: The specified discriminator model.\n",
        "    \"\"\"\n",
        "    # Weight initialiser\n",
        "    xavier_init = initializers.GlorotUniform()\n",
        "\n",
        "    # Setting default layers with most complex architecutre. Default is slightly worse than the\n",
        "    # generator because the discrminator can easily overpower.\n",
        "    if layer_sizes is None:\n",
        "        layer_sizes = (128, 64, 32)\n",
        "\n",
        "    # Defining the inputs to take the imputed data from the generator and a hint matrix.\n",
        "    data_input = layers.Input(shape=(n_features,), name=\"imputed_input\")\n",
        "    hint_input = layers.Input(shape=(n_features,), name=\"hint_input\")\n",
        "\n",
        "    # Combining the data and the hint matrix for the input\n",
        "\n",
        "    x = layers.Concatenate()([data_input, hint_input])\n",
        "\n",
        "    # Building the hidden layers. If dropout specified it is added to every layer.\n",
        "    for size in layer_sizes:\n",
        "        x = layers.Dense(units=size, kernel_initializer=xavier_init,\n",
        "                         kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
        "\n",
        "        if dropout:\n",
        "            x = layers.Dropout(dropout)(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Using None as loss function uses logits - need to understand these more\n",
        "    predictions = layers.Dense(1, activation=None, name=\"predictions\")(x)\n",
        "\n",
        "    # Final discrimnator model to predict whether data is real or fake\n",
        "    discriminator = models.Model(inputs=[data_input, hint_input], outputs=predictions,\n",
        "                                 name=\"discriminator\")\n",
        "\n",
        "    return discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vNYhZhPTHCce",
      "metadata": {
        "id": "vNYhZhPTHCce"
      },
      "outputs": [],
      "source": [
        "def calc_gradient_penalty(discriminator, real_data, fake_data, hint_matrix):\n",
        "    \"\"\"\n",
        "    Penalises gradients of the discrminator that are not equal to 1 to stabilise learning. Used to\n",
        "    enforce the Lipschitz norm requirement. Gradients are calulcated from a new combination of real\n",
        "    and fake data.\n",
        "\n",
        "    :param discriminator: The trained to discrminator to assess.\n",
        "    :param real_data: DataFrame containing only real non-missing or imputed data.\n",
        "    :param fake_data: DataFrame containing the fake imputed data.\n",
        "    :param hint_matrix: Hint matrix for the discrminator (0.1 - 0.9)\n",
        "    :return: The gradient penalty for the discriminator where gradients are not 1.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(real_data)[0]\n",
        "\n",
        "    # New version of the batch with real and fake data combined. Alpha decides on how much is real\n",
        "    # versus how much is fake.\n",
        "    alpha = tf.random.uniform(shape=[batch_size, 1], minval=0, maxval=1)\n",
        "    new_combination = alpha * real_data + (1 - alpha) * fake_data\n",
        "\n",
        "    # Testing the discriminators predictions on the new combination of data\n",
        "    with tf.GradientTape() as gp_tape:\n",
        "        gp_tape.watch(new_combination)\n",
        "        pred = discriminator([new_combination, hint_matrix], training=True)\n",
        "\n",
        "    # Compute gradients of predictions w.r.t the new combination of real and fake data\n",
        "    disc_gradients = gp_tape.gradient(pred, new_combination)\n",
        "    grad_norm = tf.sqrt(tf.reduce_sum(tf.square(disc_gradients), axis=1))\n",
        "\n",
        "    # Applying the penalty for any gradients different than 1 to stabilise learning. Used to enforce\n",
        "    # the Lipshitz norm requirement\n",
        "    gradient_penalty = tf.reduce_mean((grad_norm - 1.0) ** 2)\n",
        "\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GvgNQS4X2aw1",
      "metadata": {
        "id": "GvgNQS4X2aw1"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HZTR9c_yx0hK",
      "metadata": {
        "id": "HZTR9c_yx0hK"
      },
      "source": [
        "### Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YCOexHjP9fES",
      "metadata": {
        "id": "YCOexHjP9fES"
      },
      "outputs": [],
      "source": [
        "def train_step(batch, generator, discriminator, generator_optimizer, discriminator_optimizer,\n",
        "            update_generator=False, p_hint=0.2, noise_level=10, lambda_g_p=10, alpha=10, beta=10):\n",
        "    \"\"\"\n",
        "    Completes a fully training step for the given batch and models. Update generator is used to\n",
        "    decide whether it will be trained this round, p_hint is the probability used in the hint matrix.\n",
        "\n",
        "    :param batch: Batched data to train the models on.\n",
        "    :param generator: Intialised or partially trained generator.\n",
        "    :param discriminator: Initalised or partially trained discriminator.\n",
        "    :param generator_optimizer: The optimiser for the generator.\n",
        "    :param discriminator_optimizer: The optimiser for the discriminator.\n",
        "    :param update_generator: Boolean flag to specify if the generator should be updated this batch.\n",
        "    :param p_hint: THe hint matrix for the discrminator\n",
        "    :param noise_size: The size of the noise used in the training data\n",
        "    :return: overall losses the generator and discriminator with the individual reconstruction and\n",
        "    advasarial losses.\n",
        "    \"\"\"\n",
        "    alpha = tf.cast(alpha, tf.float32)\n",
        "    beta = tf.cast(beta, tf.float32)\n",
        "    batch = tf.cast(batch, tf.float32)\n",
        "    noise_level = tf.cast(noise_level, tf.int32)\n",
        "    batch_size = tf.shape(batch)[0]\n",
        "\n",
        "    # Missing mask: 1 if observed, 0 if missing\n",
        "    missing_mask = tf.where(tf.math.is_nan(batch), tf.zeros_like(batch), tf.ones_like(batch))\n",
        "    missing_mask = tf.cast(missing_mask, tf.float32)\n",
        "\n",
        "    # Feature means to impute missing values temporarily\n",
        "    feature_mean = tf.reduce_sum(tf.where(tf.math.is_nan(batch), tf.zeros_like(batch), batch),\n",
        "                                 axis=0) / tf.reduce_sum(missing_mask, axis=0)\n",
        "\n",
        "    batch_mean_filled = tf.where(tf.math.is_nan(batch), tf.broadcast_to(feature_mean,\n",
        "                                                                        tf.shape(batch)), batch)\n",
        "\n",
        "    # Noise for missing values\n",
        "    noise = tf.random.normal(tf.shape(batch), stddev=1)\n",
        "    noisy_batch = batch_mean_filled * missing_mask + noise * (1 - missing_mask)\n",
        "    noise_input = tf.random.normal(shape=(batch_size, noise_level))\n",
        "\n",
        "    # Create hint matrix for discriminator\n",
        "    B = tf.cast(tf.random.uniform(tf.shape(missing_mask)) < p_hint, tf.float32)\n",
        "    hint_matrix = B * (1 - missing_mask)\n",
        "\n",
        "    # Updating the discriminator every time\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        # Getting imputations from the generator\n",
        "        gen_imputations = generator([noisy_batch, missing_mask, noise_input], training=True)\n",
        "        gen_imputations = tf.cast(gen_imputations, tf.float32)\n",
        "\n",
        "        # Combining the imputed data with the original data\n",
        "        combined_data = tf.where(missing_mask == 1, batch_mean_filled, gen_imputations)\n",
        "        combined_data = tf.cast(combined_data, tf.float32)\n",
        "\n",
        "        # Getting discrminators predcitions on both the real data and the imputed version of the\n",
        "        # dataset.\n",
        "        disc_real = discriminator([batch_mean_filled, hint_matrix], training=True)\n",
        "        disc_fake = discriminator([combined_data, hint_matrix], training=True)\n",
        "\n",
        "        # Finding the overall loss for the predictions themselves\n",
        "        combined_disc_loss = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
        "        combined_disc_loss = tf.cast(combined_disc_loss, tf.float32)\n",
        "\n",
        "\n",
        "        # Adding the gradient penalty the loss\n",
        "        gradient_penalty = calc_gradient_penalty(discriminator, batch_mean_filled, combined_data,\n",
        "                                            hint_matrix)\n",
        "        disc_loss = combined_disc_loss + lambda_g_p * gradient_penalty\n",
        "\n",
        "     # Finding and updating the final gradients of the discriminator\n",
        "    disc_grad = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(disc_grad, discriminator.trainable_variables))\n",
        "\n",
        "    # Only updated every n_crtics per batch\n",
        "    if update_generator:\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            # Getting imputations from the generator\n",
        "            gen_imputations = generator([noisy_batch, missing_mask, noise_input], training=True)\n",
        "            gen_imputations = tf.cast(gen_imputations, tf.float32)\n",
        "\n",
        "            # Combine generated with real data\n",
        "            combined_data = tf.where(missing_mask == 1, batch_mean_filled, gen_imputations)\n",
        "            combined_data = tf.cast(combined_data, tf.float32)\n",
        "\n",
        "            # Discriminator prediction on just the fake data\n",
        "            disc_fake = discriminator([combined_data, hint_matrix], training=True)\n",
        "\n",
        "            # Setting nans to 0 so that the loss can be found properly\n",
        "            batch_zero_filled = tf.where(tf.math.is_nan(batch), tf.zeros_like(batch), batch)\n",
        "\n",
        "            # Getting the adversarial loss (predictions on the imputations) and the reconstruction\n",
        "            # loss (how well it recreated the missing values)\n",
        "            adv_loss = -tf.reduce_mean(disc_fake)\n",
        "            recon_loss = tf.reduce_mean(tf.square(missing_mask * batch_zero_filled -\n",
        "                                                  missing_mask * gen_imputations))\n",
        "\n",
        "            # Casting\n",
        "            adv_loss = tf.cast(adv_loss, tf.float32)\n",
        "            recon_loss = tf.cast(recon_loss, tf.float32)\n",
        "\n",
        "            # Applying mulitpliers to get the final generator loss\n",
        "            gen_loss = (alpha * adv_loss) + (beta * recon_loss)\n",
        "\n",
        "            # FInding and updating the gradient penalties\n",
        "            gen_grad = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "            generator_optimizer.apply_gradients(zip(gen_grad, generator.trainable_variables))\n",
        "    else:\n",
        "        gen_loss, adv_loss, recon_loss = tf.constant(0.0), tf.constant(0.0), tf.constant(0.0)\n",
        "\n",
        "    return gen_loss, disc_loss, adv_loss, recon_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qng19XC3E5W9",
      "metadata": {
        "id": "qng19XC3E5W9"
      },
      "outputs": [],
      "source": [
        "def train(data, n_epochs, generator, discriminator, generator_optimizer, discriminator_optimizer,\n",
        "          n_critic=5, patience=100, batch_size=128, alpha=10, beta=10, noise_level=10,\n",
        "          gen_layers=None, disc_layers=None, record_losses=True, progress_bar=True):\n",
        "    \"\"\"\n",
        "    To be written.\n",
        "\n",
        "    :param data: The training data.\n",
        "    :param n_epochs: The max number of epochs to train for.\n",
        "    :param generator: The initalised generator.\n",
        "    :param discriminator: The intialised discriminator.\n",
        "    :param generator_optimizer: The generator optimiser.\n",
        "    :param discriminator_optimizer: The discrminator optimiser.\n",
        "    :param n_critic: The ratio of training for the discrminator to the generator per batch.\n",
        "    :param patience: The limit for how many epochs can be trained without an improvement in either\n",
        "    the reconstruction loss or the advasarisal loss.\n",
        "    :return: The trained generator and discriminator and if specified their respective losses.\n",
        "    \"\"\"\n",
        "    # Tracking losses\n",
        "    gen_losses, disc_losses = [], []\n",
        "    adv_losses, recon_losses = [], []\n",
        "\n",
        "    best_adv_loss = float(\"inf\")\n",
        "    best_recon_loss = float(\"inf\")\n",
        "\n",
        "    # Initialising the count for the early stopping\n",
        "    wait = 0\n",
        "\n",
        "    # Using tf.function so it runs more efficiently\n",
        "    step_function = tf.function(train_step)\n",
        "\n",
        "    # Training for the specified number of epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_gen_loss, epoch_disc_loss = 0.0, 0.0\n",
        "        epoch_adv_loss, epoch_recon_loss = 0.0, 0.0\n",
        "\n",
        "        batches_per_epoch = tf.data.experimental.cardinality(data).numpy()\n",
        "\n",
        "        # If set to true then a progress bar is used to show the training progress\n",
        "        if progress_bar:\n",
        "            pbar = tqdm(total=batches_per_epoch, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\",\n",
        "                        leave=False)\n",
        "        else:\n",
        "            pbar = None\n",
        "\n",
        "        # Training on the batches\n",
        "        for i, batch in enumerate(data):\n",
        "            # Checking if generator should be updated\n",
        "            update_generator = (i % n_critic == 0)\n",
        "\n",
        "            # Train on the batch\n",
        "            gen_loss, disc_loss, adv_loss, recon_loss = step_function(batch, generator,\n",
        "                                                                      discriminator,\n",
        "                                                                      generator_optimizer,\n",
        "                                                                      discriminator_optimizer,\n",
        "                                                                      update_generator=update_generator,\n",
        "                                                                      alpha=alpha, beta=beta,\n",
        "                                                                      noise_level=noise_level\n",
        "                                                                      )\n",
        "            # Updating the losses for this batch\n",
        "            epoch_gen_loss += gen_loss\n",
        "            epoch_disc_loss += disc_loss\n",
        "            epoch_adv_loss += adv_loss\n",
        "            epoch_recon_loss += recon_loss\n",
        "\n",
        "            if progress_bar:\n",
        "                # Update progress bar after each batch\n",
        "                pbar.update(1)\n",
        "\n",
        "        if progress_bar:\n",
        "            pbar.close()\n",
        "\n",
        "        # Getting averages of each loss and recording them\n",
        "        avg_gen_loss = float(epoch_gen_loss / batches_per_epoch)\n",
        "        avg_disc_loss = float(epoch_disc_loss / batches_per_epoch)\n",
        "        avg_adv_loss = float(epoch_adv_loss / batches_per_epoch)\n",
        "        avg_recon_loss = float(epoch_recon_loss / batches_per_epoch)\n",
        "\n",
        "        gen_losses.append(avg_gen_loss)\n",
        "        disc_losses.append(avg_disc_loss)\n",
        "        adv_losses.append(avg_adv_loss)\n",
        "        recon_losses.append(avg_recon_loss)\n",
        "\n",
        "        if record_losses:\n",
        "            tf.print(\"\\nEpoch {}: Gen: {:.4f}, Disc: {:.4f}, Adv: {:.4f}, Recon: {:.4f}\".format(\n",
        "                epoch+1, avg_gen_loss, avg_disc_loss, avg_adv_loss, avg_recon_loss))\n",
        "\n",
        "        # Flag for early stopping counter\n",
        "        improved = False\n",
        "\n",
        "        # Check if advesarial loss improved\n",
        "        if avg_adv_loss < best_adv_loss:\n",
        "            best_adv_loss = avg_adv_loss\n",
        "            improved = True\n",
        "\n",
        "        # Check if reconstruction loss improved\n",
        "        if avg_recon_loss < best_recon_loss:\n",
        "            best_recon_loss = avg_recon_loss\n",
        "            improved = True\n",
        "\n",
        "        # If an improvement was found then the counter is reset and the best weights updated\n",
        "        if improved:\n",
        "            best_gen_weights = generator.get_weights()\n",
        "            best_disc_weights = discriminator.get_weights()\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping at epoch {}\".format(epoch+1))\n",
        "                generator.set_weights(best_gen_weights)\n",
        "                discriminator.set_weights(best_disc_weights)\n",
        "                break\n",
        "\n",
        "    if record_losses:\n",
        "        return generator, discriminator, gen_losses, disc_losses, adv_losses, recon_losses\n",
        "    else:\n",
        "        return generator, discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lQdj4Y-6qu1l",
      "metadata": {
        "id": "lQdj4Y-6qu1l"
      },
      "outputs": [],
      "source": [
        "def split_data(features_df, batch_size):\n",
        "    \"\"\"\n",
        "    Given the raw training data this function will scale it, shuffle it and split it into the\n",
        "    desired batch sizes (with any remainder dropped).\n",
        "\n",
        "    :param features_df: Dataframe containing the missing features to be imputed.\n",
        "    :param batch_size: Size of the individual batches for the training data.\n",
        "    :return: The shuffled and batched data.\n",
        "    \"\"\"\n",
        "    # Shuffling the data and batching it\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(features_df)\n",
        "    # Using high buffer size as overshooting should not affect performance\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=30000, seed=507).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HY4J4fuQx3XS",
      "metadata": {
        "id": "HY4J4fuQx3XS"
      },
      "source": [
        "### Training and Variable Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gRoNVHsC13Le",
      "metadata": {
        "id": "gRoNVHsC13Le"
      },
      "outputs": [],
      "source": [
        "def initialise_models(n_features, gen_layers, disc_layers, dropout=0.1, l2_reg=1e-5, noise_level=10):\n",
        "    \"\"\"\n",
        "    Given the dimension size, number of features and regularisation rate the models are built and\n",
        "    returned.\n",
        "\n",
        "    :n_features: The number of features to be trained on.\n",
        "    :gen_layers: The individual layer sizes for the generator.\n",
        "    :disc_layers: The individual layer sizes for the discriminator.\n",
        "    :dropout: The dropout rate for the discriminator.\n",
        "    :l2_reg: The regularisation rate applied to either model.\n",
        "    :return: The initialised generator and discriminator.\n",
        "    \"\"\"\n",
        "    # Initialise the models to be trained\n",
        "    generator = build_generator(n_features, l2_reg=l2_reg, layer_sizes=gen_layers,\n",
        "                                noise_level=noise_level)\n",
        "    discriminator = build_discriminator(n_features, l2_reg=l2_reg, layer_sizes=disc_layers,\n",
        "                                        dropout=dropout)\n",
        "\n",
        "    return generator, discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uLVVqMJLjxyh",
      "metadata": {
        "id": "uLVVqMJLjxyh"
      },
      "outputs": [],
      "source": [
        "def initialise_optimisers(gen_learning_rate=1e-3, disc_learning_rate=1e-5):\n",
        "    \"\"\"\n",
        "    Prepares the optimisers for both the generator and discriminator. Both are setup with a learning\n",
        "    schedule with exponential decay. Using Adam optimiser and clipping to prevent extreme gradients.\n",
        "\n",
        "    :param gen_learning_rate: The learning rate for the generator.\n",
        "    :param disc_learning_rate: The learning rate for the discriminator.\n",
        "    :return: The initialised optimisers for the generator and discriminator.\n",
        "    \"\"\"\n",
        "    # Seperate learning schedules for the optimisers. Using decay so as training goes on the\n",
        "    # learning rate will decrease\n",
        "    gen_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=gen_learning_rate,\n",
        "    decay_steps=5000,\n",
        "    decay_rate=0.98)\n",
        "\n",
        "    disc_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=disc_learning_rate,\n",
        "    decay_steps=5000,\n",
        "    decay_rate=0.98)\n",
        "\n",
        "    # Define optimisers, using clipnorm to prevent extreme gradients\n",
        "    generator_optimiser = keras.optimizers.Adam(learning_rate=gen_lr_schedule, clipnorm=1.0, beta_1=0.5, beta_2=0.9)\n",
        "    discriminator_optimiser = keras.optimizers.Adam(learning_rate=disc_lr_schedule, clipnorm=1.0, beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "    return generator_optimiser, discriminator_optimiser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B5cHFrt4xrza",
      "metadata": {
        "id": "B5cHFrt4xrza"
      },
      "source": [
        "### Check Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "747orrr8Dqvy",
      "metadata": {
        "id": "747orrr8Dqvy"
      },
      "outputs": [],
      "source": [
        "def plot_losses(gen_losses, disc_losses, reference=None, show=True):\n",
        "    \"\"\"\n",
        "    Using the losses after training and the reference for saving this will plot the losses over time\n",
        "    of the training.\n",
        "\n",
        "    :param gen_losses: The tracked losses for the generator.\n",
        "    :param disc_losses: THe tracked losses for the discriminator.\n",
        "    :param reference: String representing the experiment reference, used to title the plot.\n",
        "    :param show: Boolean to decide whether plot is shown in environment or saved as always.\n",
        "    \"\"\"\n",
        "    # Converting skipped losses to nan to be avoided in the plot\n",
        "    disc_losses = np.where(np.array(disc_losses) == 0.0, np.nan, disc_losses)\n",
        "\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(10, 18))\n",
        "\n",
        "    # Discriminator loss\n",
        "    axs[0].plot(range(len(disc_losses)), disc_losses, label=\"Discriminator Loss\", color=\"r\",\n",
        "                linestyle=\"-\", marker=\"o\")\n",
        "    axs[0].set_title(\"Discriminator Loss per Epoch\")\n",
        "    axs[0].set_xlabel(\"Epochs\")\n",
        "    axs[0].set_ylabel(\"Loss\")\n",
        "    axs[0].grid(True)\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Total Generator loss\n",
        "    axs[1].plot(range(len(gen_losses)), gen_losses, label=\"Generator Loss\", color=\"g\",\n",
        "                linestyle=\"-\", marker=\"o\")\n",
        "    axs[1].set_title(\"Generator Loss per Epoch\")\n",
        "    axs[1].set_xlabel(\"Epochs\")\n",
        "    axs[1].set_ylabel(\"Loss\")\n",
        "    axs[1].grid(True)\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Visualisations/losses/{}_losses_separate.png\".format(reference))\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9wRxKVD9iLZC",
      "metadata": {
        "id": "9wRxKVD9iLZC"
      },
      "source": [
        "### Saving the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2mUuDLV6iM56",
      "metadata": {
        "id": "2mUuDLV6iM56"
      },
      "outputs": [],
      "source": [
        "def save_and_clear_models(generator, discriminator, reference):\n",
        "    \"\"\"\n",
        "    This saves the trained models under the given reference and clears the keras sesssion to avoid\n",
        "    data leakage.\n",
        "\n",
        "    :param generator: The trained generator.\n",
        "    :param discriminator: The trained discriminator.\n",
        "    :param reference: String representing the experiment reference, used to name the model files.\n",
        "    \"\"\"\n",
        "    # Outputting trained models\n",
        "    generator.save(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Models/{}_generator.keras\".format(reference))\n",
        "    discriminator.save(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Models/{}_discriminator.keras\".format(reference))\n",
        "\n",
        "    # Clearing state for new model\n",
        "    keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y5iWJ6wWH8lK",
      "metadata": {
        "id": "y5iWJ6wWH8lK"
      },
      "source": [
        "# Evaluate the Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3E2WM1OyZVa",
      "metadata": {
        "id": "c3E2WM1OyZVa"
      },
      "source": [
        "## Functions to Test Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oYhVY2UE1437",
      "metadata": {
        "id": "oYhVY2UE1437"
      },
      "outputs": [],
      "source": [
        "def impute_from_model(missing_features, generator, scaler, imputer, noise_level=10):\n",
        "    \"\"\"\n",
        "    Given the data containing missing data and the relevant generator, scaler and imputer this will\n",
        "    return the final imputation from the trained generator. The scaler and imputer should be fitted\n",
        "    to the passed missing data.\n",
        "\n",
        "    The returned data is not complete and requires calling \"combine_imputations_with_categorical\"\n",
        "    for demographic data to be added back.\n",
        "\n",
        "    :param missing_features: Thhe Dataframe containing the missing features to be imputed.\n",
        "    :param generator: The trained generator to impute with.\n",
        "    :param scaler: Scaler to scale the data before imputing.\n",
        "    :param imputer: Imputer for temporary filling before scaling.\n",
        "    :param noise_level: Decides how much is injected into the data.\n",
        "    :return: The imputations from the trained generator.\n",
        "    \"\"\"\n",
        "    # Getting the feature values and the missing mask\n",
        "    feature_values = missing_features.values.astype(np.float32)\n",
        "    missing_mask = (~np.isnan(feature_values)).astype(np.float32)\n",
        "\n",
        "    # Filling and scaling the features to work with gen\n",
        "    features_filled = imputer.transform(feature_values)\n",
        "    features_scaled = scaler.transform(features_filled)\n",
        "\n",
        "    # Add noise where values are missing - this was filled in by colab so might need verifying\n",
        "    std = features_scaled.std(axis=0, keepdims=True)\n",
        "    noise = np.random.normal(0, std, size=features_scaled.shape)\n",
        "    X_noisy = features_scaled * missing_mask + noise * (1 - missing_mask)\n",
        "\n",
        "    noise_input = tf.random.normal(shape=(features_scaled.shape[0], noise_level))\n",
        "\n",
        "    # Using the generator to impute the missing values\n",
        "    feature_imputations_scaled = generator.predict([X_noisy, missing_mask, noise_input])\n",
        "\n",
        "    # Unscale the imputations\n",
        "    feature_predictions_unscaled = scaler.inverse_transform(feature_imputations_scaled)\n",
        "\n",
        "    # Replace the missing values with the imputations\n",
        "    final_imputed_features = np.where(np.isnan(feature_values), feature_predictions_unscaled,\n",
        "                                      feature_values)\n",
        "\n",
        "    df_pred = pd.DataFrame(final_imputed_features, columns=missing_features.columns,\n",
        "                           index=missing_features.index)\n",
        "\n",
        "    return df_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nK4tdfcp96qV",
      "metadata": {
        "id": "nK4tdfcp96qV"
      },
      "outputs": [],
      "source": [
        "def combine_imputations_with_categorical(imputed_data, missing_data, missing_features):\n",
        "    \"\"\"\n",
        "    Given an imputed dataset containg just the imputed features this function will add back the\n",
        "    demographic data and return it.\n",
        "\n",
        "    :param imputed_data: The data which has already been imputed by the generator.\n",
        "    :param missing_data: The original dataset with the missing values and desired columns.\n",
        "    :param missing_features: The ?\n",
        "\n",
        "    :return: The imputed data with the non-feature data returned.\n",
        "    \"\"\"\n",
        "    # Adding the demographic data back\n",
        "    non_feature_cols = missing_data.drop(columns=missing_features.columns)\n",
        "    df_final = pd.concat([non_feature_cols, imputed_data], axis=1)\n",
        "\n",
        "    return df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iXudV12mnOmY",
      "metadata": {
        "id": "iXudV12mnOmY"
      },
      "outputs": [],
      "source": [
        "def plot_imputed_distributions(original_features, imputed_data, reference, show=True):\n",
        "    \"\"\"\n",
        "    This will plot the differences in distributions of the imputed and original datasets. It works\n",
        "    with both raw and artficially missing data and the histograms are normalised.\n",
        "\n",
        "    Still work on colours - very bad\n",
        "\n",
        "    :param original_features: The original dataset with missing values that the imputed data comes\n",
        "                              from\n",
        "    :param imputed_data: The imputed dataset\n",
        "    :param reference:  String representing the experiment reference, used to title the plot.\n",
        "    :param show: Boolean to decide whether plot is shown in environment or saved as always.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(18, 16))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, col in enumerate(original_features):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Density normalises it - look into better colours\n",
        "        ax.hist(imputed_data[col], alpha=0.5, label=\"Imputed\", color=\"blue\", edgecolor=\"black\",\n",
        "                density=True)\n",
        "        ax.hist(original_features[col], alpha=0.5, label=\"Original\", color=\"orange\",\n",
        "                edgecolor=\"black\", density=True)\n",
        "\n",
        "        ax.set_title(col)\n",
        "        ax.tick_params(axis=\"x\")\n",
        "        ax.tick_params(axis=\"y\")\n",
        "        ax.legend()\n",
        "\n",
        "    # Odd number of features so need to remove extra plots\n",
        "    axes[13].axis('off')\n",
        "    axes[14].axis('off')\n",
        "    axes[15].axis('off')\n",
        "\n",
        "    plt.suptitle(\"Variable Distribututions for {}\".format(reference), fontsize=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Visualisations/imputed distributions/{}_variable_distribututions.png\".format(reference))\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y2wqsgGHfkuz",
      "metadata": {
        "id": "Y2wqsgGHfkuz"
      },
      "source": [
        "## Evaluate Artifically Missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yoRuEk6ruuC7",
      "metadata": {
        "id": "yoRuEk6ruuC7"
      },
      "outputs": [],
      "source": [
        "def evaluate_normalised_mae(ground_truth, imputation, missing_data):\n",
        "    \"\"\"\n",
        "    If the imputed data has a ground truth then this will evaluate the imputations through\n",
        "    the normalised mean squared error. It will return the normalised MAE per features and the\n",
        "    average.\n",
        "\n",
        "    :param ground_truth: The ground truth data with no missing values.\n",
        "    :param imputation: The imputed dataset from the artificially missing dataset.\n",
        "    :param missing_data: The original dataset with the missing values.\n",
        "    :return: The normalised MAE per feature and the average as a DataFrame.\n",
        "    \"\"\"\n",
        "    # Tracking the individual values and the results so they can be averaged\n",
        "    norm_mae_values = []\n",
        "    norm_mae_results = {}\n",
        "\n",
        "    # Identify where the data was missing before imputation\n",
        "    missing_mask = missing_data.isna()\n",
        "\n",
        "    for feature in imputation.columns:\n",
        "        if feature not in missing_mask.columns:\n",
        "            continue\n",
        "\n",
        "        # Comparing values that were missing only\n",
        "        missing_ground_truth = ground_truth[feature][missing_mask[feature]]\n",
        "        missing_imputed = imputation[feature][missing_mask[feature]]\n",
        "\n",
        "        # MAE\n",
        "        feature_mae = mean_absolute_error(missing_ground_truth, missing_imputed)\n",
        "\n",
        "        # Normalising through IQR\n",
        "        Q1 = missing_ground_truth.quantile(0.25)\n",
        "        Q3 = missing_ground_truth.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        norm_feature_mae = feature_mae / IQR\n",
        "\n",
        "        norm_mae_results[feature] = norm_feature_mae\n",
        "        norm_mae_values.append(norm_feature_mae)\n",
        "\n",
        "    norm_mae_results[\"average_norm_mae\"] = np.mean(norm_mae_values)\n",
        "\n",
        "    return norm_mae_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W_9kgqS5ueAj",
      "metadata": {
        "id": "W_9kgqS5ueAj"
      },
      "outputs": [],
      "source": [
        "def plot_normalised_mae(norm_mae_results, reference, show=True):\n",
        "    \"\"\"\n",
        "    This plots the normalised MAE for each of the features\n",
        "\n",
        "    :param norm_mae_results: The normalised MAE results from the evaluation.\n",
        "    :param reference: String representing the experiment reference, used to title the plot.\n",
        "    :param show: Boolean to decide whether plot is shown in environment or saved as always.\n",
        "    \"\"\"\n",
        "    features = list(norm_mae_results.keys())\n",
        "    values = list(norm_mae_results.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    bars = plt.barh(features, values, edgecolor=\"black\")\n",
        "    plt.xlabel(\"Normalised Mean Absolute Error\")\n",
        "    plt.title(\"Normalised Mean Absolute Error by Feature for {}\".format(reference))\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    for bar in bars:\n",
        "      width = bar.get_width()\n",
        "      plt.text(width + 0.005, bar.get_y() + bar.get_height() / 2, \"{:.3f}\".format(width))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Visualisations/nmae/{}_feature_norm_mae.png\".format(reference))\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yOhVWizrfncq",
      "metadata": {
        "id": "yOhVWizrfncq"
      },
      "outputs": [],
      "source": [
        "def evaluate_artifically_missing(imputed_data, reference, missing_data, show=True):\n",
        "    \"\"\"\n",
        "    This is a wrapper to read in the ground truth data, evaluate the imputed data through nMAE,\n",
        "    plot the nMAE per feature and return a dataframe containing the results.\n",
        "    \"\"\"\n",
        "    # Reading in the ground truth data\n",
        "    reference_df = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/missing/raw/measurements_0.csv\")\n",
        "\n",
        "    # Getting the nMAE for the imputations\n",
        "    norm_mae_results = evaluate_normalised_mae(reference_df, imputed_data, missing_data)\n",
        "\n",
        "    # Shows the individual nMAE for each feature\n",
        "    plot_normalised_mae(norm_mae_results, reference, show=show)\n",
        "\n",
        "    # Final df to represent the evaluation\n",
        "    results_df = pd.DataFrame(norm_mae_results, index=[0])\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rghX6F9iB9EJ",
      "metadata": {
        "id": "rghX6F9iB9EJ"
      },
      "source": [
        "# Evaluate Real Missing Through Predictive Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SGzaUXkSZZu4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGzaUXkSZZu4",
        "outputId": "95239145-8ed3-4925-91e7-13f4c65c6078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.1)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (25.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.7.0 scikit-optimize-0.10.2\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Js76LVOzZdT_",
      "metadata": {
        "id": "Js76LVOzZdT_"
      },
      "outputs": [],
      "source": [
        "# Importing optimiser package from scikit\n",
        "from skopt import gp_minimize, BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# Used to train model\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Tidy memory\n",
        "import gc\n",
        "\n",
        "# Used for processing\n",
        "from types import SimpleNamespace\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TNdkI3pPNi7t",
      "metadata": {
        "id": "TNdkI3pPNi7t"
      },
      "outputs": [],
      "source": [
        "xgboost_search_space = {\n",
        "    \"gamma\": Categorical([0.01, 0.1]),\n",
        "    \"learning_rate\": Categorical([0.001, 0.01, 0.1]),\n",
        "    \"max_depth\": Categorical([3, 6, 9]),\n",
        "    \"n_estimators\": Categorical([100, 200, 300])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UnN04tzjWP-L",
      "metadata": {
        "id": "UnN04tzjWP-L"
      },
      "outputs": [],
      "source": [
        "def data_setup(score_data):\n",
        "    \"\"\"\n",
        "    Split the data into training and test data for both the features and predicted values. Using\n",
        "    stratified sampling to get even class distributions.\n",
        "\n",
        "    :param score_data: The data to be split\n",
        "    :return: X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    # Splitting into features and target variables\n",
        "    X = score_data[numerical_features].copy()\n",
        "    y = score_data[\"outcome_encoded\"].copy()\n",
        "\n",
        "    # Splitting into training and test data, stratifying due to limited data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=507,\n",
        "                                                        stratify=y)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LRZsliaREG7Y",
      "metadata": {
        "id": "LRZsliaREG7Y"
      },
      "outputs": [],
      "source": [
        "def xgb_grid_search_optimisation(score_data, search_reference=\"no missing data\", save_results=True):\n",
        "    \"\"\"\n",
        "    Perform a grid search hyperparameter optimisation for XGBoost using the specified parameters in constants.py.\n",
        "    Models are evaluated using accuracy, recall and the F-1 score with cross validation.\n",
        "    :param score_data: The training data as a dataframe, this will be prepared through the defined function prior.\n",
        "    :param search_reference: Used to label saved results\n",
        "    \"\"\"\n",
        "    # Setting up model for grid search\n",
        "    xgb_model = xgb.XGBClassifier(enable_categorical=True)\n",
        "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=507)\n",
        "    X_train, X_test, y_train, y_test = data_setup(score_data)\n",
        "\n",
        "    bayes_search = BayesSearchCV(estimator=xgb_model, search_spaces=xgboost_search_space,\n",
        "                                 scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"],\n",
        "                                 refit=\"roc_auc\", n_iter=20, cv=stratified_cv, verbose=0)\n",
        "    bayes_search.fit(X_train, y_train)\n",
        "\n",
        "    # Saving results of grid search in order of F1 score\n",
        "    df = pd.DataFrame(bayes_search.cv_results_)\n",
        "\n",
        "    # Recording the best results of all grid searches with given reference\n",
        "    best_result = df.iloc[0].to_frame().T\n",
        "\n",
        "    return best_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1J1pSqfPEmmE",
      "metadata": {
        "id": "1J1pSqfPEmmE"
      },
      "source": [
        "# Actually Using the WGAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KRWCXss4HWhg",
      "metadata": {
        "id": "KRWCXss4HWhg"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HsQdd1OGem7Q",
      "metadata": {
        "id": "HsQdd1OGem7Q"
      },
      "outputs": [],
      "source": [
        "def run_training(data, missing_features, missing_data, is_raw_missing, level_missing, gen_lr,\n",
        "                 disc_lr, batch_size, alpha, beta, noise_level, gen_layers, disc_layers, reference,\n",
        "                 dropout=0, l2_reg=1e-5, n_critic=5, save_models=True, record_losses=True,\n",
        "                 show_plots=True, progress_bar=True, timestamp=None):\n",
        "    \"\"\"\n",
        "    This will train the given generator and discrminator with the given hyperparameters for up to\n",
        "    1000 epochs or until early stopping is triggered by no improvement in either losses.\n",
        "    \"\"\"\n",
        "    print(\"Recieved gen_lr: {}, disc_lr: {}, dropout: {}, l2_reg: {}, n_critic: {},\\\n",
        "        batch_size: {}, alpha: {}, beta: {}, noise_level: {}, gen_layers: {} and disc_layers: {}\".format(\n",
        "        gen_lr, disc_lr, dropout, l2_reg, n_critic, batch_size, alpha, beta, noise_level,\n",
        "        len(gen_layers), len(disc_layers))\n",
        "        )\n",
        "    # Number of variables to impute (feature length)\n",
        "    n_features = len(numerical_features)\n",
        "\n",
        "    # Preparing the models and their optimisers\n",
        "    generator, discriminator = initialise_models(n_features, gen_layers, disc_layers, dropout,\n",
        "                                                 l2_reg, noise_level=noise_level)\n",
        "    generator_optimiser, discriminator_optimiser = initialise_optimisers(gen_learning_rate=gen_lr,\n",
        "                                                                         disc_learning_rate=disc_lr)\n",
        "\n",
        "    # Only keeping n_epochs as testing different batch sizes\n",
        "    n_epochs = 1000\n",
        "\n",
        "    training_data = split_data(data.values, batch_size)\n",
        "\n",
        "    if record_losses:\n",
        "        # Starting training\n",
        "        generator, discriminator, gen_losses, disc_losses, adv_losses, recon_losses \\\n",
        "        = train(training_data, n_epochs, generator, discriminator, generator_optimiser,\n",
        "                discriminator_optimiser, n_critic, batch_size=batch_size, alpha=alpha, beta=beta,\n",
        "                noise_level=noise_level, gen_layers=gen_layers, disc_layers=disc_layers,\n",
        "                record_losses=record_losses, progress_bar=progress_bar)\n",
        "        if show_plots:\n",
        "          # Visualising losses for this model\n",
        "          plot_losses(gen_losses, disc_losses, reference=reference, show=show_plots)\n",
        "\n",
        "        del gen_losses, disc_losses, adv_losses, recon_losses\n",
        "    else:\n",
        "        generator, discriminator = train(training_data, n_epochs, generator, discriminator,\n",
        "                                         generator_optimiser, discriminator_optimiser, n_critic,\n",
        "                                         batch_size=batch_size, alpha=alpha, beta=beta,\n",
        "                                         noise_level=noise_level, gen_layers=gen_layers,\n",
        "                                         disc_layers=disc_layers, record_losses=record_losses,\n",
        "                                         progress_bar=progress_bar)\n",
        "\n",
        "    # Using that trained model to impute the data\n",
        "    complete_imputation = impute_from_model(missing_features, generator, scaler, imputer,\n",
        "                                            noise_level=noise_level)\n",
        "\n",
        "    complete_imputation.to_csv(\"imputed_data.csv\")\n",
        "\n",
        "    # Checking distributions for mean collapse\n",
        "    if show_plots:\n",
        "      plot_imputed_distributions(missing_features, complete_imputation, reference, show=show_plots)\n",
        "\n",
        "    if \"artificial\" in reference:\n",
        "        imputation_scores = evaluate_artifically_missing(complete_imputation, reference,\n",
        "                                                         missing_data, show=show_plots)\n",
        "    else:\n",
        "        # Restoring the outcome column for prediction\n",
        "        complete_imputation = combine_imputations_with_categorical(complete_imputation,\n",
        "                                                                   missing_data, missing_features)\n",
        "        # Encoding outcome for prediction\n",
        "        complete_imputation[\"outcome_encoded\"] = le.fit_transform(complete_imputation[\"outcome\"])\n",
        "\n",
        "        # Accuracy, Precision, Recall, F-1 and ROC-AUC with means and std.'s\n",
        "        imputation_scores = xgb_grid_search_optimisation(complete_imputation,\n",
        "                                                         search_reference=\"testing\",\n",
        "                                                         save_results=False)\n",
        "\n",
        "    if save_models:\n",
        "        if timestamp is not None:\n",
        "            reference = reference + \"_\" + timestamp\n",
        "\n",
        "        save_and_clear_models(generator, discriminator, reference)\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Deleting after their use\n",
        "    del generator, discriminator, generator_optimiser, discriminator_optimiser, training_data, \\\n",
        "     data, complete_imputation\n",
        "    gc.collect()\n",
        "\n",
        "    plt.close(\"all\")\n",
        "\n",
        "    return imputation_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aXFxYvtuxpbM",
      "metadata": {
        "id": "aXFxYvtuxpbM"
      },
      "source": [
        "# Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g91BBN90AcEv",
      "metadata": {
        "id": "g91BBN90AcEv"
      },
      "outputs": [],
      "source": [
        "# Combinations from here will be tested in a bayesian search\n",
        "wgain_search_space = [\n",
        "    Categorical([0.5, 0.4, 0.3, 0.2, 0.1, 0.01, 0.001, 0.0001], name=\"gen_lr\"),\n",
        "    Categorical([0.01, 0.001, 0.0001, 0.00001], name=\"disc_lr\"),\n",
        "    Categorical([0.05, 0.1, 0.2, 0.3], name=\"dropout\"),\n",
        "    Categorical([1e-5, 1e-4, 1e-3, 1e-2], name=\"l2_reg\"),\n",
        "    Categorical([1, 2, 3, 4, 5], name=\"n_critic\"),\n",
        "    Categorical([8, 16, 32, 64, 128, 256, 512], name=\"batch_size\"),\n",
        "    Categorical([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], name=\"alpha\"),\n",
        "    Categorical([10], name=\"beta\"), # Forgot to modify so unfortunately not included in final results\n",
        "    Categorical([5, 10, 15, 20], name=\"noise_level\"),\n",
        "    Categorical([6, 5, 4, 3, 2, 1], name=\"gen_layers_count\"),\n",
        "    Categorical([6, 5, 4, 3, 2, 1], name=\"disc_layers_count\")\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to save the result of each grid search iteration\n",
        "score_save_path = \"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/results/wgain_{}_individual_scores.csv\""
      ],
      "metadata": {
        "id": "TX1ZZTBl2e1B"
      },
      "id": "TX1ZZTBl2e1B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tk1IvsUMpyWR",
      "metadata": {
        "id": "Tk1IvsUMpyWR"
      },
      "outputs": [],
      "source": [
        "def generate_layer_sizes(layer_count, model_type=\"gen\", base_sizes=[256, 128, 64, 32, 16, 8]):\n",
        "    \"\"\"\n",
        "    Given a number of layers this will return the matching layer sizes to be used. i.e. a layer\n",
        "    count of 3 will return (32, 16, 8), with a maximum of 6 going from 256 to 8.\n",
        "\n",
        "    :param layer_count: An integer representing the total number of layers to be returned.\n",
        "    :param model_type: String representing whether the model is a generator or discriminator.\n",
        "    :param base_sizes: The sizes of the layers to make the selection from, the default is [256, 128,\n",
        "                       64, 32, 16, 8]\n",
        "    :return: A list of layer sizes to be used. If generator it will go from small to large and vice\n",
        "             versa for the discrminator\n",
        "    \"\"\"\n",
        "    n_sizes = len(base_sizes)\n",
        "\n",
        "    if layer_count > n_sizes:\n",
        "        layer_count = n_sizes\n",
        "\n",
        "    layer_sizes = base_sizes[-layer_count:]\n",
        "\n",
        "    if model_type == \"gen\":\n",
        "        return layer_sizes[::-1]\n",
        "    elif model_type == \"disc\":\n",
        "        return layer_sizes\n",
        "    else:\n",
        "        raise ValueError(\"model_type must be 'gen' or 'disc'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uuLKlpn6CVR3",
      "metadata": {
        "id": "uuLKlpn6CVR3"
      },
      "outputs": [],
      "source": [
        "def check_previously_run(reference, results_dir):\n",
        "    \"\"\"\n",
        "    If bayesian search or testing has been done for a specific dataset the results can be passed\n",
        "    to the optimiser to speed up the process.\n",
        "\n",
        "    :param reference: The reference for the data that is being tested.\n",
        "    :param results_dir: The directory containing the results of specific grid search.\n",
        "    :return: Boolean specifying whether the given test has already been completed.\n",
        "    \"\"\"\n",
        "    previous_runs = pd.read_csv(results_dir)\n",
        "\n",
        "    if \"artificial\" in reference:\n",
        "        m_level, m_type = reference.replace(\"artificial_\", \"\").split(\"_\", 1)\n",
        "    else:\n",
        "        m_type, m_level = reference.split(\"_\", 1)\n",
        "\n",
        "    m_level = float(m_level)\n",
        "\n",
        "    relevant_row = previous_runs[(previous_runs[\"missing_type\"] == m_type) &\n",
        "                                (previous_runs[\"missing_level\"] == m_level)]\n",
        "\n",
        "    if relevant_row.empty:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_scores(scores, reference, timestamp, parameters, save_path):\n",
        "    \"\"\"\n",
        "\n",
        "    :param scores:\n",
        "    :param reference:\n",
        "    :param timestamp:\n",
        "    :param parameters:\n",
        "    :param save_path:\n",
        "    \"\"\"\n",
        "    row = {\n",
        "        \"reference\": reference,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"mean_test_accuracy\": scores[\"mean_test_accuracy\"][0],\n",
        "        \"std_test_accuracy\": scores[\"std_test_accuracy\"][0],\n",
        "        \"mean_test_precision\": scores[\"mean_test_precision\"][0],\n",
        "        \"std_test_precision\": scores[\"std_test_precision\"][0],\n",
        "        \"mean_test_recall\": scores[\"mean_test_recall\"][0],\n",
        "        \"std_test_recall\": scores[\"std_test_recall\"][0],\n",
        "        \"mean_test_f1\": scores[\"mean_test_f1\"][0],\n",
        "        \"std_test_f1\": scores[\"std_test_f1\"][0],\n",
        "        \"mean_test_roc_auc\": scores[\"mean_test_roc_auc\"][0],\n",
        "        \"std_test_roc_auc\": scores[\"std_test_roc_auc\"][0]\n",
        "    }\n",
        "\n",
        "    row.update(parameters)\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        pd.DataFrame([row]).to_csv(save_path, mode=\"a\", header=False, index=False)\n",
        "    else:\n",
        "        pd.DataFrame([row]).to_csv(save_path, mode=\"w\", header=True, index=False)\n"
      ],
      "metadata": {
        "id": "YXbT1Ww_2mGO"
      },
      "id": "YXbT1Ww_2mGO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_previous_search_results(save_path, reference):\n",
        "    \"\"\"\n",
        "\n",
        "    :param save_path:\n",
        "    :param reference:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if os.path.exists(save_path):\n",
        "        if \"raw\" in reference:\n",
        "            metric = \"mean_test_roc_auc\"\n",
        "        elif \"artificial\" in reference:\n",
        "            metric = \"average_nmae\"\n",
        "        else:\n",
        "            raise ValueError(\"Invalid reference provided. Must be either artificial or raw\")\n",
        "\n",
        "        previous_runs = pd.read_csv(save_path)\n",
        "\n",
        "        x0 = previous_runs[[\"gen_lr\", \"disc_lr\", \"dropout\", \"l2_reg\", \"n_critic\", \"batch_size\", \"alpha\", \"beta\", \"noise_level\", \"gen_Layers\", \"disc_layers\"]].values.tolist()\n",
        "        y0 = previous_runs[metric].values.tolist()\n",
        "\n",
        "        # roc-auc is saved as a positive, but need to minimise in context of bayesian search so\n",
        "        # making values negative.\n",
        "        if metric == \"mean_test_roc_auc\":\n",
        "            y0 = [-val for val in y0]\n",
        "    else:\n",
        "        x0, y0 = None, None\n",
        "\n",
        "    return x0, y0"
      ],
      "metadata": {
        "id": "lT4kpiBL2sFU"
      },
      "id": "lT4kpiBL2sFU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hLfjoXQ6A7nf",
      "metadata": {
        "id": "hLfjoXQ6A7nf"
      },
      "outputs": [],
      "source": [
        "def run_bayesian_optimisation(features, is_raw_missing, missing_features, missing_data, level_missing, reference, grid_search_dir):\n",
        "    \"\"\"\n",
        "    Completes a bayesian search with 15 random points and 15 targeted points. This minimises either\n",
        "    the MAE when using ground truth data or the AUC-ROC when using real missing data.\n",
        "\n",
        "    :param features: Dataframe containing missing features that require imputation.\n",
        "    :param is_raw_missing: Boolean flag representing whether using raw missing data or data with\n",
        "                            ground truth available.\n",
        "    :param missing_features: The missing features that require imputation.\n",
        "    :param missing_data: The complete dataset from which the features are selected from.\n",
        "    :param level_missing: Either the percentage or number of values missing per feature/row.\n",
        "    :param reference: String representing the reference for this test.\n",
        "    :param grid_search_dir: String representing the directory to store results in.\n",
        "    :return: The best parameters and score found in the bayesian search.\n",
        "    \"\"\"\n",
        "    already_tested = check_previously_run(reference, grid_search_dir)\n",
        "    if already_tested:\n",
        "        print(\"Already tested: {}, so skipping.\".format(reference))\n",
        "        return\n",
        "\n",
        "    save_path = score_save_path.format(reference)\n",
        "    features_filled = features.fillna(features.mean())\n",
        "\n",
        "    x0, y0 = get_previous_search_results(save_path, reference)\n",
        "    # Default is 40 calls\n",
        "    n_calls = 40\n",
        "    initial_points = 20\n",
        "\n",
        "    if x0 == None:\n",
        "        print(\"No previous results found for {}, so starting new bayesian search\".format(reference))\n",
        "    # There are previous runs available to include in search\n",
        "    else:\n",
        "        # Checking how many calls remain in total and at random\n",
        "        n_calls -= len(x0)\n",
        "        initial_points -= len(x0)\n",
        "\n",
        "        # There are no remaining calls so returning the best found result\n",
        "        if n_calls < 1:\n",
        "            print(\"Completed all calls so returning best results.\")\n",
        "\n",
        "            best_run_id = y0.index(min(y0))\n",
        "            best_params = x0[best_run_id]\n",
        "            best_score = y0[best_run_id]\n",
        "\n",
        "            # Early return, matching format of the minimise function\n",
        "            return SimpleNamespace(x=best_params, fun=best_score)\n",
        "\n",
        "        # Checking if any random calls remain\n",
        "        elif initial_points < 1:\n",
        "            print(\"Completed all random points so now refining.\")\n",
        "            initial_points = 0\n",
        "\n",
        "        print(\"Using previous search results for {}, doing {} calls with {} random\".format(reference, n_calls, initial_points))\n",
        "\n",
        "    # Using scikit optimiser function - objective function is to test models using either\n",
        "    # - nMAE for ground truth data\n",
        "    # - ROC-AUC for raw data\n",
        "    @use_named_args(wgain_search_space)\n",
        "    def objective(gen_lr, disc_lr, dropout, l2_reg, n_critic, batch_size, alpha, beta, noise_level,\n",
        "                  gen_layers_count, disc_layers_count):\n",
        "        # Setting up the model architecutres\n",
        "        gen_layers = generate_layer_sizes(gen_layers_count)\n",
        "        disc_layers = generate_layer_sizes(disc_layers_count)\n",
        "\n",
        "        params = {\n",
        "            \"gen_lr\": gen_lr,\n",
        "            \"disc_lr\": disc_lr,\n",
        "            \"dropout\": dropout,\n",
        "            \"l2_reg\": l2_reg,\n",
        "            \"n_critic\": n_critic,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"alpha\": alpha,\n",
        "            \"beta\": beta,\n",
        "            \"noise_level\": noise_level,\n",
        "            \"gen_Layers\": gen_layers_count,\n",
        "            \"disc_layers\": disc_layers_count\n",
        "            }\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Fully training a model and evaluating it given passed values\n",
        "        scores = run_training(features_filled, missing_features, missing_data, is_raw_missing,\n",
        "                              level_missing, gen_lr=gen_lr, disc_lr=disc_lr, dropout=dropout,\n",
        "                              l2_reg=l2_reg, n_critic=n_critic, timestamp=timestamp,\n",
        "                              batch_size=batch_size, alpha=alpha, beta=beta,\n",
        "                              noise_level=noise_level, gen_layers=gen_layers,\n",
        "                              disc_layers=disc_layers, save_models=True, record_losses=False,\n",
        "                              reference=reference, show_plots=False, progress_bar=False)\n",
        "\n",
        "        if is_raw_missing:\n",
        "            save_scores(scores, reference, timestamp, params, save_path)\n",
        "            return -scores[\"mean_test_roc_auc\"].iloc[0]\n",
        "        else:\n",
        "            # Returning the average nMAE value\n",
        "            return scores[\"average_norm_mae\"][0]\n",
        "\n",
        "    # Minimising nMAE, 20 searches within specified search space. 20 random searches and then 20\n",
        "    # probability based searches. Can give x0 and y0 as known good configuration to build on\n",
        "    return gp_minimize(objective, dimensions=gain_search_space, n_calls=n_calls,\n",
        "                       n_initial_points=initial_points, random_state=507, verbose=True, x0=x0,\n",
        "                       y0=y0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IB-moiCexKwK",
      "metadata": {
        "id": "IB-moiCexKwK"
      },
      "source": [
        "# Train and Evaluate Models on Artifically Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v3sYr1DbvlVY",
      "metadata": {
        "id": "v3sYr1DbvlVY"
      },
      "outputs": [],
      "source": [
        "# Used to go through all the combinations of artificially missing data\n",
        "missing_types = [\"mcar\", \"mnar_central\", \"mnar_upper\", \"mnar_lower\"]\n",
        "missing_levels = [\"0.2\", \"0.5\", \"0.7\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ajz4IjnKr4vh",
      "metadata": {
        "id": "ajz4IjnKr4vh"
      },
      "outputs": [],
      "source": [
        "def grid_search_for_artificial_data():\n",
        "    \"\"\"\n",
        "    Completes a grid search for the artificial data by minimising MAE through a bayesian search,\n",
        "    tuning the generator and discriminator hyperparameters.\n",
        "    \"\"\"\n",
        "    artificial_grid_search_dir = (\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/artificial_wgain_gridsearch.csv\")\n",
        "    # Ground truth data\n",
        "    reference_df = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/missing/raw/measurements_0.csv\")\n",
        "    imputer, scaler = fit_scaler_and_imputer(reference_df[numerical_features], temp_imputer,\n",
        "                                             min_max_scaler)\n",
        "\n",
        "    # DF to store the results\n",
        "    if not os.path.exists(artificial_grid_search_dir):\n",
        "        blank_df = pd.DataFrame(columns=[\"missing_type\", \"missing_level\", \"best_nmae\", \"gen_lr\",\n",
        "                                         \"disc_lr\", \"dropout\", \"l2_reg\", \"n_critic\", \"batch_size\",\n",
        "                                         \"alpha\", \"beta\", \"noise_level\", \"gen_Layers\",\n",
        "                                         \"disc_layers\"])\n",
        "        blank_df.to_csv(artificial_grid_search_dir, index=False)\n",
        "        # Only required for first appendage\n",
        "        header = True\n",
        "    else:\n",
        "        header = False\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Go through every combination of artificially missing data\n",
        "    for m_type in missing_types:\n",
        "        for m_level in missing_levels:\n",
        "            # Get the data with the specified missing type and level\n",
        "            artificial_df, artificial_df_missing, artificial_reference = get_data_and_reference(\n",
        "                is_raw_missing=False, level_missing=m_level, missing_type=m_type)\n",
        "            # Scaling the data\n",
        "            artificial_feature_values, artificial_scaled_features = scale_data(artificial_df_missing,\n",
        "                                                                               imputer, scaler)\n",
        "\n",
        "            print(\"Training and testing with {}\".format(artificial_reference))\n",
        "\n",
        "            # Running a bayesian search to optimise\n",
        "            search_result = run_bayesian_optimisation(\n",
        "                                                      features=artificial_scaled_features,\n",
        "                                                      missing_features=artificial_df_missing,\n",
        "                                                      missing_data=artificial_df,\n",
        "                                                      is_raw_missing=False,\n",
        "                                                      level_missing=m_level,\n",
        "                                                      reference=artificial_reference,\n",
        "                                                      grid_search_dir=artificial_grid_search_dir\n",
        "                                                      )\n",
        "\n",
        "            if search_result is None:\n",
        "                continue\n",
        "\n",
        "            # Getting the optimal parameters found\n",
        "            gen_lr, disc_lr, dropout, l2_reg, n_critic, batch_size, alpha, beta, noise_level, \\\n",
        "             gen_layers, disc_layers = search_result.x\n",
        "\n",
        "            # Confirming the results\n",
        "            print(\"Best nMAE {}\".format(search_result.fun))\n",
        "            print(\"Best parameters {}\".format(search_result.x))\n",
        "\n",
        "            # Saving results to csv, appending so if runtime expires a recovery is possible\n",
        "            result_dict = {\"missing_type\": m_type, \"missing_level\": m_level,\n",
        "                           \"best_nmae\": search_result.fun, \"gen_lr\": gen_lr, \"disc_lr\": disc_lr,\n",
        "                           \"dropout\": dropout, \"l2_reg\": l2_reg, \"n_critic\": n_critic,\n",
        "                           \"batch_size\": batch_size, \"alpha\": alpha, \"beta\": beta,\n",
        "                           \"noise_level\": noise_level, \"gen_layers\": gen_layers,\n",
        "                           \"disc_layers\": disc_layers\n",
        "                           }\n",
        "            result_df = pd.DataFrame([result_dict])\n",
        "            result_df.to_csv(artificial_grid_search_dir, mode='a', header=header, index=False)\n",
        "\n",
        "            header=False\n",
        "\n",
        "    print(\"Finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ybXYJKuvvUVY",
      "metadata": {
        "id": "ybXYJKuvvUVY"
      },
      "outputs": [],
      "source": [
        "# grid_search_for_artificial_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-JpSpoQ1GZv5",
      "metadata": {
        "id": "-JpSpoQ1GZv5"
      },
      "source": [
        "#### Checking the results on artificial data and finalising the imputations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2lbAnbc6GhQA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "2lbAnbc6GhQA",
        "outputId": "1ad3e655-9a39-4b16-e339-2bf4f1417a47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    missing_type  missing_level  best_nmae  gen_lr  disc_lr  dropout   l2_reg  \\\n",
              "0           mcar            0.2   0.726789     0.3  0.01000     0.05  0.01000   \n",
              "1           mcar            0.5   0.716690     0.2  0.00010     0.05  0.01000   \n",
              "2           mcar            0.7   0.718369     0.2  0.00001     0.30  0.00100   \n",
              "3   mnar_central            0.2   0.696785     0.2  0.00010     0.05  0.01000   \n",
              "4   mnar_central            0.5   0.790741     0.1  0.00100     0.20  0.01000   \n",
              "5   mnar_central            0.7   0.990974     0.1  0.00010     0.20  0.00010   \n",
              "6     mnar_upper            0.2   0.697681     0.3  0.00010     0.20  0.00001   \n",
              "7     mnar_upper            0.5   0.690121     0.1  0.00001     0.10  0.00001   \n",
              "8     mnar_upper            0.7   0.703761     0.2  0.00001     0.30  0.01000   \n",
              "9     mnar_lower            0.2   0.718231     0.2  0.00001     0.05  0.00100   \n",
              "10    mnar_lower            0.5   0.710807     0.2  0.01000     0.20  0.01000   \n",
              "11    mnar_lower            0.7   0.703869     0.1  0.00010     0.05  0.01000   \n",
              "\n",
              "    n_critic  batch_size  alpha  beta  noise_level  gen_Layers  disc_layers  \n",
              "0          3          32     20    10           15           1            6  \n",
              "1          1         256     20    10           15           1            2  \n",
              "2          3         256    100    10           15           2            3  \n",
              "3          2          16     30    10            5           1            5  \n",
              "4          1          32     20    10           10           4            1  \n",
              "5          4         128     30    10           10           2            5  \n",
              "6          3         256     90    10           15           3            3  \n",
              "7          3         128     30    10           20           4            1  \n",
              "8          5         512     70    10           10           1            6  \n",
              "9          2          16     80    10           20           3            1  \n",
              "10         1          32     50    10           15           1            1  \n",
              "11         1         128    100    10           10           3            6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5819d6ce-750b-4cf7-9721-9f2cc197e43c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>missing_type</th>\n",
              "      <th>missing_level</th>\n",
              "      <th>best_nmae</th>\n",
              "      <th>gen_lr</th>\n",
              "      <th>disc_lr</th>\n",
              "      <th>dropout</th>\n",
              "      <th>l2_reg</th>\n",
              "      <th>n_critic</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>alpha</th>\n",
              "      <th>beta</th>\n",
              "      <th>noise_level</th>\n",
              "      <th>gen_Layers</th>\n",
              "      <th>disc_layers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mcar</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.726789</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mcar</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.716690</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>1</td>\n",
              "      <td>256</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mcar</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.718369</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>3</td>\n",
              "      <td>256</td>\n",
              "      <td>100</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mnar_central</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.696785</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mnar_central</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.790741</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>mnar_central</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.990974</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>4</td>\n",
              "      <td>128</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mnar_upper</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.697681</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>3</td>\n",
              "      <td>256</td>\n",
              "      <td>90</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mnar_upper</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.690121</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>3</td>\n",
              "      <td>128</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>mnar_upper</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.703761</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>5</td>\n",
              "      <td>512</td>\n",
              "      <td>70</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>mnar_lower</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.718231</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>80</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>mnar_lower</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.710807</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>50</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>mnar_lower</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.703869</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>1</td>\n",
              "      <td>128</td>\n",
              "      <td>100</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5819d6ce-750b-4cf7-9721-9f2cc197e43c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5819d6ce-750b-4cf7-9721-9f2cc197e43c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5819d6ce-750b-4cf7-9721-9f2cc197e43c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6090b6ea-b301-4640-b0cb-4258ca5a52d7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6090b6ea-b301-4640-b0cb-4258ca5a52d7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6090b6ea-b301-4640-b0cb-4258ca5a52d7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "artificial_scores",
              "summary": "{\n  \"name\": \"artificial_scores\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"missing_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"mnar_central\",\n          \"mnar_lower\",\n          \"mcar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"missing_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2146173479954639,\n        \"min\": 0.2,\n        \"max\": 0.7,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.2,\n          0.5,\n          0.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_nmae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08356829118466871,\n        \"min\": 0.6901205249611103,\n        \"max\": 0.990973626194178,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.7108070395862736,\n          0.7182305962041896,\n          0.726789113127118\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gen_lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07177405625652734,\n        \"min\": 0.1,\n        \"max\": 0.3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.3,\n          0.2,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disc_lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0038421833281706806,\n        \"min\": 1e-05,\n        \"max\": 0.01,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0001,\n          0.001,\n          0.01\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09876126709792599,\n        \"min\": 0.05,\n        \"max\": 0.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.3,\n          0.1,\n          0.05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"l2_reg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004941190508001451,\n        \"min\": 1e-05,\n        \"max\": 0.01,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.001,\n          1e-05,\n          0.01\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_critic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          5,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 148,\n        \"min\": 16,\n        \"max\": 512,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          256,\n          512,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alpha\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32,\n        \"min\": 20,\n        \"max\": 100,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          20,\n          100,\n          80\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"beta\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10,\n        \"max\": 10,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"noise_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 5,\n        \"max\": 20,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gen_Layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disc_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "artificial_scores = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/artificial_wgain_gridsearch.csv\")\n",
        "artificial_scores.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uW-_Y-NBbz1H",
      "metadata": {
        "id": "uW-_Y-NBbz1H"
      },
      "outputs": [],
      "source": [
        "def generate_best_imputations(scores, test_type=\"artificial\"):\n",
        "    \"\"\"\n",
        "    Function to re-run the best found models with the complete MAE for all the features instead\n",
        "    of purely the average.\n",
        "\n",
        "    :param scores: Dataframe containing the results of the grid search.\n",
        "    :param test_type: String representing the main cateogry of missing data (artificial or raw)\n",
        "    \"\"\"\n",
        "    if test_type == \"artificial\":\n",
        "        is_raw_missing = False\n",
        "        best_scores_dir = (\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/results/artificial_wgain_scores.csv\")\n",
        "        df_columns = [\"reference\", \"mean arterial pressure\",\n",
        "                                         \"heart rate\", \"respiratory rate\", \"PCO2 (Arterial)\",\n",
        "                                         \"PO2 (Arterial)\", \"FiO2\", \"arterial pH\", \"sodium\",\n",
        "                                         \"postassium\", \"creatinine\", \"hematocrit\",\n",
        "                                         \"white blood cell\", \"HCO3 (serum)\", \"average_norm_mae\"]\n",
        "    elif test_type == \"raw\":\n",
        "        is_raw_missing = True\n",
        "        best_scores_dir = (\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/results/raw_wgain_scores.csv\")\n",
        "        df_columns = [\"missing_type\", \"missing_level\", \"mean_test_accuracy\", \"std_test_accuracy\",\n",
        "                      \"mean_test_precision\", \"std_test_precision\", \"mean_test_recall\",\n",
        "                      \"std_test_recall\", \"mean_test_f1\", \"std_test_f1\", \"mean_test_roc_auc\",\n",
        "                      \"std_test_roc_auc\"]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid score type\")\n",
        "\n",
        "    all_tests = scores.groupby([\"missing_type\", \"missing_level\"])\n",
        "    # Check if the results file exists\n",
        "    if not os.path.exists(best_scores_dir):\n",
        "        blank_df = pd.DataFrame(columns=df_columns)\n",
        "        blank_df.to_csv(best_scores_dir, index=False)\n",
        "        # Only required for first appendage\n",
        "        header = True\n",
        "    else:\n",
        "        header = False\n",
        "\n",
        "    # Going through each of the grid search results\n",
        "    for (missing_type, missing_level), group in all_tests:\n",
        "        # Preparing data and reference for imputation\n",
        "        shuffled_data, shuffled_features, data_reference = get_data_and_reference(is_raw_missing,\n",
        "                                                                                  level_missing=missing_level,\n",
        "                                                                                  missing_type=missing_type)\n",
        "\n",
        "        tested_combination = check_previously_run(data_reference, best_scores_dir)\n",
        "\n",
        "        if tested_combination:\n",
        "            print(\"Already tested {}, so skipping.\".format(data_reference))\n",
        "            continue\n",
        "        else:\n",
        "            print(\"Testing, {}\".format(data_reference))\n",
        "\n",
        "        imputer, scaler = fit_scaler_and_imputer(shuffled_data[numerical_features], temp_imputer,\n",
        "                                                 min_max_scaler)\n",
        "\n",
        "        shuffled_feature_values, scaled_features = scale_data(shuffled_features, imputer, scaler)\n",
        "        features_filled = scaled_features.fillna(scaled_features.mean())\n",
        "\n",
        "        # Extracting the best combination of variables\n",
        "        gen_lr = group[\"gen_lr\"].iloc[0]\n",
        "        disc_lr = group[\"disc_lr\"].iloc[0]\n",
        "        dropout = group[\"dropout\"].iloc[0]\n",
        "        l2_reg = group[\"l2_reg\"].iloc[0]\n",
        "        n_critic = group[\"n_critic\"].iloc[0]\n",
        "        batch_size = group[\"batch_size\"].iloc[0]\n",
        "        alpha = group[\"alpha\"].iloc[0]\n",
        "        beta = group[\"beta\"].iloc[0]\n",
        "        noise_level = group[\"noise_level\"].iloc[0]\n",
        "        gen_layers_count = group[\"gen_Layers\"].iloc[0]\n",
        "        disc_layers_count = group[\"disc_layers\"].iloc[0]\n",
        "\n",
        "        # Getting the layer sizes for the models\n",
        "        gen_layers = generate_layer_sizes(gen_layers_count)\n",
        "        disc_layers = generate_layer_sizes(disc_layers_count)\n",
        "\n",
        "        # Training and evaluating the quality of the imputation for the provided model\n",
        "        scores = run_training(data=features_filled, missing_features=shuffled_features,\n",
        "                              missing_data=shuffled_data, is_raw_missing=False,\n",
        "                              level_missing=missing_level, gen_lr=gen_lr, disc_lr=disc_lr,\n",
        "                              dropout=dropout, l2_reg=l2_reg, n_critic=n_critic,\n",
        "                              batch_size=batch_size, alpha=alpha, beta=beta,\n",
        "                              noise_level=noise_level, gen_layers=gen_layers,\n",
        "                              disc_layers=disc_layers, save_models=True, record_losses=False,\n",
        "                              reference=data_reference, show_plots=False, progress_bar=False)\n",
        "\n",
        "        if test_type == \"artificial\":\n",
        "            # Preparing scores and saving to shared file\n",
        "            score_df = pd.DataFrame({\n",
        "                \"missing_type\": [missing_type],\n",
        "                \"missing_level\": [missing_level],\n",
        "                \"mean arterial pressure\": [scores[\"mean arterial pressure\"][0]],\n",
        "                \"heart rate\": [scores[\"heart rate\"][0]],\n",
        "                \"respiratory rate\": [scores[\"respiratory rate\"][0]],\n",
        "                \"PCO2 (Arterial)\": [scores[\"PCO2 (Arterial)\"][0]],\n",
        "                \"PO2 (Arterial)\": [scores[\"PO2 (Arterial)\"][0]],\n",
        "                \"FiO2\": [scores[\"FiO2\"][0]],\n",
        "                \"arterial pH\": [scores[\"arterial pH\"][0]],\n",
        "                \"sodium\": [scores[\"sodium\"][0]],\n",
        "                \"postassium\": [scores[\"postassium\"][0]],\n",
        "                \"creatinine\": [scores[\"creatinine\"][0]],\n",
        "                \"hematocrit\": [scores[\"hematocrit\"][0]],\n",
        "                \"white blood cell\": [scores[\"white blood cell\"][0]],\n",
        "                \"HCO3 (serum)\": [scores[\"HCO3 (serum)\"][0]],\n",
        "                \"average_norm_mae\": [scores[\"average_norm_mae\"][0]]\n",
        "            })\n",
        "        else:\n",
        "            score_df = pd.DataFrame({\n",
        "                \"missing_type\": [missing_type],\n",
        "                \"missing_level\": [missing_level],\n",
        "                \"mean_test_accuracy\": [scores[\"mean_test_accuracy\"][0]],\n",
        "                \"std_test_accuracy\": [scores[\"std_test_accuracy\"][0]],\n",
        "                \"mean_test_precision\": [scores[\"mean_test_precision\"][0]],\n",
        "                \"std_test_precision\": [scores[\"std_test_precision\"][0]],\n",
        "                \"mean_test_recall\": [scores[\"mean_test_recall\"][0]],\n",
        "                \"std_test_recall\": [scores[\"std_test_recall\"][0]],\n",
        "                \"mean_test_f1\": [scores[\"mean_test_f1\"][0]],\n",
        "                \"std_test_f1\": [scores[\"std_test_f1\"][0]],\n",
        "                \"mean_test_roc_auc\": [scores[\"mean_test_roc_auc\"][0]],\n",
        "                \"std_test_roc_auc\": [scores[\"std_test_roc_auc\"][0]]\n",
        "            })\n",
        "\n",
        "        score_df.to_csv(best_scores_dir, mode='a', header=header, index=False)\n",
        "        header = False\n",
        "\n",
        "    complete_scores = pd.read_csv(best_scores_dir)\n",
        "    complete_scores.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dIbpAxmMc32n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIbpAxmMc32n",
        "outputId": "6e9dd454-8f73-4f93-8f4c-8b98d3af84aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already tested artificial_0.2_mcar, so skipping.\n",
            "Already tested artificial_0.5_mcar, so skipping.\n",
            "Already tested artificial_0.7_mcar, so skipping.\n",
            "Already tested artificial_0.2_mnar_central, so skipping.\n",
            "Already tested artificial_0.5_mnar_central, so skipping.\n",
            "Already tested artificial_0.7_mnar_central, so skipping.\n",
            "Already tested artificial_0.2_mnar_lower, so skipping.\n",
            "Already tested artificial_0.5_mnar_lower, so skipping.\n",
            "Already tested artificial_0.7_mnar_lower, so skipping.\n",
            "Already tested artificial_0.2_mnar_upper, so skipping.\n",
            "Already tested artificial_0.5_mnar_upper, so skipping.\n",
            "Already tested artificial_0.7_mnar_upper, so skipping.\n"
          ]
        }
      ],
      "source": [
        "generate_best_imputations(artificial_scores, test_type=\"artificial\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84kucDyLCHia",
      "metadata": {
        "id": "84kucDyLCHia"
      },
      "source": [
        "# Train Models on All Levels of Real Missing Data\n",
        "- evaluation takes place in main code, only getting the imputed datasets to test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4U0wLdnOCS7o",
      "metadata": {
        "id": "4U0wLdnOCS7o"
      },
      "outputs": [],
      "source": [
        "missing_levels = [2, 5, 10]\n",
        "is_raw_missing = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D5I35r7BCSUQ",
      "metadata": {
        "id": "D5I35r7BCSUQ"
      },
      "outputs": [],
      "source": [
        "def grid_search_real_missing():\n",
        "    \"\"\"\n",
        "    Completes a grid search for the artificial data by maximising ROC-AUC through a bayesian search,\n",
        "    tuning the generator and discriminator hyperparameters.\n",
        "    \"\"\"\n",
        "    real_grid_search_dir = (\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/real_wgain_gridsearch.csv\")\n",
        "\n",
        "    # Number of variables to impute\n",
        "    n_features = len(numerical_features)\n",
        "\n",
        "    # DF to store the results\n",
        "    if not os.path.exists(real_grid_search_dir):\n",
        "        blank_df = pd.DataFrame(columns=[\"missing_type\", \"missing_level\", \"best_roc_auc\", \"gen_lr\", \"disc_lr\",\n",
        "                                         \"dropout\", \"l2_reg\", \"n_critic\", \"batch_size\", \"alpha\",\n",
        "                                         \"beta\", \"gen_Layers\", \"disc_layers\"])\n",
        "        blank_df.to_csv(real_grid_search_dir, index=False)\n",
        "        # Only required for first appendage\n",
        "        header = True\n",
        "    else:\n",
        "        header = False\n",
        "    results = []\n",
        "\n",
        "    for m_level in missing_levels:\n",
        "        # Get the data with the specified missing type and level\n",
        "        real_df, real_df_missing, real_reference = get_data_and_reference(\n",
        "            is_raw_missing=True, level_missing=m_level, missing_type=None)\n",
        "\n",
        "        print(\"Testing, {}\".format(real_reference))\n",
        "\n",
        "        # Scaling the data\n",
        "        imputer, scaler = fit_scaler_and_imputer(real_df[numerical_features], temp_imputer,\n",
        "                                                 min_max_scaler)\n",
        "        real_feature_values, real_scaled_features = scale_data(real_df_missing, imputer, scaler)\n",
        "        # Running a bayesian search to optimise\n",
        "        search_result = run_bayesian_optimisation(features=real_scaled_features,\n",
        "                                                    missing_features=real_df_missing,\n",
        "                                                    missing_data=real_df,\n",
        "                                                    is_raw_missing=True,\n",
        "                                                    level_missing=m_level,\n",
        "                                                    reference=real_reference,\n",
        "                                                    grid_search_dir=real_grid_search_dir\n",
        "                                                    )\n",
        "        if search_result is None:\n",
        "            continue\n",
        "\n",
        "        # Getting the optimal parameters found\n",
        "        gen_lr, disc_lr, dropout, l2_reg, n_critic, batch_size, alpha, beta, noise_level, \\\n",
        "         gen_layers, disc_layers = search_result.x\n",
        "\n",
        "        # Confirming the results\n",
        "        print(\"Best ROC-AUC {}\".format(search_result.fun))\n",
        "        print(\"Best parameters {}\".format(search_result.x))\n",
        "\n",
        "        # Saving results to csv, appending so if runtime expires a recovery is possible\n",
        "        result_dict = {\"missing_type\": \"raw\", \"missing_level\": m_level,\n",
        "                       \"best_roc_auc\": search_result.fun, \"gen_lr\": gen_lr,\n",
        "                       \"disc_lr\": disc_lr, \"dropout\": dropout, \"l2_reg\": l2_reg,\n",
        "                       \"n_critic\": n_critic, \"batch_size\": batch_size, \"alpha\": alpha, \"beta\": beta,\n",
        "                       \"noise_level\": noise_level, \"gen_layers\": gen_layers,\n",
        "                       \"disc_layers\": disc_layers\n",
        "                       }\n",
        "        result_df = pd.DataFrame([result_dict])\n",
        "        result_df.to_csv(real_grid_search_dir, mode='a', header=header, index=False)\n",
        "\n",
        "        header=False\n",
        "\n",
        "    print(\"Finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IcmYzxZnDodf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcmYzxZnDodf",
        "outputId": "5f4b3dd1-55b8-46af-fb06-5f48d7c6dd81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing, raw_2\n",
            "Completed all calls so returning best results.\n",
            "Best ROC-AUC -0.8454809770470092\n",
            "Best parameters [0.2, 0.01, 0.05, 0.001, 2.0, 32.0, 90.0, 10.0, 20.0, 4.0, 5.0]\n",
            "Testing, raw_5\n",
            "Completed all calls so returning best results.\n",
            "Best ROC-AUC -0.8425605432761577\n",
            "Best parameters [0.1, 0.0001, 0.1, 0.0001, 4.0, 32.0, 40.0, 10.0, 10.0, 2.0, 2.0]\n",
            "Testing, raw_10\n",
            "Completed all calls so returning best results.\n",
            "Best ROC-AUC -0.8411343328340013\n",
            "Best parameters [0.5, 0.001, 0.05, 1e-05, 3.0, 128.0, 100.0, 10.0, 10.0, 4.0, 1.0]\n",
            "Finished\n"
          ]
        }
      ],
      "source": [
        "# Training the models to work on the real data with different levels of missingness\n",
        "grid_search_real_missing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1biJhSYX9tqN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "1biJhSYX9tqN",
        "outputId": "c4c32fa5-8dbd-47c2-94f3-479a69fc0010"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     missing_type  missing_level  best_roc_auc   gen_lr  disc_lr  dropout  \\\n",
              "raw             2      -0.832858        0.1000  0.00001     0.10  0.00100   \n",
              "raw             2       0.771727        0.0001  0.00100     0.30  0.00001   \n",
              "raw             2       0.771727        0.0001  0.00100     0.30  0.00001   \n",
              "raw             2       0.771727        0.0001  0.00100     0.30  0.00001   \n",
              "raw             2      -0.845481        0.2000  0.01000     0.05  0.00100   \n",
              "raw             2      -0.845481        0.2000  0.01000     0.05  0.00100   \n",
              "raw             5      -0.842561        0.1000  0.00010     0.10  0.00010   \n",
              "raw             2      -0.845481        0.2000  0.01000     0.05  0.00100   \n",
              "raw             5      -0.842561        0.1000  0.00010     0.10  0.00010   \n",
              "raw             2      -0.845481        0.2000  0.01000     0.05  0.00100   \n",
              "raw             5      -0.842561        0.1000  0.00010     0.10  0.00010   \n",
              "raw             2      -0.845481        0.2000  0.01000     0.05  0.00100   \n",
              "\n",
              "     l2_reg  n_critic  batch_size  alpha  beta  gen_Layers  disc_layers  \n",
              "raw     4.0     256.0        70.0   10.0   5.0         1.0          4.0  \n",
              "raw     3.0     128.0        20.0   10.0  20.0         4.0          1.0  \n",
              "raw     3.0     128.0        20.0   10.0  20.0         4.0          1.0  \n",
              "raw     3.0     128.0        20.0   10.0  20.0         4.0          1.0  \n",
              "raw     2.0      32.0        90.0   10.0  20.0         4.0          5.0  \n",
              "raw     2.0      32.0        90.0   10.0  20.0         4.0          5.0  \n",
              "raw     4.0      32.0        40.0   10.0  10.0         2.0          2.0  \n",
              "raw     2.0      32.0        90.0   10.0  20.0         4.0          5.0  \n",
              "raw     4.0      32.0        40.0   10.0  10.0         2.0          2.0  \n",
              "raw     2.0      32.0        90.0   10.0  20.0         4.0          5.0  \n",
              "raw     4.0      32.0        40.0   10.0  10.0         2.0          2.0  \n",
              "raw     2.0      32.0        90.0   10.0  20.0         4.0          5.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65f222e2-f9c9-4432-8b9f-72e12cfbffad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>missing_type</th>\n",
              "      <th>missing_level</th>\n",
              "      <th>best_roc_auc</th>\n",
              "      <th>gen_lr</th>\n",
              "      <th>disc_lr</th>\n",
              "      <th>dropout</th>\n",
              "      <th>l2_reg</th>\n",
              "      <th>n_critic</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>alpha</th>\n",
              "      <th>beta</th>\n",
              "      <th>gen_Layers</th>\n",
              "      <th>disc_layers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.832858</td>\n",
              "      <td>0.1000</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>4.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>2</td>\n",
              "      <td>0.771727</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>3.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>2</td>\n",
              "      <td>0.771727</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>3.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>2</td>\n",
              "      <td>0.771727</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>3.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.845481</td>\n",
              "      <td>0.2000</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>2.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.845481</td>\n",
              "      <td>0.2000</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>2.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>5</td>\n",
              "      <td>-0.842561</td>\n",
              "      <td>0.1000</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>4.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.845481</td>\n",
              "      <td>0.2000</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>2.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>5</td>\n",
              "      <td>-0.842561</td>\n",
              "      <td>0.1000</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>4.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.845481</td>\n",
              "      <td>0.2000</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>2.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>5</td>\n",
              "      <td>-0.842561</td>\n",
              "      <td>0.1000</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>4.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raw</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.845481</td>\n",
              "      <td>0.2000</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>2.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65f222e2-f9c9-4432-8b9f-72e12cfbffad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-65f222e2-f9c9-4432-8b9f-72e12cfbffad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-65f222e2-f9c9-4432-8b9f-72e12cfbffad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e1d21b94-9c9a-4979-a353-e2eaed849af1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e1d21b94-9c9a-4979-a353-e2eaed849af1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e1d21b94-9c9a-4979-a353-e2eaed849af1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "real_scores",
              "summary": "{\n  \"name\": \"real_scores\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"missing_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2,\n        \"max\": 10,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          5,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"missing_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5915364498716578,\n        \"min\": -0.8454809770470092,\n        \"max\": 0.7717267389161435,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7717267389161435,\n          -0.8411343328340013,\n          -0.8454809770470092\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_roc_auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15421994298983993,\n        \"min\": 0.0001,\n        \"max\": 0.5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0001,\n          0.5,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gen_lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004660051360683756,\n        \"min\": 1e-05,\n        \"max\": 0.01,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.001,\n          0.0001,\n          1e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disc_lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08720815992723807,\n        \"min\": 0.05,\n        \"max\": 0.3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.1,\n          0.3,\n          0.05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00047632474548137295,\n        \"min\": 1e-05,\n        \"max\": 0.001,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.001,\n          1e-05,\n          0.0001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"l2_reg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8583950752789521,\n        \"min\": 2.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4.0,\n          3.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_critic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.184784058650976,\n        \"min\": 32.0,\n        \"max\": 256.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          256.0,\n          128.0,\n          32.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.69373328804912,\n        \"min\": 20.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          20.0,\n          100.0,\n          90.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alpha\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 10.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"beta\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.495213227987849,\n        \"min\": 5.0,\n        \"max\": 20.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gen_Layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0699237552766379,\n        \"min\": 1.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disc_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7554426642213128,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "real_scores = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/real_wgain_gridsearch.csv\")\n",
        "real_scores.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lalVUipP6vLa",
      "metadata": {
        "id": "lalVUipP6vLa"
      },
      "outputs": [],
      "source": [
        "def extract_best_scores(test_type=\"raw\"):\n",
        "    \"\"\"\n",
        "    Used to extract the best ROC-AUC scores from the complete grid search. A new csv is created\n",
        "    containing just the best runs with their hyperparameters and either complete ground truth scores\n",
        "    or the downstream scores.\n",
        "\n",
        "    This is used instead of generate best imputations as it is less intensive and more telling.\n",
        "\n",
        "    :param test_type: String representing the main cateogry of missing data (artificial or raw).\n",
        "    \"\"\"\n",
        "    if test_type == \"raw\":\n",
        "        # Only 3 raw files and tested in downstream.\n",
        "        file_references = [\"raw_2\", \"raw_5\", \"raw_10\"]\n",
        "        metric = \"mean_test_roc_auc\"\n",
        "    elif test_type == \"artificial\":\n",
        "        # 12 combinations of missing types and different missing levels\n",
        "        missing_types = [\"mcar\", \"mnar_central\", \"mnar_upper\", \"mnar_lower\"]\n",
        "        missing_levels = [\"0.2\", \"0.5\", \"0.7\"]\n",
        "        file_references = []\n",
        "\n",
        "        for missing_type in missing_types:\n",
        "            for missing_level in missing_levels:\n",
        "                file_references.append(\"{}_{}\".format(missing_type, missing_level))\n",
        "\n",
        "        # Tested in their ground truth reconstruction\n",
        "        metric = \"average_norm_mae\"\n",
        "    else:\n",
        "        raise ValueError(\"Invalid score type.\")\n",
        "    best_results = []\n",
        "\n",
        "    # Go through each  of the results files\n",
        "    for file in file_references:\n",
        "        file_dir = \"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/results/wgain_{}_individual_scores.csv\".format(file)\n",
        "\n",
        "        complete_scores = pd.read_csv(file_dir)\n",
        "        complete_scores = complete_scores.drop(columns=[\"timestamp\"])\n",
        "\n",
        "        # Extract result with the best result, maximising ROC-AUC and minimising nMAE\n",
        "        if test_type == \"raw\":\n",
        "           best_result = complete_scores.iloc[complete_scores[metric].idxmax()]\n",
        "        else:\n",
        "            best_result = complete_scores.iloc[complete_scores[metric].idxmin()]\n",
        "        # Save the best result to list\n",
        "        best_results.append(best_result)\n",
        "\n",
        "    # Convert best results to a dataframe and save\n",
        "    best_results_df = pd.DataFrame(best_results)\n",
        "    best_results_df.to_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/results/best_{}_wgain_scores.csv\".format(test_type), index=False)\n",
        "\n",
        "    return best_results_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_best_scores()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "EkQTlusaziVg",
        "outputId": "8d5ffaeb-2d4a-47f5-ed72-3b657c369e4f"
      },
      "id": "EkQTlusaziVg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   reference  mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
              "31     raw_2            0.875148           0.001215             0.884116   \n",
              "24     raw_5            0.920880           0.001753             0.927293   \n",
              "36    raw_10            0.918327           0.001105             0.925380   \n",
              "\n",
              "    std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
              "31            0.001184          0.981573         0.002692      0.930295   \n",
              "24            0.001396          0.990760         0.001625      0.957975   \n",
              "36            0.000808          0.990144         0.001281      0.956667   \n",
              "\n",
              "    std_test_f1  mean_test_roc_auc  ...  disc_lr  dropout   l2_reg  n_critic  \\\n",
              "31     0.000769           0.845481  ...   0.0100     0.05  0.00100         2   \n",
              "24     0.000932           0.842561  ...   0.0001     0.10  0.00010         4   \n",
              "36     0.000597           0.841134  ...   0.0010     0.05  0.00001         3   \n",
              "\n",
              "    batch_size  alpha  beta  noise_level  gen_Layers  disc_layers  \n",
              "31          32     90    10           20           4            5  \n",
              "24          32     40    10           10           2            2  \n",
              "36         128    100    10           10           4            1  \n",
              "\n",
              "[3 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c014fcc-bfda-443d-8819-9c254a204b93\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reference</th>\n",
              "      <th>mean_test_accuracy</th>\n",
              "      <th>std_test_accuracy</th>\n",
              "      <th>mean_test_precision</th>\n",
              "      <th>std_test_precision</th>\n",
              "      <th>mean_test_recall</th>\n",
              "      <th>std_test_recall</th>\n",
              "      <th>mean_test_f1</th>\n",
              "      <th>std_test_f1</th>\n",
              "      <th>mean_test_roc_auc</th>\n",
              "      <th>...</th>\n",
              "      <th>disc_lr</th>\n",
              "      <th>dropout</th>\n",
              "      <th>l2_reg</th>\n",
              "      <th>n_critic</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>alpha</th>\n",
              "      <th>beta</th>\n",
              "      <th>noise_level</th>\n",
              "      <th>gen_Layers</th>\n",
              "      <th>disc_layers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>raw_2</td>\n",
              "      <td>0.875148</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>0.884116</td>\n",
              "      <td>0.001184</td>\n",
              "      <td>0.981573</td>\n",
              "      <td>0.002692</td>\n",
              "      <td>0.930295</td>\n",
              "      <td>0.000769</td>\n",
              "      <td>0.845481</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>90</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>raw_5</td>\n",
              "      <td>0.920880</td>\n",
              "      <td>0.001753</td>\n",
              "      <td>0.927293</td>\n",
              "      <td>0.001396</td>\n",
              "      <td>0.990760</td>\n",
              "      <td>0.001625</td>\n",
              "      <td>0.957975</td>\n",
              "      <td>0.000932</td>\n",
              "      <td>0.842561</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>4</td>\n",
              "      <td>32</td>\n",
              "      <td>40</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>raw_10</td>\n",
              "      <td>0.918327</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.925380</td>\n",
              "      <td>0.000808</td>\n",
              "      <td>0.990144</td>\n",
              "      <td>0.001281</td>\n",
              "      <td>0.956667</td>\n",
              "      <td>0.000597</td>\n",
              "      <td>0.841134</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>3</td>\n",
              "      <td>128</td>\n",
              "      <td>100</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 22 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c014fcc-bfda-443d-8819-9c254a204b93')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c014fcc-bfda-443d-8819-9c254a204b93 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c014fcc-bfda-443d-8819-9c254a204b93');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-beedf226-3527-48cc-a379-ab0ef17f2349\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-beedf226-3527-48cc-a379-ab0ef17f2349')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-beedf226-3527-48cc-a379-ab0ef17f2349 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}