{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "47jQTu_3xYbz",
      "metadata": {
        "id": "47jQTu_3xYbz"
      },
      "source": [
        "# Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dT3IOhs1Kj_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dT3IOhs1Kj_",
        "outputId": "c524f53d-0eee-4eb6-8464-d55463b137bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gSbtK1xE1lFm",
      "metadata": {
        "id": "gSbtK1xE1lFm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models, layers, mixed_precision, regularizers, initializers\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dy2_1b_bqKZ-",
      "metadata": {
        "id": "dy2_1b_bqKZ-"
      },
      "outputs": [],
      "source": [
        "# Seeds for reproduciblity\n",
        "random_seed = 50701\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n",
        "\n",
        "tf.keras.utils.set_random_seed(random_seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "# Defining scaler and imputer to use throughout. Imputation is temporary so  scaling works properly.\n",
        "min_max_scaler = MinMaxScaler()\n",
        "temp_imputer = SimpleImputer(strategy=\"mean\")\n",
        "\n",
        "# Used when making predictions\n",
        "le = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DvZeVTAC1nnt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvZeVTAC1nnt",
        "outputId": "7a004127-225b-4511-f7df-d898af4cc5dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow is not using the GPU.\n"
          ]
        }
      ],
      "source": [
        "# Ensuring using best setup with colab\n",
        "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "# Flag for if GPU is enabled\n",
        "\n",
        "if len(gpus) > 0:\n",
        "    GPU = True\n",
        "    policy = mixed_precision.Policy(\"mixed_float16\")\n",
        "    mixed_precision.set_global_policy(policy)\n",
        "\n",
        "    print(\"TensorFlow is using the GPU.\")\n",
        "else:\n",
        "    GPU = False\n",
        "\n",
        "    print(\"TensorFlow is not using the GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YMBGYg3E3jmA",
      "metadata": {
        "id": "YMBGYg3E3jmA"
      },
      "source": [
        "# Decide on the Data\n",
        "These can be modified to test 1 specific model which is broken down fully in the\n",
        "following sections.\n",
        "\n",
        "To test all models the section is towards the end under \"Train and Evaluate All\n",
        "Models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8pezPq803mUX",
      "metadata": {
        "id": "8pezPq803mUX"
      },
      "outputs": [],
      "source": [
        "# Select numerical features for imputation to impute\n",
        "numerical_features = [\"mean arterial pressure\", \"heart rate\", \"respiratory rate\", \"PCO2 (Arterial)\",\n",
        "                      \"PO2 (Arterial)\", \"FiO2\", \"arterial pH\", \"sodium\", \"postassium\", \"creatinine\",\n",
        "                      \"hematocrit\", \"white blood cell\", \"HCO3 (serum)\"]\n",
        "\n",
        "# Decide data specifics to test either raw or artificial with different missing mechanisms\n",
        "is_raw_missing = False  # Artificial otherwise\n",
        "missing_type = \"mcar\"  # Artificially missing only - mcar, mnar_central, mnar_upper, mnar_lower\n",
        "\n",
        "# Depends on missing limits\n",
        "# - Artificial is a percentage missing i.e. 0.2, 0.5 and 0.7\n",
        "# - Raw is number of values missing per row i.e. 2, 5, 10\n",
        "if is_raw_missing:\n",
        "    level_missing = 5\n",
        "else:\n",
        "    level_missing = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jzc8dFGx2Sz1",
      "metadata": {
        "id": "jzc8dFGx2Sz1"
      },
      "source": [
        "# Read and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UjyCeiq76_E1",
      "metadata": {
        "id": "UjyCeiq76_E1"
      },
      "outputs": [],
      "source": [
        "def get_data_and_reference(is_raw_missing=False, level_missing=0.5, missing_type=\"mcar\"):\n",
        "    \"\"\"\n",
        "    Given the specified flags return the specified original data, the numerical features to be\n",
        "    imputed and a reference specifying the data used.\n",
        "\n",
        "    :param is_raw_missing: Boolean flag specifying whether the data is artficially missing or raw.\n",
        "    :param level_missing: A float or integer representing either the percentage of missingness or\n",
        "                          the number of missing values per row.\n",
        "    :param missing_type: A string representing the type of missingness used - only applicable to\n",
        "                         artificially missing data.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if is_raw_missing:\n",
        "        reference = \"raw_{}\".format(level_missing)\n",
        "        df = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/missing/raw/measurements_{}.csv\".format(level_missing))\n",
        "    else:\n",
        "        reference = \"artificial_{}_{}\".format(level_missing, missing_type)\n",
        "        df = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/missing/artificial/measurements_{}_{}.csv\".format(level_missing, missing_type))\n",
        "\n",
        "    # Shuffling the data\n",
        "    df = df.sample(frac=1, random_state=507).reset_index(drop=True)\n",
        "\n",
        "    df_features = df[numerical_features]\n",
        "\n",
        "    return df, df_features, reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xY2wForNvnrx",
      "metadata": {
        "id": "xY2wForNvnrx"
      },
      "outputs": [],
      "source": [
        "def fit_scaler_and_imputer(df_to_fit_to, imputer, scaler):\n",
        "    \"\"\"\n",
        "    Given a dataset containing the training data, the temporary imputer and scaler this will fit\n",
        "    both the imputer and scale on the given data and return them.\n",
        "\n",
        "    :param df_to_fit_to:\n",
        "    :param imputer:\n",
        "    :param scaler:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # Fitting imputer on original data\n",
        "    imputer.fit(df_to_fit_to.values)\n",
        "\n",
        "    # Temporarily imputing the data\n",
        "    filled_train = imputer.transform(df_to_fit_to.values)\n",
        "\n",
        "    # Fitting the scaler on the temporary imputation\n",
        "    scaler.fit(filled_train)\n",
        "\n",
        "    return imputer, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pRAfyl_toVo-",
      "metadata": {
        "id": "pRAfyl_toVo-"
      },
      "outputs": [],
      "source": [
        "def scale_data(features_df, imputer, scaler):\n",
        "    \"\"\"\n",
        "    Given the data with missing features this will temporarily fill them through the given imputer\n",
        "    and scale the features.\n",
        "\n",
        "    :param features_df: The dataframe containing the missing features to be scaled.\n",
        "    :param imputer: The fitteed imputer.\n",
        "    :param scaler: The fitted scaler.\n",
        "    :return: The raw data and the scaled features.\n",
        "    \"\"\"\n",
        "    feature_values = features_df.values\n",
        "    missing_mask = features_df.isna()\n",
        "\n",
        "    # Using temporary imputation to fill nan's before scalling\n",
        "    features_temp_filled = imputer.transform(feature_values)\n",
        "\n",
        "    # Scaling the data\n",
        "    featured_scaled_filled = scaler.transform(features_temp_filled)\n",
        "\n",
        "    # Restoring the nans\n",
        "    featured_scaled_filled[missing_mask] = np.nan\n",
        "\n",
        "    scaled_features = pd.DataFrame(featured_scaled_filled, index=features_df.index, columns=features_df.columns)\n",
        "\n",
        "    return scaled_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pBseUuHUbSAZ",
      "metadata": {
        "id": "pBseUuHUbSAZ"
      },
      "outputs": [],
      "source": [
        "# Get the specified data\n",
        "shuffled_data, shuffled_features, data_reference = get_data_and_reference(is_raw_missing,\n",
        "                                                                          level_missing,\n",
        "                                                                          missing_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ScRTPYl9pNw-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ScRTPYl9pNw-",
        "outputId": "e1b8ad9b-a2bb-4f0f-8465-cb2b1c7c0d48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   mean arterial pressure  heart rate  respiratory rate  PCO2 (Arterial)  \\\n",
              "0                0.213720    0.287356          0.085271              NaN   \n",
              "1                     NaN    0.183908               NaN         0.188976   \n",
              "2                     NaN         NaN               NaN         0.173228   \n",
              "3                0.258575         NaN          0.093023              NaN   \n",
              "4                     NaN    0.551724               NaN         0.362205   \n",
              "5                0.284960         NaN          0.147287              NaN   \n",
              "6                     NaN         NaN          0.100775              NaN   \n",
              "7                0.224274         NaN               NaN         0.212598   \n",
              "8                     NaN    0.396552               NaN         0.141732   \n",
              "9                0.271768    0.339080               NaN         0.220472   \n",
              "\n",
              "   PO2 (Arterial)  FiO2  arterial pH    sodium  postassium  creatinine  \\\n",
              "0             NaN   1.0     0.525641  0.363636         NaN    0.041860   \n",
              "1             NaN   1.0          NaN       NaN         NaN    0.037209   \n",
              "2        0.393324   NaN     0.615385       NaN         NaN         NaN   \n",
              "3        0.571843   NaN          NaN       NaN         NaN    0.046512   \n",
              "4        0.044993   1.0          NaN       NaN    0.734177         NaN   \n",
              "5             NaN   1.0          NaN  0.530303    0.468354    0.069767   \n",
              "6        0.336720   NaN          NaN       NaN         NaN         NaN   \n",
              "7             NaN   NaN          NaN  0.469697    0.253165    0.027907   \n",
              "8        0.119013   NaN          NaN       NaN         NaN    0.027907   \n",
              "9             NaN   NaN          NaN  0.439394         NaN    0.027907   \n",
              "\n",
              "   hematocrit  white blood cell  HCO3 (serum)  \n",
              "0    0.453125          0.156395           NaN  \n",
              "1    0.687500               NaN           NaN  \n",
              "2    0.398438          0.069583      0.390244  \n",
              "3    0.367188          0.088138      0.463415  \n",
              "4         NaN          0.237906           NaN  \n",
              "5    0.390625          0.132538           NaN  \n",
              "6    0.406250               NaN           NaN  \n",
              "7         NaN               NaN      0.390244  \n",
              "8    0.445312               NaN           NaN  \n",
              "9         NaN          0.059642      0.463415  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29afc8c6-1600-4e1f-84c2-25a09605a9e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean arterial pressure</th>\n",
              "      <th>heart rate</th>\n",
              "      <th>respiratory rate</th>\n",
              "      <th>PCO2 (Arterial)</th>\n",
              "      <th>PO2 (Arterial)</th>\n",
              "      <th>FiO2</th>\n",
              "      <th>arterial pH</th>\n",
              "      <th>sodium</th>\n",
              "      <th>postassium</th>\n",
              "      <th>creatinine</th>\n",
              "      <th>hematocrit</th>\n",
              "      <th>white blood cell</th>\n",
              "      <th>HCO3 (serum)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.213720</td>\n",
              "      <td>0.287356</td>\n",
              "      <td>0.085271</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.525641</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.041860</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>0.156395</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.183908</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.188976</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.037209</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.173228</td>\n",
              "      <td>0.393324</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.398438</td>\n",
              "      <td>0.069583</td>\n",
              "      <td>0.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.258575</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.093023</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.571843</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.046512</td>\n",
              "      <td>0.367188</td>\n",
              "      <td>0.088138</td>\n",
              "      <td>0.463415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.362205</td>\n",
              "      <td>0.044993</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.734177</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.237906</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.284960</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.147287</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.530303</td>\n",
              "      <td>0.468354</td>\n",
              "      <td>0.069767</td>\n",
              "      <td>0.390625</td>\n",
              "      <td>0.132538</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.100775</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.336720</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.406250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.224274</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.212598</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.469697</td>\n",
              "      <td>0.253165</td>\n",
              "      <td>0.027907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.396552</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.141732</td>\n",
              "      <td>0.119013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.027907</td>\n",
              "      <td>0.445312</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.271768</td>\n",
              "      <td>0.339080</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.220472</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.439394</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.027907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.059642</td>\n",
              "      <td>0.463415</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29afc8c6-1600-4e1f-84c2-25a09605a9e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29afc8c6-1600-4e1f-84c2-25a09605a9e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29afc8c6-1600-4e1f-84c2-25a09605a9e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-73ad5dc5-3978-46c4-9bd6-8e077a9bb035\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-73ad5dc5-3978-46c4-9bd6-8e077a9bb035')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-73ad5dc5-3978-46c4-9bd6-8e077a9bb035 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "scaled_featured",
              "summary": "{\n  \"name\": \"scaled_featured\",\n  \"rows\": 8223,\n  \"fields\": [\n    {\n      \"column\": \"mean arterial pressure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11734029718711267,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 244,\n        \"samples\": [\n          0.24010554089709762,\n          0.2453825857519789,\n          0.7651715039577837\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"heart rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11793169498869165,\n        \"min\": 0.0,\n        \"max\": 0.9999999999999999,\n        \"num_unique_values\": 131,\n        \"samples\": [\n          0.16091954022988503,\n          0.21264367816091956,\n          0.20114942528735635\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"respiratory rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.058349112382160985,\n        \"min\": 0.0,\n        \"max\": 0.9999999999999999,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          0.08527131782945736,\n          0.20155038759689925,\n          0.4806201550387597\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PCO2 (Arterial)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08699186516570281,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 92,\n        \"samples\": [\n          0.06299212598425197,\n          0.20472440944881892,\n          0.6692913385826771\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PO2 (Arterial)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18192770814693957,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 506,\n        \"samples\": [\n          0.6748911465892597,\n          0.4716981132075472,\n          0.02612481857764877\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FiO2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24262880046383262,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1.0,\n          0.39759036144578314,\n          0.3473895582329317\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arterial pH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0957208550299168,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 68,\n        \"samples\": [\n          0.2051282051282044,\n          0.5384615384615365,\n          0.6282051282051277\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sodium\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06883046087497355,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          0.3787878787878787,\n          0.18181818181818188,\n          0.6666666666666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"postassium\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09019675990340634,\n        \"min\": 0.0,\n        \"max\": 0.9999999999999999,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          0.7341772151898733,\n          0.3417721518987341,\n          0.6962025316455694\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"creatinine\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06104470391033187,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          0.4744186046511628,\n          0.2186046511627907,\n          0.4279069767441861\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hematocrit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14913276956661334,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 120,\n        \"samples\": [\n          0.203125,\n          0.6796875,\n          0.390625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"white blood cell\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05594082039384032,\n        \"min\": 0.0,\n        \"max\": 0.9999999999999999,\n        \"num_unique_values\": 402,\n        \"samples\": [\n          0.22001325381047046,\n          0.16898608349900596,\n          0.0874751491053678\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HCO3 (serum)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09387909188472057,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 37,\n        \"samples\": [\n          0.7073170731707318,\n          0.5609756097560976,\n          0.41463414634146345\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# Scale the data, using placeholder imputationss\n",
        "imputer, scaler = fit_scaler_and_imputer(shuffled_features, temp_imputer, min_max_scaler)\n",
        "scaled_featured = scale_data(shuffled_features, imputer, scaler)\n",
        "\n",
        "scaled_featured.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ELhaqHSV2VyY",
      "metadata": {
        "id": "ELhaqHSV2VyY"
      },
      "source": [
        "# Using the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UMGOurdoxlzq",
      "metadata": {
        "id": "UMGOurdoxlzq"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L9fqoKMV4b_W",
      "metadata": {
        "id": "L9fqoKMV4b_W"
      },
      "outputs": [],
      "source": [
        "def build_encoder(n_features, latent_dimensions, l2_reg=1e-5, layer_sizes=None, add_noise=False):\n",
        "    \"\"\"\n",
        "    Used to build an encoder for MIWAE. This takes the temporarily filled dataset and outputs the\n",
        "    mean and log variance of the latent representation of the variables. The latent representation\n",
        "    is a compressed representation of the features and can be used to predict the missing values\n",
        "    when decoded.\n",
        "\n",
        "    The outputted mean and variance represent the mean/expected value of the features in the latent\n",
        "    space and the log variance represents the certaintity of the mean (lower is better).\n",
        "\n",
        "    :param n_features: The number of features that require imputation.\n",
        "    :param latent_dimensions: The number of dimensions to \"compress\" the feature data into.\n",
        "    :param l2_reg: The regulairsation rate applied to the dense hidden layers.\n",
        "    :param layer_sizes: A tuple representing the sizes and order of layers (e.g. (32, 64, 128))\n",
        "    :param add_noise: Boolean to add noise to the encoder.\n",
        "    :return: The specified encoder model, which outputs the mean and log variance.\n",
        "    \"\"\"\n",
        "    # Weight initialiser - need to look into why they used this\n",
        "    xavier_init = initializers.GlorotUniform()\n",
        "\n",
        "    # Setting default layers, small to large to increase complexity of captured patterns\n",
        "    if layer_sizes is None:\n",
        "        layer_sizes = (32, 64, 128)\n",
        "\n",
        "    # Input data is the complete dataset with missing values temporarily imputed\n",
        "    input_layer = layers.Input(shape=(n_features * 2,), name=\"input_layer\")\n",
        "    x = input_layer\n",
        "\n",
        "    # Building the architecture from the given layer sizes.\n",
        "    for size in layer_sizes:\n",
        "        x = layers.Dense(units=size, kernel_initializer=xavier_init,\n",
        "                         kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
        "\n",
        "        if add_noise:\n",
        "            x = layers.GaussianNoise(0.05)(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Mean of the latent representation of the variables\n",
        "    mean = layers.Dense(units=latent_dimensions, name=\"mean\")(x)\n",
        "    # Represents the range/spread of certaintity for the feature variables. Want it to be low,\n",
        "    # meaning it closer to the mean and the latnet representation is better.\n",
        "    log_variance = layers.Dense(units=latent_dimensions, name=\"log_variance\")(x)\n",
        "\n",
        "    # Built model on the given specification, outputting the mean and log variance\n",
        "    encoder = models.Model(inputs=input_layer, outputs=[mean, log_variance], name=\"encoder\")\n",
        "\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7EoO34qR4t6j",
      "metadata": {
        "id": "7EoO34qR4t6j"
      },
      "outputs": [],
      "source": [
        "def build_decoder(n_features, latent_dimensions, l2_reg, layer_sizes=None, dropout=0.2):\n",
        "    \"\"\"\n",
        "    Used to build a decoder for the MIWAE. It takes the output of the encoder which is the features\n",
        "    in their latent space. Using this representation the decoder will \"uncompress\" the data into\n",
        "    its original dimensionality, with the missing values imputed.\n",
        "\n",
        "    :param n_features: The number of features in the data.\n",
        "    :param latent_dimensions: The number of latent dimensions the data was compressed into.\n",
        "    :param l2_reg: The regualirsation rate applied to the dense hidden layers.\n",
        "    :param layer_sizes: Tuple specifying the sizes of the hidden dense layers (e.g. (128, 64, 32))\n",
        "    :param dropout: The dropout rate applied to the dense layers.\n",
        "    :return: The specified decoder model, which outputs the imputed dataset.\n",
        "    \"\"\"\n",
        "    # Weight initialiser\n",
        "    xavier_init = initializers.GlorotUniform()\n",
        "\n",
        "    # Applying default layer sizs\n",
        "    if layer_sizes is None:\n",
        "        layer_sizes = (128, 64, 32)\n",
        "\n",
        "    # The input of the model is the latent dimensions\n",
        "    input_layer = layers.Input(shape=(latent_dimensions,), name=\"latent_input\")\n",
        "    x = input_layer\n",
        "\n",
        "    # Building the hidden layers. If dropout specified it is added to every layer.\n",
        "    for size in layer_sizes:\n",
        "        x = layers.Dense(units=size, kernel_initializer=xavier_init,\n",
        "                         kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
        "\n",
        "        if dropout:\n",
        "            x = layers.Dropout(dropout)(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Reconstructing the original features from the latent space. Using sigmoid as the data is\n",
        "    # scaled between 0 and 1.\n",
        "    reconstructed_data = layers.Dense(n_features, activation=\"sigmoid\", name=\"reconstructed_data\")(x)\n",
        "\n",
        "    # Final decoder model which outputs the reconstructed data with missing values imputed\n",
        "    decoder = models.Model(inputs=input_layer, outputs=reconstructed_data, name=\"decoder\")\n",
        "\n",
        "    return decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vNYhZhPTHCce",
      "metadata": {
        "id": "vNYhZhPTHCce"
      },
      "outputs": [],
      "source": [
        "def find_miwae_loss(original_data, reconstructed_data, missing_mask, latent_mean, latent_log_var,\n",
        "                    n_latent_samples, latent_samples):\n",
        "    \"\"\"\n",
        "    Finds the MIWAE loss of the reconstructed data, specifically the difference between the real\n",
        "    observed data and the reconstructed data. Still don't fully understand it and code aided by AI\n",
        "    because of the lack of understanding.\n",
        "\n",
        "    :param original_data: The original dataset.\n",
        "    :param reconstructed_data: The reconstruction of the data from the decoder.\n",
        "    :param missing_mask: Mask representing where data was originally missing.\n",
        "    :param latent_mean: The mean outputted from the encoder.\n",
        "    :param latent_log_var: THe log variance outputted from the encoder.\n",
        "    :param n_latent_samples: The number of latent samples provided by the encoder.\n",
        "    :param latent_samples: The samples that have been taken from the encoder.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # Expand the original data and mask to match the reconstructed dimensions. Then repeating the\n",
        "    # data across the new dimensions.\n",
        "    original_data = tf.expand_dims(original_data, axis=1)\n",
        "    original_data = tf.repeat(original_data, n_latent_samples, axis=1)\n",
        "\n",
        "    missing_mask = tf.expand_dims(missing_mask, axis=1)\n",
        "    missing_mask = tf.repeat(missing_mask, n_latent_samples, axis=1)\n",
        "\n",
        "    latent_log_var = tf.clip_by_value(latent_log_var, -10, 10)\n",
        "\n",
        "    # Remove log from the variance, adding small value to avoid 0 division\n",
        "    variance = tf.exp(latent_log_var) + 1e-8\n",
        "\n",
        "    latent_samples = tf.cast(latent_samples, tf.float32)\n",
        "    latent_mean = tf.cast(latent_mean, tf.float32)\n",
        "    latent_log_var = tf.cast(latent_log_var, tf.float32)\n",
        "    variance = tf.cast(variance, tf.float32)\n",
        "\n",
        "    # Calculate the reconstruction loss of the observed feature values\n",
        "    feature_recon_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(original_data, reconstructed_data)\n",
        "    recon_loss = -tf.reduce_sum(missing_mask * feature_recon_loss, axis=-1)\n",
        "\n",
        "    # Expand other dimensions to match the latent dimensions\n",
        "    latent_mean = tf.expand_dims(latent_mean, axis=1)\n",
        "    latent_log_var = tf.expand_dims(latent_log_var, axis=1)\n",
        "    variance = tf.expand_dims(variance, axis=1)\n",
        "\n",
        "    # Use probability density function (pdf) to determine how likely a sample is to occur in the\n",
        "    # encoders distribution and the gaussian distribution\n",
        "    log_likelihood_encoder_distribution = -0.5 * tf.reduce_sum(latent_log_var + tf.square(latent_samples - latent_mean) / variance + tf.math.log(2 * np.pi), axis=-1)\n",
        "\n",
        "    log_likelihood_normal_distribution = -0.5 * tf.reduce_sum(tf.square(latent_samples) + tf.math.log(2 * np.pi), axis=-1)\n",
        "\n",
        "    # Find the importance weights\n",
        "    log_weights = recon_loss + log_likelihood_normal_distribution - log_likelihood_encoder_distribution\n",
        "\n",
        "    # Normalise the weights\n",
        "    maximum_weights = tf.reduce_max(log_weights, axis=1, keepdims=True)\n",
        "    weights = tf.exp(log_weights - maximum_weights)\n",
        "    weights = weights / tf.reduce_sum(weights, axis=1, keepdims=True)\n",
        "\n",
        "    # Caclulate the final loss\n",
        "    weighted_log_likelihood = tf.reduce_sum(weights * log_weights, axis=1)\n",
        "    miwae_loss = -tf.reduce_mean(weighted_log_likelihood)\n",
        "\n",
        "    return miwae_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GvgNQS4X2aw1",
      "metadata": {
        "id": "GvgNQS4X2aw1"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HZTR9c_yx0hK",
      "metadata": {
        "id": "HZTR9c_yx0hK"
      },
      "source": [
        "### Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "udtvAhfsjMuH",
      "metadata": {
        "id": "udtvAhfsjMuH"
      },
      "outputs": [],
      "source": [
        "def sample_latent(batch_size, latent_mean, latent_log_var, n_latent_samples):\n",
        "    \"\"\"\n",
        "    Given the output of the encoder, sample latent variables from it to be used in reconstruction\n",
        "    and loss evaluation.\n",
        "\n",
        "    :param batch_size: The batch size of the current training data/\n",
        "    :param latent_mean: The latent mean from the encoder.\n",
        "    :param latent_log_var: The latent log variation from the encoder.\n",
        "    :param n_latent_samples: The number of samples to get from the latent space.\n",
        "    :return: The specified number of latent samples.\n",
        "    \"\"\"\n",
        "    # Number of latent variables\n",
        "    latent_dim = tf.shape(latent_mean)[1]\n",
        "\n",
        "    # Random gaussian noise\n",
        "    gaussian_noise = tf.random.normal(shape=(batch_size, n_latent_samples, latent_dim))\n",
        "\n",
        "    # Standard deviation of the variance\n",
        "    var_std = tf.exp(0.5 * latent_log_var)\n",
        "\n",
        "    # Expanding the mean and std to match the latent dimensions\n",
        "    mean_expanded = tf.expand_dims(latent_mean, axis=1)\n",
        "    var_std_expanded = tf.expand_dims(var_std, axis=1)\n",
        "\n",
        "    mean_expanded = tf.cast(mean_expanded, tf.float32)\n",
        "    gaussian_noise = tf.cast(gaussian_noise, tf.float32)\n",
        "    var_std_expanded = tf.cast(var_std_expanded, tf.float32)\n",
        "\n",
        "    # Sampling the latent variables - some maths behind this i dont yet understand\n",
        "    latent_samples = mean_expanded + gaussian_noise * var_std_expanded\n",
        "\n",
        "    return latent_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YCOexHjP9fES",
      "metadata": {
        "id": "YCOexHjP9fES"
      },
      "outputs": [],
      "source": [
        "def train_step(batch, encoder, decoder, optimiser, n_latent_samples, training=True):\n",
        "    \"\"\"\n",
        "    Completes one training step for the encoder and decoder, using the provided batch data.\n",
        "\n",
        "    :param batch: The current batch of data.\n",
        "    :param encoder: The encoder model.\n",
        "    :param decoder: The decoder model.\n",
        "    :param optimiser: The optimiser used for the MIWAE.\n",
        "    :param n_latent_samples: The number of latent samples to use.\n",
        "    \"\"\"\n",
        "    batch = tf.cast(batch, tf.float32)\n",
        "    n_latent_samples = tf.cast(n_latent_samples, tf.int32)\n",
        "\n",
        "    batch_size = tf.shape(batch)[0]\n",
        "\n",
        "    # Missing mask: 1 if observed, 0 if missing\n",
        "    missing_mask = tf.where(tf.math.is_nan(batch), tf.zeros_like(batch), tf.ones_like(batch))\n",
        "    missing_mask = tf.cast(missing_mask, tf.float32)\n",
        "\n",
        "    # Feature means to impute missing values temporarily\n",
        "    feature_mean = tf.reduce_sum(tf.where(tf.math.is_nan(batch), tf.zeros_like(batch), batch),\n",
        "                                 axis=0) / (tf.reduce_sum(missing_mask, axis=0))\n",
        "    batch_mean_filled = tf.where(tf.math.is_nan(batch), tf.broadcast_to(feature_mean,\n",
        "                                                                        tf.shape(batch)), batch)\n",
        "\n",
        "    # Noise for missing values\n",
        "    noise = tf.random.normal(tf.shape(batch), stddev=1)\n",
        "    noisy_batch = batch_mean_filled * missing_mask + noise * (1 - missing_mask)\n",
        "\n",
        "    if training:\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Get the latent means and log variances from the encoder\n",
        "            encoder_input = tf.concat([noisy_batch, missing_mask], axis=1)\n",
        "            latent_mean, latent_log_var = encoder(encoder_input, training=True)\n",
        "\n",
        "            # Getting latent samples from the encoders output\n",
        "            latent_samples = sample_latent(batch_size, latent_mean, latent_log_var, n_latent_samples)\n",
        "\n",
        "            latent_samples_for_decoder = tf.reshape(latent_samples, (-1, tf.shape(latent_samples)[-1]))\n",
        "\n",
        "            # Get the rconstruction from the decoder\n",
        "            reconstructed_data = decoder(latent_samples_for_decoder, training=True)\n",
        "            reconstructed_data = tf.reshape(reconstructed_data, (batch_size, n_latent_samples, -1))\n",
        "            reconstructed_data = tf.cast(reconstructed_data, tf.float32)\n",
        "\n",
        "            # Calculate the MIWAE loss\n",
        "            loss = find_miwae_loss(original_data=batch, reconstructed_data=reconstructed_data,\n",
        "                                missing_mask=missing_mask, latent_mean=latent_mean,\n",
        "                                latent_log_var=latent_log_var, n_latent_samples=n_latent_samples,\n",
        "                                latent_samples=latent_samples)\n",
        "\n",
        "            # Find and update the gradients for the encoder an decoder\n",
        "            gradients = tape.gradient(loss, encoder.trainable_variables + decoder.trainable_variables)\n",
        "            optimiser.apply_gradients(zip(gradients, encoder.trainable_variables +\n",
        "                                        decoder.trainable_variables))\n",
        "    else:\n",
        "        # validation test - repeating training version but with no gradient updates\n",
        "        encoder_input = tf.concat([noisy_batch, missing_mask], axis=1)\n",
        "        latent_mean, latent_log_var = encoder(encoder_input, training=False)\n",
        "\n",
        "        latent_samples = sample_latent(batch_size, latent_mean, latent_log_var, n_latent_samples)\n",
        "        latent_samples_for_decoder = tf.reshape(latent_samples, (-1, tf.shape(latent_samples)[-1]))\n",
        "\n",
        "        reconstructed_data = decoder(latent_samples_for_decoder, training=False)\n",
        "        reconstructed_data = tf.reshape(reconstructed_data, (batch_size, n_latent_samples, -1))\n",
        "        reconstructed_data = tf.cast(reconstructed_data, tf.float32)\n",
        "\n",
        "        loss = find_miwae_loss(original_data=batch, reconstructed_data=reconstructed_data,\n",
        "                              missing_mask=missing_mask, latent_mean=latent_mean,\n",
        "                              latent_log_var=latent_log_var, n_latent_samples=n_latent_samples,\n",
        "                              latent_samples=latent_samples)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qng19XC3E5W9",
      "metadata": {
        "id": "qng19XC3E5W9"
      },
      "outputs": [],
      "source": [
        "def train(training_data, validation_data, n_epochs, encoder, decoder, optimiser,\n",
        "          n_latent_samples=10, patience=50, progress_bar=True):\n",
        "    \"\"\"\n",
        "    To be written.\n",
        "\n",
        "    :param training_data: The training data.\n",
        "    :param validation_data: The validation data.\n",
        "    :param n_epochs: The max number of epochs to train for.\n",
        "    :param encoder: The initalised encoder.\n",
        "    :param decoder: The intialised decoder.\n",
        "    :param optimiser: The optimiser for the encoder/decoder.\n",
        "    :param n_latent_samples: The number of latent samples to be used in the loss function.\n",
        "    :param patience: The limit for how many epochs can be trained without an improvement in epoch\n",
        "                     loss\n",
        "    :return: The trained encoder and decoder and their losses.\n",
        "    \"\"\"\n",
        "    # Tracking losses and early stopping\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    wait = 0\n",
        "    best_val_loss = np.inf\n",
        "\n",
        "    # Initialising the best weights to defaults\n",
        "    best_encoder_weights = encoder.get_weights()\n",
        "    best_decoder_weights = decoder.get_weights()\n",
        "\n",
        "    # Using tf.function so it runs more efficiently\n",
        "    step_function = tf.function(train_step)\n",
        "\n",
        "    # Training for the specified number of epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss = 0.0\n",
        "        training_batches = tf.data.experimental.cardinality(training_data).numpy()\n",
        "\n",
        "        # If set to true then a progress bar is used to show the training progress\n",
        "        if progress_bar:\n",
        "            pbar = tqdm(total=training_batches, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\",\n",
        "                        leave=False)\n",
        "        else:\n",
        "            pbar = None\n",
        "\n",
        "        # Training on the batches\n",
        "        for train_batch in training_data:\n",
        "            # Train on the batch\n",
        "            train_step_loss = step_function(train_batch, encoder, decoder, optimiser, n_latent_samples, training=True)\n",
        "\n",
        "            # Update the overall epoch loss\n",
        "            train_loss += float(train_step_loss)\n",
        "\n",
        "            if progress_bar:\n",
        "                # Update progress bar after each batch\n",
        "                pbar.update(1)\n",
        "\n",
        "        # Averaging to get the final epoch loss\n",
        "        train_loss = train_loss / training_batches\n",
        "        training_losses.append(train_loss)\n",
        "\n",
        "        if progress_bar:\n",
        "            pbar.close()\n",
        "\n",
        "        # Running validation now, so not training but testing the model\n",
        "        val_loss = 0\n",
        "        val_batches = tf.data.experimental.cardinality(validation_data).numpy()\n",
        "\n",
        "        for val_batch in validation_data:\n",
        "            step_val_loss = step_function(val_batch, encoder, decoder, optimiser, n_latent_samples, training=False)\n",
        "            val_loss += float(step_val_loss)\n",
        "\n",
        "        val_loss /= val_batches\n",
        "        validation_losses.append(val_loss)\n",
        "\n",
        "        # if (epoch + 1) % 50 == 0:\n",
        "        #     tf.print(\"Epoch {} Train Loss: {} and Validation Loss: {}\".format(epoch + 1, train_loss, val_loss))\n",
        "\n",
        "        # Check if this loss is better than the last epochs\n",
        "        if val_loss < best_val_loss - 1e-4:\n",
        "            # Reset wait counter and update the best loss\n",
        "            wait = 0\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "            # Record best weights\n",
        "            best_encoder_weights = encoder.get_weights()\n",
        "            best_decoder_weights = decoder.get_weights()\n",
        "\n",
        "        # Increment wait counter and initiate early stopping if patience exceeded\n",
        "        else:\n",
        "            wait += 1\n",
        "\n",
        "            # If patience exceeded end training and revert models to their best weights\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping at epoch {}\".format(epoch+1))\n",
        "                encoder.set_weights(best_encoder_weights)\n",
        "                decoder.set_weights(best_decoder_weights)\n",
        "                break\n",
        "\n",
        "    return encoder, decoder, training_losses, validation_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lQdj4Y-6qu1l",
      "metadata": {
        "id": "lQdj4Y-6qu1l"
      },
      "outputs": [],
      "source": [
        "def split_data(features_df, batch_size, validation_split=0.2):\n",
        "    \"\"\"\n",
        "    Given the raw training data this function will scale it, shuffle it and split it into the\n",
        "    desired batch sizes (with any remainder dropped).\n",
        "\n",
        "    :param features_df: Dataframe containing the missing features to be imputed.\n",
        "    :param batch_size: Size of the individual batches for the training data.\n",
        "    :return: The shuffled and batched data.\n",
        "    \"\"\"\n",
        "    # Shuffling the data and batching it\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(features_df)\n",
        "    # Using high buffer size as overshooting should not affect performance\n",
        "    dataset = dataset.shuffle(buffer_size=len(features_df), seed=507)\n",
        "\n",
        "    # Splitting into validation and training\n",
        "    validation_size = int(len(features_df) * validation_split)\n",
        "    validation_dataset = dataset.take(validation_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    train_dataset = dataset.skip(validation_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_dataset, validation_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HY4J4fuQx3XS",
      "metadata": {
        "id": "HY4J4fuQx3XS"
      },
      "source": [
        "### Training and Variable Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gRoNVHsC13Le",
      "metadata": {
        "id": "gRoNVHsC13Le"
      },
      "outputs": [],
      "source": [
        "def initialise_models(n_features, encoder_layers, decoder_layers, latent_dimensions=10, dropout=0.1,\n",
        "                      l2_reg=1e-5, noise_level=10):\n",
        "    \"\"\"\n",
        "    Given the dimension size, number of features and regularisation rate the models are built and\n",
        "    returned.\n",
        "\n",
        "    :n_features: The number of features to be trained on.\n",
        "    :encoder_layers: The individual layer sizes for the encoder.\n",
        "    :decoder_layers: The individual layer sizes for the decoder.\n",
        "    :dropout: The dropout rate for the decoder - check if makes sense.\n",
        "    :l2_reg: The regularisation rate applied to either model.\n",
        "    :return: The initialised encoder and decoder.\n",
        "    \"\"\"\n",
        "    # Initialise the models to be trained\n",
        "    encoder = build_encoder(n_features, latent_dimensions, l2_reg=l2_reg,\n",
        "                            layer_sizes=encoder_layers)\n",
        "    decoder = build_decoder(n_features, latent_dimensions, l2_reg=l2_reg,\n",
        "                            layer_sizes=decoder_layers, dropout=dropout)\n",
        "\n",
        "    return encoder, decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uLVVqMJLjxyh",
      "metadata": {
        "id": "uLVVqMJLjxyh"
      },
      "outputs": [],
      "source": [
        "def initialise_optimisers(learning_rate=1e-5):\n",
        "    \"\"\"\n",
        "    Prepares the optimiser for the MIWAE model with the given learning rate and a schedule with\n",
        "    exponential decay. Using Adam optimiser and clipping.\n",
        "\n",
        "    :param learning_rate: The learning rate for the MIWAE encoder and decoder.\n",
        "    :return: The initialised optimisers for the MIWAE encoder and decoder.\n",
        "    \"\"\"\n",
        "    # Using decay so as training goes on the learning rate will decrease.\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=learning_rate,\n",
        "    decay_steps=5000,\n",
        "    decay_rate=0.98)\n",
        "\n",
        "    # Define optimiser, using clipnorm to prevent extreme gradients\n",
        "    optimiser = keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0, beta_1=0.5,\n",
        "                                      beta_2=0.9)\n",
        "\n",
        "    return optimiser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B5cHFrt4xrza",
      "metadata": {
        "id": "B5cHFrt4xrza"
      },
      "source": [
        "### Check Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "747orrr8Dqvy",
      "metadata": {
        "id": "747orrr8Dqvy"
      },
      "outputs": [],
      "source": [
        "def plot_losses(miwae_losses, reference=None, show=True):\n",
        "    \"\"\"\n",
        "    Using the losses after training and the reference for saving this will plot the losses over time\n",
        "    of the training.\n",
        "\n",
        "    :param miwae_losses: The tracked losses for the MIWAE.\n",
        "    :param reference: String representing the experiment reference, used to title the plot.\n",
        "    :param show: Boolean to decide whether plot is shown in environment or saved as always.\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    plt.plot(range(len(miwae_losses)), miwae_losses, label=\"MIWAE Loss\")\n",
        "    plt.grid()\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Losses for {}\".format(reference))\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Visualisations/losses/{}_losses_separate.png\".format(reference))\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9wRxKVD9iLZC",
      "metadata": {
        "id": "9wRxKVD9iLZC"
      },
      "source": [
        "### Saving the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2mUuDLV6iM56",
      "metadata": {
        "id": "2mUuDLV6iM56"
      },
      "outputs": [],
      "source": [
        "def save_and_clear_models(encoder, decoder, reference):\n",
        "    \"\"\"\n",
        "    This saves the trained models under the given reference and clears the keras sesssion to avoid\n",
        "    data leakage.\n",
        "\n",
        "    :param encoder: The trained encoder.\n",
        "    :param decoder: The trained decoder.\n",
        "    :param reference: String representing the experiment reference, used to name the model files.\n",
        "    \"\"\"\n",
        "    # Outputting trained models\n",
        "    print(\"saving\")\n",
        "    encoder.save(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Models/{}_encoder.keras\".format(reference))\n",
        "    decoder.save(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Models/{}_decoder.keras\".format(reference))\n",
        "\n",
        "    # Clearing state for new model\n",
        "    keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y5iWJ6wWH8lK",
      "metadata": {
        "id": "y5iWJ6wWH8lK"
      },
      "source": [
        "# Evaluate the Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3E2WM1OyZVa",
      "metadata": {
        "id": "c3E2WM1OyZVa"
      },
      "source": [
        "## Functions to Test Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oYhVY2UE1437",
      "metadata": {
        "id": "oYhVY2UE1437"
      },
      "outputs": [],
      "source": [
        "def impute_from_model(missing_features, encoder, decoder, scaler, imputer, n_latent_samples=10):\n",
        "    \"\"\"\n",
        "    Given the data containing missing data and the relevant encoder, decoder, scaler and imputer\n",
        "    this will return the final imputation from the MIWAE. The scaler and imputer should be fitted\n",
        "    to the passed missing data.\n",
        "\n",
        "    The returned data is not complete and requires calling \"combine_imputations_with_categorical\"\n",
        "    for demographic data to be added back.\n",
        "\n",
        "    :param missing_features: Thhe Dataframe containing the missing features to be imputed.\n",
        "    :param encoder: The trained encoder to impute with.\n",
        "    :param decoder: The trained decoder to impute with.\n",
        "    :param scaler: Scaler to scale the data before imputing.\n",
        "    :param imputer: Imputer for temporary filling before scaling.\n",
        "    :param n_latent_samples: Number of latent samples to use.\n",
        "    :return: The imputations from the MIWAE.\n",
        "    \"\"\"\n",
        "    # Getting the feature values and the missing mask\n",
        "    feature_values = missing_features.values.astype(np.float32)\n",
        "    missing_mask = (~np.isnan(feature_values)).astype(np.float32)\n",
        "\n",
        "    # Filling and scaling the features to work with MIWAE\n",
        "    features_filled = imputer.transform(feature_values)\n",
        "    features_scaled = scaler.transform(features_filled)\n",
        "\n",
        "    # Add noise where values are missing - this was filled in by colab so might need verifying\n",
        "    std = features_scaled.std(axis=0, keepdims=True)\n",
        "    noise = np.random.normal(0, std, size=features_scaled.shape)\n",
        "    X_noisy = features_scaled * missing_mask + noise * (1 - missing_mask)\n",
        "\n",
        "    # Getting the latent mean and log variance from the encoder\n",
        "    missing_mask = (~np.isnan(feature_values)).astype(np.float32)\n",
        "    encoder_input = np.concatenate([X_noisy, missing_mask], axis=1)\n",
        "    latent_mean, latent_log_var = encoder(encoder_input, training=False)\n",
        "\n",
        "    batch_size = X_noisy.shape[0]\n",
        "\n",
        "    n_latent_samples = tf.cast(n_latent_samples, tf.int32)\n",
        "\n",
        "    latent_samples = sample_latent(batch_size=batch_size, latent_mean=latent_mean,\n",
        "                                   latent_log_var=latent_log_var, n_latent_samples=n_latent_samples)\n",
        "\n",
        "    # Need to change the dimensions as they wont fit in the decoder currently\n",
        "    latent_samples_for_decoder = tf.reshape(latent_samples, (batch_size * n_latent_samples,\n",
        "                                                             latent_samples.shape[2]))\n",
        "\n",
        "    # Get the rconstruction from the decoder and reshaping into original\n",
        "    reconstructed_data_shaped = decoder(latent_samples_for_decoder, training=False)\n",
        "    reconstructed_data = tf.reshape(reconstructed_data_shaped, (batch_size, n_latent_samples, -1))\n",
        "\n",
        "    # Getting the average of the latent reconstructions\n",
        "    reconstruction_mean = tf.reduce_mean(reconstructed_data, axis=1).numpy()\n",
        "\n",
        "    reconstructed_unscaled = scaler.inverse_transform(reconstruction_mean)\n",
        "\n",
        "    # Replace the missing values with the imputations\n",
        "    final_imputed_features = np.where(np.isnan(feature_values), reconstructed_unscaled,\n",
        "                                      feature_values)\n",
        "\n",
        "    df_pred = pd.DataFrame(final_imputed_features, columns=missing_features.columns,\n",
        "                           index=missing_features.index)\n",
        "\n",
        "    return df_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nK4tdfcp96qV",
      "metadata": {
        "id": "nK4tdfcp96qV"
      },
      "outputs": [],
      "source": [
        "def combine_imputations_with_categorical(imputed_data, missing_data, missing_features):\n",
        "    \"\"\"\n",
        "    Given an imputed dataset containg just the imputed features this function will add back the\n",
        "    demographic data and return it.\n",
        "\n",
        "    :param imputed_data: The data which has already been imputed by the MIWAE.\n",
        "    :param missing_data: The original dataset with the missing values and desired columns.\n",
        "    :param missing_features: The ?\n",
        "\n",
        "    :return: The imputed data with the non-feature data returned.\n",
        "    \"\"\"\n",
        "    # Adding the demographic data back\n",
        "    non_feature_cols = missing_data.drop(columns=missing_features.columns)\n",
        "    df_final = pd.concat([non_feature_cols, imputed_data], axis=1)\n",
        "\n",
        "    return df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iXudV12mnOmY",
      "metadata": {
        "id": "iXudV12mnOmY"
      },
      "outputs": [],
      "source": [
        "def plot_imputed_distributions(original_features, imputed_data, reference, show=True):\n",
        "    \"\"\"\n",
        "    This will plot the differences in distributions of the imputed and original datasets. It works\n",
        "    with both raw and artficially missing data and the histograms are normalised.\n",
        "\n",
        "    Still work on colours - very bad\n",
        "\n",
        "    :param original_features: The original dataset with missing values that the imputed data comes from\n",
        "    :param imputed_data: The imputed dataset\n",
        "    :param reference:  String representing the experiment reference, used to title the plot.\n",
        "    :param show: Boolean to decide whether plot is shown in environment or saved as always.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(18, 16))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, col in enumerate(original_features):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Density normalises it - look into better colours\n",
        "        ax.hist(imputed_data[col], alpha=0.5, label=\"Imputed\", color=\"blue\", edgecolor=\"black\",\n",
        "                density=True)\n",
        "        ax.hist(original_features[col], alpha=0.5, label=\"Original\", color=\"orange\",\n",
        "                edgecolor=\"black\", density=True)\n",
        "\n",
        "        ax.set_title(col)\n",
        "        ax.tick_params(axis=\"x\")\n",
        "        ax.tick_params(axis=\"y\")\n",
        "        ax.legend()\n",
        "\n",
        "    # Odd number of features so need to remove extra plots\n",
        "    axes[13].axis('off')\n",
        "    axes[14].axis('off')\n",
        "    axes[15].axis('off')\n",
        "\n",
        "    plt.suptitle(\"Variable Distribututions for {}\".format(reference), fontsize=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Visualisations/imputed distributions/{}_variable_distribututions.png\".format(reference))\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y2wqsgGHfkuz",
      "metadata": {
        "id": "Y2wqsgGHfkuz"
      },
      "source": [
        "## Evaluate Artifically Missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yoRuEk6ruuC7",
      "metadata": {
        "id": "yoRuEk6ruuC7"
      },
      "outputs": [],
      "source": [
        "def evaluate_normalised_mae(ground_truth, imputation, missing_data):\n",
        "    \"\"\"\n",
        "    If the imputed data has a ground truth then this will evaluate the imputations through\n",
        "    the normalised mean squared error. It will return the normalised MAE per features and the\n",
        "    average.\n",
        "\n",
        "    :param ground_truth: The ground truth data with no missing values.\n",
        "    :param imputation: The imputed dataset from the artificially missing dataset.\n",
        "    :param missing_data: The original dataset with the missing values.\n",
        "    :return: The normalised MAE per feature and the average as a DataFrame.\n",
        "    \"\"\"\n",
        "    # Tracking the individual values and the results so they can be averaged\n",
        "    norm_mae_values = []\n",
        "    norm_mae_results = {}\n",
        "\n",
        "    # Identify where the data was missing before imputation\n",
        "    missing_mask = missing_data.isna()\n",
        "\n",
        "    for feature in imputation.columns:\n",
        "        if feature not in missing_mask.columns:\n",
        "            continue\n",
        "\n",
        "        # Comparing values that were missing only\n",
        "        missing_ground_truth = ground_truth[feature][missing_mask[feature]]\n",
        "        missing_imputed = imputation[feature][missing_mask[feature]]\n",
        "\n",
        "        # MAE\n",
        "        feature_mae = mean_absolute_error(missing_ground_truth, missing_imputed)\n",
        "\n",
        "        # Normalising through IQR\n",
        "        Q1 = missing_ground_truth.quantile(0.25)\n",
        "        Q3 = missing_ground_truth.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        norm_feature_mae = feature_mae / IQR\n",
        "\n",
        "        norm_mae_results[feature] = norm_feature_mae\n",
        "        norm_mae_values.append(norm_feature_mae)\n",
        "\n",
        "    norm_mae_results[\"average_norm_mae\"] = np.mean(norm_mae_values)\n",
        "\n",
        "    return norm_mae_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W_9kgqS5ueAj",
      "metadata": {
        "id": "W_9kgqS5ueAj"
      },
      "outputs": [],
      "source": [
        "def plot_normalised_mae(norm_mae_results, reference, show=True):\n",
        "    \"\"\"\n",
        "    This plots the normalised MAE for each of the features\n",
        "\n",
        "    :param norm_mae_results: The normalised MAE results from the evaluation.\n",
        "    :param reference: String representing the experiment reference, used to title the plot.\n",
        "    :param show: Boolean to decide whether plot is shown in environment or saved as always.\n",
        "    \"\"\"\n",
        "    features = list(norm_mae_results.keys())\n",
        "    values = list(norm_mae_results.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    bars = plt.barh(features, values, edgecolor=\"black\")\n",
        "    plt.xlabel(\"Normalised Mean Absolute Error\")\n",
        "    plt.title(\"Normalised Mean Absolute Error by Feature for {}\".format(reference))\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    for bar in bars:\n",
        "      width = bar.get_width()\n",
        "      plt.text(width + 0.005, bar.get_y() + bar.get_height() / 2, \"{:.3f}\".format(width))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Visualisations/nmae/{}_feature_norm_mae.png\".format(reference))\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yOhVWizrfncq",
      "metadata": {
        "id": "yOhVWizrfncq"
      },
      "outputs": [],
      "source": [
        "def evaluate_artifically_missing(imputed_data, reference, missing_data, show=True):\n",
        "    \"\"\"\n",
        "    This is a wrapper to read in the ground truth data, evaluate the imputed data through nMAE,\n",
        "    plot the nMAE per feature and return a dataframe containing the results.\n",
        "    \"\"\"\n",
        "    # Reading in the ground truth data\n",
        "    reference_df = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/missing/raw/measurements_0.csv\")\n",
        "\n",
        "    # Getting the nMAE for the imputations\n",
        "    norm_mae_results = evaluate_normalised_mae(reference_df, imputed_data, missing_data)\n",
        "\n",
        "    # Shows the individual nMAE for each feature\n",
        "    plot_normalised_mae(norm_mae_results, reference, show=show)\n",
        "\n",
        "    # Final df to represent the evaluation\n",
        "    results_df = pd.DataFrame(norm_mae_results, index=[0])\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rghX6F9iB9EJ",
      "metadata": {
        "id": "rghX6F9iB9EJ"
      },
      "source": [
        "# Evaluate Real Missing Through Predictive Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SGzaUXkSZZu4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGzaUXkSZZu4",
        "outputId": "9c6b3f07-74fe-467a-e388-843ba281fb4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (25.7.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (25.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Js76LVOzZdT_",
      "metadata": {
        "id": "Js76LVOzZdT_"
      },
      "outputs": [],
      "source": [
        "# Importing optimiser package from scikit\n",
        "from skopt import gp_minimize, BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# Used to train model\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Tidy memory\n",
        "import gc\n",
        "\n",
        "# Used for processing\n",
        "from types import SimpleNamespace\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TNdkI3pPNi7t",
      "metadata": {
        "id": "TNdkI3pPNi7t"
      },
      "outputs": [],
      "source": [
        "# Parameter search space for the prediction model\n",
        "xgboost_search_space = {\n",
        "    \"gamma\": Categorical([0.01, 0.1]),\n",
        "    \"learning_rate\": Categorical([0.001, 0.01, 0.1]),\n",
        "    \"max_depth\": Categorical([3, 6, 9]),\n",
        "    \"n_estimators\": Categorical([100, 200, 300])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UnN04tzjWP-L",
      "metadata": {
        "id": "UnN04tzjWP-L"
      },
      "outputs": [],
      "source": [
        "def data_setup(score_data):\n",
        "    \"\"\"\n",
        "    Split the data into training and test data for both the features and predicted values. Using stratified sampling to\n",
        "    get even class distributions.\n",
        "    :param score_data: The data to be split\n",
        "    :return: X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    # Splitting into features and target variables\n",
        "    X = score_data[numerical_features].copy()\n",
        "    y = score_data[\"outcome_encoded\"].copy()\n",
        "\n",
        "    # Splitting into training and test data, stratifying due to limited data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=507,\n",
        "                                                        stratify=y)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LRZsliaREG7Y",
      "metadata": {
        "id": "LRZsliaREG7Y"
      },
      "outputs": [],
      "source": [
        "def xgb_grid_search_optimisation(score_data, search_reference=\"no missing data\", save_results=True):\n",
        "    \"\"\"\n",
        "    Perform a grid search hyperparameter optimisation for XGBoost using the specified parameters in constants.py.\n",
        "    Models are evaluated using accuracy, recall and the F-1 score with cross validation.\n",
        "    :param score_data: The training data as a dataframe, this will be prepared through the defined function prior.\n",
        "    :param search_reference: Used to label saved results\n",
        "    \"\"\"\n",
        "    # Setting up model for grid search\n",
        "    xgb_model = xgb.XGBClassifier(enable_categorical=True)\n",
        "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=507)\n",
        "    X_train, X_test, y_train, y_test = data_setup(score_data)\n",
        "\n",
        "    bayes_search = BayesSearchCV(estimator=xgb_model, search_spaces=xgboost_search_space,\n",
        "                                 scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"],\n",
        "                                 refit=\"roc_auc\", n_iter=20, cv=stratified_cv, verbose=0)\n",
        "    bayes_search.fit(X_train, y_train)\n",
        "\n",
        "    # Saving results of grid search in order of F1 score\n",
        "    df = pd.DataFrame(bayes_search.cv_results_)\n",
        "\n",
        "    # Recording the best results of all grid searches with given reference\n",
        "    best_result = df.iloc[0].to_frame().T\n",
        "\n",
        "    return best_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1J1pSqfPEmmE",
      "metadata": {
        "id": "1J1pSqfPEmmE"
      },
      "source": [
        "# Actually Using the MIWAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HsQdd1OGem7Q",
      "metadata": {
        "id": "HsQdd1OGem7Q"
      },
      "outputs": [],
      "source": [
        "def run_training(data, missing_features, missing_data, is_raw_missing, level_missing, lr,\n",
        "                 latent_dimensions, batch_size, encoder_layers, decoder_layers, reference, scaler,\n",
        "                 imputer, n_latent_samples=10, dropout=0, l2_reg=1e-5, show_plots=True,\n",
        "                 progress_bar=True, save_models=False, timestamp=None):\n",
        "    \"\"\"\n",
        "    This will train the given encoder and decoder with the given hyperparameters for up to 1000\n",
        "    epochs or until early stopping is triggered by no improvement in either losses.\n",
        "    \"\"\"\n",
        "    # Number of variables to impute (feature length)\n",
        "    n_features = len(numerical_features)\n",
        "\n",
        "    # Preparing the models and the optimiser\n",
        "    encoder, decoder = initialise_models(n_features, encoder_layers, decoder_layers,\n",
        "                                         latent_dimensions, dropout, l2_reg)\n",
        "    optimiser = initialise_optimisers(lr)\n",
        "\n",
        "    # Only keeping n_epochs as testing different batch sizes\n",
        "    n_epochs = 1000\n",
        "\n",
        "    training_data, validation_data = split_data(data.values, batch_size)\n",
        "\n",
        "    # Starting training\n",
        "    encoder, decoder, train_losses, val_losses = train(training_data, validation_data, n_epochs,\n",
        "                                                       encoder, decoder, optimiser,\n",
        "                                                       n_latent_samples, progress_bar=progress_bar)\n",
        "    if show_plots:\n",
        "        # Visualising losses for this model\n",
        "        plot_losses(val_losses, reference=reference, show=show_plots)\n",
        "\n",
        "    # Using that trained model to impute the data\n",
        "    complete_imputation = impute_from_model(missing_features, encoder, decoder, scaler, imputer,\n",
        "                                            n_latent_samples)\n",
        "\n",
        "    complete_imputation.to_csv(\"imputed_data.csv\")\n",
        "\n",
        "    # Checking distributions for mean collapse\n",
        "    plot_imputed_distributions(missing_features, complete_imputation, reference, show=show_plots)\n",
        "\n",
        "    if \"artificial\" in reference:\n",
        "        imputation_scores = evaluate_artifically_missing(complete_imputation, reference,\n",
        "                                                         missing_data, show=show_plots)\n",
        "    else:\n",
        "        # Restoring the outcome column for prediction\n",
        "        complete_imputation = combine_imputations_with_categorical(complete_imputation,\n",
        "                                                                   missing_data, missing_features)\n",
        "        # Encoding outcome for prediction\n",
        "        complete_imputation[\"outcome_encoded\"] = le.fit_transform(complete_imputation[\"outcome\"])\n",
        "\n",
        "        # Accuracy, Precision, Recall, F-1 and ROC-AUC with means and std.'s\n",
        "        imputation_scores = xgb_grid_search_optimisation(complete_imputation,\n",
        "                                                         search_reference=\"testing\",\n",
        "                                                         save_results=False)\n",
        "\n",
        "    if save_models:\n",
        "        if timestamp is not None:\n",
        "            reference = reference + \"_\" + timestamp\n",
        "\n",
        "        save_and_clear_models(encoder, decoder, reference)\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Deleting after their use\n",
        "    del encoder, decoder, train_losses, val_losses, optimiser, training_data, validation_data, data, complete_imputation\n",
        "    gc.collect()\n",
        "\n",
        "    plt.close(\"all\")\n",
        "\n",
        "    return imputation_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aXFxYvtuxpbM",
      "metadata": {
        "id": "aXFxYvtuxpbM"
      },
      "source": [
        "# Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g91BBN90AcEv",
      "metadata": {
        "id": "g91BBN90AcEv"
      },
      "outputs": [],
      "source": [
        "# Combinations from here will be tested in a bayesian search\n",
        "miwae_search_space = [\n",
        "    Categorical([0.01, 0.001, 0.0001, 0.00001], name=\"lr\"),\n",
        "    Categorical([4, 8, 16, 32], name=\"latent_dimensions\"),\n",
        "    Categorical([5, 10, 20, 30, 40, 50], name=\"n_latent_samples\"),\n",
        "    Categorical([0.05, 0.1, 0.2, 0.3], name=\"dropout\"),\n",
        "    Categorical([1e-5, 1e-4, 1e-3], name=\"l2_reg\"),\n",
        "    Categorical([32, 64, 128, 256], name=\"batch_size\"),\n",
        "    Categorical([5, 4, 3, 2, 1], name=\"encoder_layers_count\"),\n",
        "    Categorical([5, 4, 3, 2, 1], name=\"decoder_layers_count\")\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to save the result of each grid search iteration\n",
        "score_save_path = \"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/results/miwae_{}_individual_scores.csv\""
      ],
      "metadata": {
        "id": "rgAKKfAl3TtP"
      },
      "id": "rgAKKfAl3TtP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tk1IvsUMpyWR",
      "metadata": {
        "id": "Tk1IvsUMpyWR"
      },
      "outputs": [],
      "source": [
        "def generate_layer_sizes(layer_count, model_type=\"encoder\", base_sizes=[128, 64, 32, 16, 8]):\n",
        "    \"\"\"\n",
        "    Given a number of layers this will return the matching layer sizes to be used. i.e. a layer\n",
        "    count of 3 will return (32, 16, 8), with a maximum of 6 going from 256 to 8.\n",
        "\n",
        "    :param layer_count: An integer representing the total number of layers to be returned.\n",
        "    :param model_type: String representing whether the model is a encoder or decoder.\n",
        "    :param base_sizes: The sizes of the layers to make the selection from, the default is [256, 128,\n",
        "                       64, 32, 16, 8]\n",
        "    :return: A list of layer sizes to be used. If decoder it will go from small to large and vice\n",
        "             versa for the discrminator\n",
        "    \"\"\"\n",
        "    n_sizes = len(base_sizes)\n",
        "\n",
        "    if layer_count > n_sizes:\n",
        "        layer_count = n_sizes\n",
        "\n",
        "    layer_sizes = base_sizes[-layer_count:]\n",
        "\n",
        "    if model_type == \"encoder\":\n",
        "        return layer_sizes\n",
        "    elif model_type == \"decoder\":\n",
        "        # Going from large to small (reverse)\n",
        "        return layer_sizes[::-1]\n",
        "    else:\n",
        "        raise ValueError(\"model type must be 'encoder' or 'decoder'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uuLKlpn6CVR3",
      "metadata": {
        "id": "uuLKlpn6CVR3"
      },
      "outputs": [],
      "source": [
        "def check_previously_run(reference, results_dir):\n",
        "    \"\"\"\n",
        "    If bayesian search or testing has been done for a specific dataset the results can be passed\n",
        "    to the optimiser to speed up the process.\n",
        "\n",
        "    :param reference: The reference for the data that is being tested.\n",
        "    :param results_dir: The directory containing the results of specific grid search.\n",
        "    :return: Boolean specifying whether the given test has already been completed.\n",
        "    \"\"\"\n",
        "    previous_runs = pd.read_csv(results_dir)\n",
        "\n",
        "    if \"artificial\" in reference:\n",
        "        m_level, m_type = reference.replace(\"artificial_\", \"\").split(\"_\", 1)\n",
        "    elif \"raw\" in reference:\n",
        "        m_type, m_level = reference.split(\"_\", 1)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid reference provided. Must be either artificial or raw/\")\n",
        "\n",
        "    m_level = float(m_level)\n",
        "\n",
        "    relevant_row = previous_runs[(previous_runs[\"missing_type\"] == m_type) &\n",
        "                                (previous_runs[\"missing_level\"] == m_level)]\n",
        "\n",
        "    if relevant_row.empty:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_scores(scores, reference, timestamp, parameters, save_path):\n",
        "    \"\"\"\n",
        "\n",
        "    :param scores:\n",
        "    :param reference:\n",
        "    :param timestamp:\n",
        "    :param parameters:\n",
        "    :param save_path:\n",
        "    \"\"\"\n",
        "    row = {\n",
        "        \"reference\": reference,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"mean_test_accuracy\": scores[\"mean_test_accuracy\"][0],\n",
        "        \"std_test_accuracy\": scores[\"std_test_accuracy\"][0],\n",
        "        \"mean_test_precision\": scores[\"mean_test_precision\"][0],\n",
        "        \"std_test_precision\": scores[\"std_test_precision\"][0],\n",
        "        \"mean_test_recall\": scores[\"mean_test_recall\"][0],\n",
        "        \"std_test_recall\": scores[\"std_test_recall\"][0],\n",
        "        \"mean_test_f1\": scores[\"mean_test_f1\"][0],\n",
        "        \"std_test_f1\": scores[\"std_test_f1\"][0],\n",
        "        \"mean_test_roc_auc\": scores[\"mean_test_roc_auc\"][0],\n",
        "        \"std_test_roc_auc\": scores[\"std_test_roc_auc\"][0]\n",
        "    }\n",
        "\n",
        "    row.update(parameters)\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        pd.DataFrame([row]).to_csv(save_path, mode=\"a\", header=False, index=False)\n",
        "    else:\n",
        "        pd.DataFrame([row]).to_csv(save_path, mode=\"w\", header=True, index=False)"
      ],
      "metadata": {
        "id": "MlTLrD9L3Zme"
      },
      "id": "MlTLrD9L3Zme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_previous_search_results(save_path, reference):\n",
        "    \"\"\"\n",
        "\n",
        "    :param save_path:\n",
        "    :param reference:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if os.path.exists(save_path):\n",
        "        if \"raw\" in reference:\n",
        "            metric = \"mean_test_roc_auc\"\n",
        "        elif \"artificial\" in reference:\n",
        "            metric = \"average_nmae\"\n",
        "        else:\n",
        "            raise ValueError(\"Invalid reference provided. Must be either artificial or raw\")\n",
        "\n",
        "        previous_runs = pd.read_csv(save_path)\n",
        "\n",
        "        x0 = previous_runs[[\"learning_rate\", \"latent_dimensions\", \"n_latent_samples\", \"dropout\", \"l2_reg\", \"batch_size\", \"encoder_layers_count\", \"decoder_layers_count\"]].values.tolist()\n",
        "        y0 = previous_runs[metric].values.tolist()\n",
        "\n",
        "        # ROC-AUC is saved as a positive, but need to minimise in context of bayesian search so\n",
        "        # making values negative.\n",
        "        if metric == \"mean_test_roc_auc\":\n",
        "            y0 = [-val for val in y0]\n",
        "    else:\n",
        "        x0, y0 = None, None\n",
        "\n",
        "    return x0, y0"
      ],
      "metadata": {
        "id": "FR6hJFRQ3epL"
      },
      "id": "FR6hJFRQ3epL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hLfjoXQ6A7nf",
      "metadata": {
        "id": "hLfjoXQ6A7nf"
      },
      "outputs": [],
      "source": [
        "def run_bayesian_optimisation(features, is_raw_missing, missing_features, missing_data,\n",
        "                              level_missing, reference, grid_search_dir, scaler, imputer):\n",
        "    \"\"\"\n",
        "    Completes a bayesian search with 15 random points and 15 targeted points. This minimises either\n",
        "    the MAE when using ground truth data or the AUC-ROC when using real missing data.\n",
        "\n",
        "    :param features: Dataframe containing missing features that require imputation.\n",
        "    :param is_raw_missing: Boolean flag representing whether using raw missing data or data with\n",
        "                            ground truth available.\n",
        "    :param missing_features: The missing features that require imputation.\n",
        "    :param missing_data: The complete dataset from which the features are selected from.\n",
        "    :param level_missing: Either the percentage or number of values missing per feature/row.\n",
        "    :param reference: String representing the reference for this test.\n",
        "    :param grid_search_dir: String representing the directory to store results in.\n",
        "    :return: The best parameters and score found in the bayesian search.\n",
        "    \"\"\"\n",
        "    already_tested = check_previously_run(reference, grid_search_dir)\n",
        "\n",
        "    if already_tested and not is_raw_missing:\n",
        "        print(\"Already tested: {}, so skipping.\".format(reference))\n",
        "        return\n",
        "\n",
        "    features = features.fillna(features.mean())\n",
        "    save_path = score_save_path.format(reference)\n",
        "\n",
        "    x0, y0 = get_previous_search_results(save_path, reference)\n",
        "    # Default is 40 calls\n",
        "    n_calls = 40\n",
        "    initial_points = 20\n",
        "\n",
        "    if x0 == None:\n",
        "        print(\"No previous results found for {}, so starting new bayesian search\".format(reference))\n",
        "        x0 = [[0.0001, 8, 10, 0.1, 1e-4, 64, 3, 3]]\n",
        "    # There are previous runs available to include in search\n",
        "    else:\n",
        "        # Checking how many calls remain in total and at random\n",
        "        n_calls -= len(x0)\n",
        "        initial_points -= len(x0)\n",
        "\n",
        "        # There are no remaining calls so returning the best found result\n",
        "        if n_calls < 1:\n",
        "            print(\"Completed all calls so returning best results.\")\n",
        "\n",
        "            best_run_id = y0.index(min(y0))\n",
        "            best_params = x0[best_run_id]\n",
        "            best_score = y0[best_run_id]\n",
        "\n",
        "            # Early return, matching format of the minimise function\n",
        "            return SimpleNamespace(x=best_params, fun=best_score)\n",
        "\n",
        "        # Checking if any random calls remain\n",
        "        elif initial_points < 1:\n",
        "            print(\"Completed all random points so now refining.\")\n",
        "            initial_points = 0\n",
        "\n",
        "        print(\"Using previous search results for {}, doing {} calls with {} random\".format(reference, n_calls, initial_points))\n",
        "\n",
        "    # Using scikit optimiser function - objective function is to test models using either\n",
        "    # - nMAE for ground truth data\n",
        "    # - ROC-AUC for raw data\n",
        "    @use_named_args(miwae_search_space)\n",
        "    def objective(lr, latent_dimensions, n_latent_samples, dropout, l2_reg, batch_size,\n",
        "                  encoder_layers_count, decoder_layers_count):\n",
        "        # Setting up the model architecutres\n",
        "        encoder_layers = generate_layer_sizes(encoder_layers_count)\n",
        "        decoder_layers = generate_layer_sizes(decoder_layers_count)\n",
        "\n",
        "        params = {\n",
        "            \"learning_rate\": lr,\n",
        "            \"latent_dimensions\": latent_dimensions,\n",
        "            \"n_latent_samples\": n_latent_samples,\n",
        "            \"dropout\": dropout,\n",
        "            \"l2_reg\": l2_reg,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"encoder_layers_count\": encoder_layers_count,\n",
        "            \"decoder_layers_count\": decoder_layers_count\n",
        "        }\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Fully training a model and evaluating it given passed values\n",
        "        scores = run_training(data=features, missing_features=missing_features,\n",
        "                              missing_data=missing_data, is_raw_missing= is_raw_missing,\n",
        "                              level_missing=level_missing, lr=lr, timestamp=timestamp,\n",
        "                              latent_dimensions=latent_dimensions,\n",
        "                              n_latent_samples=n_latent_samples, dropout=dropout, l2_reg=l2_reg,\n",
        "                              batch_size=batch_size, encoder_layers=encoder_layers,\n",
        "                              decoder_layers=decoder_layers, save_models=True, reference=reference,\n",
        "                              show_plots=False, progress_bar=False, scaler=scaler, imputer=imputer)\n",
        "\n",
        "        if is_raw_missing:\n",
        "            # save scores here to a new file with the timestamp\n",
        "            save_scores(scores, reference, timestamp, params, save_path)\n",
        "            return -scores[\"mean_test_roc_auc\"].iloc[0]\n",
        "        else:\n",
        "            # Returning the average nMAE value\n",
        "            return scores[\"average_norm_mae\"][0]\n",
        "\n",
        "    # Minimising nMAE, 20 searches within specified search space. 20 random searches and then 20\n",
        "    # probability based searches. Can give x0 and y0 as known good configuration to build on\n",
        "    return gp_minimize(objective, dimensions=miwae_search_space, n_calls=n_calls,\n",
        "                       n_initial_points=initial_points, random_state=507, verbose=True, x0=x0,\n",
        "                       y0=y0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IB-moiCexKwK",
      "metadata": {
        "id": "IB-moiCexKwK"
      },
      "source": [
        "# Train and Evaluate Models on Artifically Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v3sYr1DbvlVY",
      "metadata": {
        "id": "v3sYr1DbvlVY"
      },
      "outputs": [],
      "source": [
        "# Used to go through all the combinations of artificially missing data\n",
        "missing_types = [\"mcar\", \"mnar_central\", \"mnar_upper\", \"mnar_lower\"]\n",
        "missing_levels = [\"0.2\", \"0.5\", \"0.7\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ajz4IjnKr4vh",
      "metadata": {
        "id": "ajz4IjnKr4vh"
      },
      "outputs": [],
      "source": [
        "def grid_search_for_artificial_data():\n",
        "    \"\"\"\n",
        "    Completes a grid search for the artificial data by minimising MAE through a bayesian search,\n",
        "    tuning the generator and discriminator hyperparameters.\n",
        "    \"\"\"\n",
        "    artificial_grid_search_dir = (\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/artificial_miwae_gridsearch.csv\")\n",
        "    # Ground truth data\n",
        "    reference_df = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/missing/raw/measurements_0.csv\")\n",
        "    imputer, scaler = fit_scaler_and_imputer(reference_df[numerical_features], temp_imputer,\n",
        "                                             min_max_scaler)\n",
        "    is_raw_missing = False\n",
        "\n",
        "    # DF to store the results\n",
        "    if not os.path.exists(artificial_grid_search_dir):\n",
        "        blank_df = pd.DataFrame(columns=[\"missing_type\", \"missing_level\", \"best_nmae\", \"lr\",\n",
        "                                         \"latent_dim\", \"n_latent_samples\", \"dropout\", \"l2_reg\",\n",
        "                                         \"batch_size\", \"encoder_layers\", \"decoder_layers\"])\n",
        "        blank_df.to_csv(artificial_grid_search_dir, index=False)\n",
        "        # Only required for first appendage\n",
        "        header = True\n",
        "    else:\n",
        "        header = False\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Go through every combination of artificially missing data\n",
        "    for m_type in missing_types:\n",
        "        for m_level in missing_levels:\n",
        "            # Get the data with the specified missing type and level\n",
        "            artificial_df, artificial_df_missing, artificial_reference = get_data_and_reference(\n",
        "                is_raw_missing=is_raw_missing, level_missing=m_level, missing_type=m_type)\n",
        "            # Scaling the data\n",
        "            artificial_scaled_features = scale_data(artificial_df_missing, imputer, scaler)\n",
        "\n",
        "            print(\"Training and testing with {}\".format(artificial_reference))\n",
        "\n",
        "            # Running a bayesian search to optimise\n",
        "            search_result = run_bayesian_optimisation(features=artificial_scaled_features,\n",
        "                                                      missing_features=artificial_df_missing,\n",
        "                                                      missing_data=artificial_df,\n",
        "                                                      is_raw_missing=is_raw_missing,\n",
        "                                                      level_missing=m_level,\n",
        "                                                      reference=artificial_reference,\n",
        "                                                      grid_search_dir=artificial_grid_search_dir,\n",
        "                                                      imputer=imputer, scaler=scaler\n",
        "                                                      )\n",
        "\n",
        "            if search_result is None:\n",
        "                continue\n",
        "\n",
        "            # Getting the optimal parameters found\n",
        "            lr, latent_dim, n_latent_samples, dropout, l2_reg, batch_size, encoder_layers, \\\n",
        "            decoder_layers = search_result.x\n",
        "\n",
        "            # Confirming the results\n",
        "            print(\"Best nMAE {}\".format(search_result.fun))\n",
        "            print(\"Best parameters {}\".format(search_result.x))\n",
        "\n",
        "            # Saving results to csv, appending so if runtime expires a recovery is possible\n",
        "            result_dict = {\"missing_type\": m_type, \"missing_level\": m_level,\n",
        "                           \"best_nmae\": search_result.fun, \"lr\": lr, \"latent_dim\": latent_dim,\n",
        "                           \"n_latent_samples\": n_latent_samples, \"dropout\": dropout,\n",
        "                           \"l2_reg\": l2_reg, \"batch_size\": batch_size,\n",
        "                           \"encoder_layers\": encoder_layers, \"decoder_layers\": decoder_layers\n",
        "                           }\n",
        "            result_df = pd.DataFrame([result_dict])\n",
        "            result_df.to_csv(artificial_grid_search_dir, mode='a', header=header, index=False)\n",
        "\n",
        "            header=False\n",
        "\n",
        "    print(\"Finished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-JpSpoQ1GZv5",
      "metadata": {
        "id": "-JpSpoQ1GZv5"
      },
      "source": [
        "#### Checking the results on artificial data and finalising the imputations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2lbAnbc6GhQA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "2lbAnbc6GhQA",
        "outputId": "d5df4165-f35f-4526-ec1a-41ac23d41997"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    missing_type  missing_level  best_nmae       lr  latent_dim  \\\n",
              "0           mcar            0.2   0.706812  0.00001           8   \n",
              "1           mcar            0.5   0.693397  0.10000           4   \n",
              "2           mcar            0.2   0.706812  0.00001           8   \n",
              "3           mcar            0.7   0.706394  0.00010           2   \n",
              "4   mnar_central            0.2   0.708636  0.01000           4   \n",
              "5   mnar_central            0.5   0.736478  0.00001           8   \n",
              "6   mnar_central            0.7   0.983048  0.00001          16   \n",
              "7     mnar_upper            0.2   0.691473  0.01000           8   \n",
              "8     mnar_upper            0.5   0.689939  0.00100          32   \n",
              "9     mnar_upper            0.7   0.703584  0.00100           8   \n",
              "10    mnar_lower            0.2   0.703600  0.01000           8   \n",
              "11    mnar_lower            0.5   0.686348  0.00010          16   \n",
              "\n",
              "    n_latent_samples  dropout   l2_reg  batch_size  encoder_layers  \\\n",
              "0                 10     0.10  0.00001          64               5   \n",
              "1                 10     0.20  0.00001         512               5   \n",
              "2                 10     0.10  0.00001          64               5   \n",
              "3                 10     0.20  0.00001          32               1   \n",
              "4                  1     0.30  0.00100         256               3   \n",
              "5                 50     0.10  0.00001          64               3   \n",
              "6                 20     0.05  0.00010         256               3   \n",
              "7                  1     0.05  0.01000         512               1   \n",
              "8                  5     0.30  0.00010          16               3   \n",
              "9                  1     0.30  0.00100          64               4   \n",
              "10                 1     0.05  0.01000         512               1   \n",
              "11                20     0.10  0.00010          64               5   \n",
              "\n",
              "    decoder_layers  \n",
              "0                2  \n",
              "1                4  \n",
              "2                2  \n",
              "3                4  \n",
              "4                4  \n",
              "5                2  \n",
              "6                3  \n",
              "7                3  \n",
              "8                1  \n",
              "9                2  \n",
              "10               3  \n",
              "11               1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a8453e4-ef16-4d79-b96f-bfc004670b0b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>missing_type</th>\n",
              "      <th>missing_level</th>\n",
              "      <th>best_nmae</th>\n",
              "      <th>lr</th>\n",
              "      <th>latent_dim</th>\n",
              "      <th>n_latent_samples</th>\n",
              "      <th>dropout</th>\n",
              "      <th>l2_reg</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>encoder_layers</th>\n",
              "      <th>decoder_layers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mcar</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.706812</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mcar</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.693397</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>512</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mcar</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.706812</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mcar</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.706394</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mnar_central</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.708636</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>mnar_central</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.736478</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>8</td>\n",
              "      <td>50</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mnar_central</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.983048</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>16</td>\n",
              "      <td>20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>256</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mnar_upper</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.691473</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>512</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>mnar_upper</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.689939</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>32</td>\n",
              "      <td>5</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>mnar_upper</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.703584</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>mnar_lower</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.703600</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>512</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>mnar_lower</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.686348</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>16</td>\n",
              "      <td>20</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a8453e4-ef16-4d79-b96f-bfc004670b0b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a8453e4-ef16-4d79-b96f-bfc004670b0b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a8453e4-ef16-4d79-b96f-bfc004670b0b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b11b48df-f5b7-4f8d-b550-1898e7482207\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b11b48df-f5b7-4f8d-b550-1898e7482207')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b11b48df-f5b7-4f8d-b550-1898e7482207 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "artificial_scores",
              "summary": "{\n  \"name\": \"artificial_scores\",\n  \"rows\": 13,\n  \"fields\": [\n    {\n      \"column\": \"missing_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"mnar_central\",\n          \"mnar_lower\",\n          \"mcar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"missing_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2183856856396675,\n        \"min\": 0.2,\n        \"max\": 0.7,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.2,\n          0.5,\n          0.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_nmae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07886903262249284,\n        \"min\": 0.6863479437070349,\n        \"max\": 0.983048228652872,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.6863479437070349,\n          0.7036,\n          0.706811955\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02729073849664585,\n        \"min\": 1e-05,\n        \"max\": 0.1,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.1,\n          0.001,\n          0.0001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latent_dim\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 2,\n        \"max\": 32,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          32,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_latent_samples\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 1,\n        \"max\": 50,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          10,\n          1,\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10095695960312835,\n        \"min\": 0.05,\n        \"max\": 0.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.2,\n          0.05,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"l2_reg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003688620755366845,\n        \"min\": 1e-05,\n        \"max\": 0.01,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.001,\n          0.01,\n          1e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 194,\n        \"min\": 16,\n        \"max\": 512,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          512,\n          16,\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"encoder_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          4,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"decoder_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "artificial_scores = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/artificial_miwae_gridsearch.csv\")\n",
        "artificial_scores.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uW-_Y-NBbz1H",
      "metadata": {
        "id": "uW-_Y-NBbz1H"
      },
      "outputs": [],
      "source": [
        "def generate_best_imputations(original_tests, test_type=\"artificial\"):\n",
        "    \"\"\"\n",
        "    Function to re-run the best found models with the complete MAE for all the features instead\n",
        "    of purely the average.\n",
        "\n",
        "    :param test_type: String representing the main cateogry of missing data (artificial or raw)\n",
        "    \"\"\"\n",
        "    if test_type == \"artificial\":\n",
        "        is_raw_missing = False\n",
        "        best_scores_dir = (\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/results/artificial_miwae_scores.csv\")\n",
        "        df_columns = [\"reference\", \"mean arterial pressure\",\n",
        "                                         \"heart rate\", \"respiratory rate\", \"PCO2 (Arterial)\",\n",
        "                                         \"PO2 (Arterial)\", \"FiO2\", \"arterial pH\", \"sodium\",\n",
        "                                         \"postassium\", \"creatinine\", \"hematocrit\",\n",
        "                                         \"white blood cell\", \"HCO3 (serum)\", \"average_norm_mae\"]\n",
        "    elif test_type == \"raw\":\n",
        "        is_raw_missing = True\n",
        "        best_scores_dir = (\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/results/raw_miwae_scores.csv\")\n",
        "        df_columns = [\"missing_type\", \"missing_level\", \"mean_test_accuracy\", \"std_test_accuracy\",\n",
        "                      \"mean_test_precision\", \"std_test_precision\", \"mean_test_recall\",\n",
        "                      \"std_test_recall\", \"mean_test_f1\", \"std_test_f1\", \"mean_test_roc_auc\",\n",
        "                      \"std_test_roc_auc\"]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid score type\")\n",
        "\n",
        "    all_tests = original_tests.groupby([\"missing_type\", \"missing_level\"])\n",
        "\n",
        "    # Check if the results file exists\n",
        "    if not os.path.exists(best_scores_dir):\n",
        "        blank_df = pd.DataFrame(columns=df_columns)\n",
        "        blank_df.to_csv(best_scores_dir, index=False)\n",
        "        # Only required for first appendage\n",
        "        header = True\n",
        "    else:\n",
        "        header = False\n",
        "\n",
        "    # Going through each of the grid search results\n",
        "    for (missing_type, missing_level), group in all_tests:\n",
        "        # Preparing data and reference for imputation\n",
        "        shuffled_data, shuffled_features, data_reference = get_data_and_reference(is_raw_missing,\n",
        "                                                                                  level_missing=missing_level,\n",
        "                                                                                  missing_type=missing_type)\n",
        "        tested_combination = check_previously_run(data_reference, best_scores_dir)\n",
        "\n",
        "        if tested_combination:\n",
        "            print(\"Already tested {}, so skipping.\".format(data_reference))\n",
        "            continue\n",
        "        else:\n",
        "            print(\"Testing, {}\".format(data_reference))\n",
        "\n",
        "        imputer, scaler = fit_scaler_and_imputer(shuffled_data[numerical_features], temp_imputer,\n",
        "                                                 min_max_scaler)\n",
        "\n",
        "        scaled_features = scale_data(shuffled_features, imputer, scaler)\n",
        "\n",
        "        # Extracting the best combination of variables\n",
        "        lr = group[\"lr\"].iloc[0]\n",
        "        latent_dimensions = group[\"latent_dim\"].iloc[0]\n",
        "        n_latent_samples = group[\"n_latent_samples\"].iloc[0]\n",
        "        dropout = group[\"dropout\"].iloc[0]\n",
        "        l2_reg = group[\"l2_reg\"].iloc[0]\n",
        "        batch_size = group[\"batch_size\"].iloc[0]\n",
        "        encoder_layers_count = group[\"encoder_layers\"].iloc[0]\n",
        "        decoder_layers_count = group[\"decoder_layers\"].iloc[0]\n",
        "\n",
        "        # Getting the layer sizes for the models\n",
        "        encoder_layers = generate_layer_sizes(encoder_layers_count)\n",
        "        decoder_layers = generate_layer_sizes(decoder_layers_count)\n",
        "\n",
        "        # Training and evaluating the quality of the imputation for the provided model\n",
        "        scores = run_training(data=scaled_features, missing_features=shuffled_features,\n",
        "                              missing_data=shuffled_data, is_raw_missing=is_raw_missing,\n",
        "                              level_missing=missing_level, lr=lr,\n",
        "                              latent_dimensions=latent_dimensions,\n",
        "                              n_latent_samples=n_latent_samples, dropout=dropout, l2_reg=l2_reg,\n",
        "                              batch_size=batch_size, encoder_layers=encoder_layers,\n",
        "                              decoder_layers=decoder_layers, save_models=True,\n",
        "                              reference=data_reference, show_plots=False, progress_bar=False,\n",
        "                              scaler=scaler, imputer=imputer)\n",
        "\n",
        "        if test_type == \"artificial\":\n",
        "            # Preparing scores and saving to shared file\n",
        "            score_df = pd.DataFrame({\n",
        "                \"missing_type\": [missing_type],\n",
        "                \"missing_level\": [missing_level],\n",
        "                \"mean arterial pressure\": [scores[\"mean arterial pressure\"][0]],\n",
        "                \"heart rate\": [scores[\"heart rate\"][0]],\n",
        "                \"respiratory rate\": [scores[\"respiratory rate\"][0]],\n",
        "                \"PCO2 (Arterial)\": [scores[\"PCO2 (Arterial)\"][0]],\n",
        "                \"PO2 (Arterial)\": [scores[\"PO2 (Arterial)\"][0]],\n",
        "                \"FiO2\": [scores[\"FiO2\"][0]],\n",
        "                \"arterial pH\": [scores[\"arterial pH\"][0]],\n",
        "                \"sodium\": [scores[\"sodium\"][0]],\n",
        "                \"postassium\": [scores[\"postassium\"][0]],\n",
        "                \"creatinine\": [scores[\"creatinine\"][0]],\n",
        "                \"hematocrit\": [scores[\"hematocrit\"][0]],\n",
        "                \"white blood cell\": [scores[\"white blood cell\"][0]],\n",
        "                \"HCO3 (serum)\": [scores[\"HCO3 (serum)\"][0]],\n",
        "                \"average_norm_mae\": [scores[\"average_norm_mae\"][0]]\n",
        "            })\n",
        "        else:\n",
        "            score_df = pd.DataFrame({\n",
        "                \"missing_type\": [missing_type],\n",
        "                \"missing_level\": [missing_level],\n",
        "                \"mean_test_accuracy\": [scores[\"mean_test_accuracy\"][0]],\n",
        "                \"std_test_accuracy\": [scores[\"std_test_accuracy\"][0]],\n",
        "                \"mean_test_precision\": [scores[\"mean_test_precision\"][0]],\n",
        "                \"std_test_precision\": [scores[\"std_test_precision\"][0]],\n",
        "                \"mean_test_recall\": [scores[\"mean_test_recall\"][0]],\n",
        "                \"std_test_recall\": [scores[\"std_test_recall\"][0]],\n",
        "                \"mean_test_f1\": [scores[\"mean_test_f1\"][0]],\n",
        "                \"std_test_f1\": [scores[\"std_test_f1\"][0]],\n",
        "                \"mean_test_roc_auc\": [scores[\"mean_test_roc_auc\"][0]],\n",
        "                \"std_test_roc_auc\": [scores[\"std_test_roc_auc\"][0]]\n",
        "            })\n",
        "\n",
        "        score_df.to_csv(best_scores_dir, mode='a', header=header, index=False)\n",
        "        header = False\n",
        "\n",
        "    complete_scores = pd.read_csv(best_scores_dir)\n",
        "    complete_scores.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dIbpAxmMc32n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIbpAxmMc32n",
        "outputId": "463e0cc9-95c6-4248-93ca-a14c32f244dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already tested artificial_0.2_mcar, so skipping.\n",
            "Already tested artificial_0.5_mcar, so skipping.\n",
            "Already tested artificial_0.7_mcar, so skipping.\n",
            "Already tested artificial_0.2_mnar_central, so skipping.\n",
            "Already tested artificial_0.5_mnar_central, so skipping.\n",
            "Already tested artificial_0.7_mnar_central, so skipping.\n",
            "Already tested artificial_0.2_mnar_lower, so skipping.\n",
            "Already tested artificial_0.5_mnar_lower, so skipping.\n",
            "Already tested artificial_0.7_mnar_lower, so skipping.\n",
            "Already tested artificial_0.2_mnar_upper, so skipping.\n",
            "Already tested artificial_0.5_mnar_upper, so skipping.\n",
            "Already tested artificial_0.7_mnar_upper, so skipping.\n"
          ]
        }
      ],
      "source": [
        "generate_best_imputations(artificial_scores, test_type=\"artificial\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84kucDyLCHia",
      "metadata": {
        "id": "84kucDyLCHia"
      },
      "source": [
        "# Train Models on All Levels of Real Missing Data\n",
        "- evaluation takes place in main code, only getting the imputed datasets to test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4U0wLdnOCS7o",
      "metadata": {
        "id": "4U0wLdnOCS7o"
      },
      "outputs": [],
      "source": [
        "missing_levels = [2, 5, 10]\n",
        "is_raw_missing = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D5I35r7BCSUQ",
      "metadata": {
        "id": "D5I35r7BCSUQ"
      },
      "outputs": [],
      "source": [
        "def grid_search_real_missing():\n",
        "    \"\"\"\n",
        "    Completes a grid search for the artificial data by maximising ROC-AUC through a bayesian search,\n",
        "    tuning the encoder and decoder hyperparameters.\n",
        "    \"\"\"\n",
        "    real_grid_search_dir = (\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/real_miwae_gridsearch.csv\")\n",
        "    is_raw_missing = True\n",
        "\n",
        "    # Number of variables to impute\n",
        "    n_features = len(numerical_features)\n",
        "\n",
        "    # DF to store the results\n",
        "    if not os.path.exists(real_grid_search_dir):\n",
        "        blank_df = pd.DataFrame(columns=[\"missing_type\", \"missing_level\", \"best_roc_auc\", \"lr\",\n",
        "                                         \"latent_dim\", \"n_latent_samples\", \"dropout\", \"l2_reg\",\n",
        "                                         \"batch_size\", \"encoder_layers\", \"decoder_layers\"])\n",
        "        blank_df.to_csv(real_grid_search_dir, index=False)\n",
        "        # Only required for first appendage\n",
        "        header = True\n",
        "    else:\n",
        "        header = False\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for m_level in missing_levels:\n",
        "        # Get the data with the specified missing type and level\n",
        "        real_df, real_df_missing, real_reference = get_data_and_reference(is_raw_missing=is_raw_missing,\n",
        "                                                                          level_missing=m_level,\n",
        "                                                                          missing_type=None)\n",
        "\n",
        "        print(\"Testing, {}\".format(real_reference))\n",
        "\n",
        "        # Scaling the data\n",
        "        imputer, scaler = fit_scaler_and_imputer(real_df[numerical_features], temp_imputer,\n",
        "                                                 min_max_scaler)\n",
        "        real_scaled_features = scale_data(real_df_missing, imputer, scaler)\n",
        "\n",
        "        # Running a bayesian search to optimise\n",
        "        search_result = run_bayesian_optimisation(features=real_scaled_features,\n",
        "                                                    missing_features=real_df_missing,\n",
        "                                                    missing_data=real_df,\n",
        "                                                    is_raw_missing=True,\n",
        "                                                    level_missing=m_level,\n",
        "                                                    reference=real_reference,\n",
        "                                                    grid_search_dir=real_grid_search_dir,\n",
        "                                                    scaler=scaler, imputer=imputer\n",
        "                                                    )\n",
        "\n",
        "        if search_result is None:\n",
        "            continue\n",
        "\n",
        "        # Getting the optimal parameters found\n",
        "        lr, latent_dim, n_latent_samples, dropout, l2_reg, batch_size, encoder_layers, \\\n",
        "         decoder_layers = search_result.x\n",
        "\n",
        "        # Confirming the results\n",
        "        print(\"Best ROC-AUC {}\".format(search_result.fun))\n",
        "        print(\"Best parameters {}\".format(search_result.x))\n",
        "\n",
        "        # Saving results to csv, appending so if runtime expires a recovery is possible\n",
        "        result_dict = {\"missing_type\": \"raw\", \"missing_level\": m_level,\n",
        "                        \"best_roc_auc\": search_result.fun, \"lr\": lr, \"latent_dim\": latent_dim,\n",
        "                        \"n_latent_samples\": n_latent_samples, \"dropout\": dropout, \"l2_reg\": l2_reg,\n",
        "                        \"batch_size\": batch_size, \"encoder_layers\": encoder_layers,\n",
        "                        \"decoder_layers\": decoder_layers\n",
        "                        }\n",
        "        result_df = pd.DataFrame([result_dict])\n",
        "        result_df.to_csv(real_grid_search_dir, mode='a', header=header, index=False)\n",
        "\n",
        "        header=False\n",
        "\n",
        "    print(\"Finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IcmYzxZnDodf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcmYzxZnDodf",
        "outputId": "84bce237-7bc8-49d0-d48c-c158d3d99e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing, raw_2\n",
            "Already tested: raw_2, so skipping.\n",
            "None\n",
            "Testing, raw_5\n",
            "Already tested: raw_5, so skipping.\n",
            "None\n",
            "Testing, raw_10\n",
            "Already tested: raw_10, so skipping.\n",
            "None\n",
            "Finished\n"
          ]
        }
      ],
      "source": [
        "# Training the models to work on the real data with different levels of missingness\n",
        "grid_search_real_missing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J6tY5J8w-Med",
      "metadata": {
        "id": "J6tY5J8w-Med",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "b98d85e8-dade-4024-ab6d-ba1011a135a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  missing_type  missing_level  best_roc_auc    lr  latent_dim  \\\n",
              "0          raw              2     -0.846831  0.01         4.0   \n",
              "1          raw              5     -0.846080  0.01        16.0   \n",
              "2          raw             10     -0.850313  0.01         8.0   \n",
              "\n",
              "   n_latent_samples  dropout  l2_reg  batch_size  encoder_layers  \\\n",
              "0              40.0     0.05  0.0010        32.0             1.0   \n",
              "1              40.0     0.05  0.0001       256.0             4.0   \n",
              "2              50.0     0.30  0.0010        32.0             2.0   \n",
              "\n",
              "   decoder_layers  \n",
              "0             2.0  \n",
              "1             1.0  \n",
              "2             5.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bb00409-171d-4d14-8a2c-b11e81716b38\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>missing_type</th>\n",
              "      <th>missing_level</th>\n",
              "      <th>best_roc_auc</th>\n",
              "      <th>lr</th>\n",
              "      <th>latent_dim</th>\n",
              "      <th>n_latent_samples</th>\n",
              "      <th>dropout</th>\n",
              "      <th>l2_reg</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>encoder_layers</th>\n",
              "      <th>decoder_layers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>raw</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.846831</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>raw</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.846080</td>\n",
              "      <td>0.01</td>\n",
              "      <td>16.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>256.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>raw</td>\n",
              "      <td>10</td>\n",
              "      <td>-0.850313</td>\n",
              "      <td>0.01</td>\n",
              "      <td>8.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bb00409-171d-4d14-8a2c-b11e81716b38')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2bb00409-171d-4d14-8a2c-b11e81716b38 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2bb00409-171d-4d14-8a2c-b11e81716b38');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e6ebffbd-75cd-4a15-9959-96f844726cf9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6ebffbd-75cd-4a15-9959-96f844726cf9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e6ebffbd-75cd-4a15-9959-96f844726cf9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "real_scores",
              "summary": "{\n  \"name\": \"real_scores\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"missing_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"raw\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"missing_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 2,\n        \"max\": 10,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_roc_auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0022583915147094645,\n        \"min\": -0.8503127895120459,\n        \"max\": -0.8460802887061426,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.8468307245546411\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.01,\n        \"max\": 0.01,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.01\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latent_dim\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.110100926607787,\n        \"min\": 4.0,\n        \"max\": 16.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_latent_samples\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.773502691896258,\n        \"min\": 40.0,\n        \"max\": 50.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          50.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14433756729740643,\n        \"min\": 0.05,\n        \"max\": 0.3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"l2_reg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005196152422706631,\n        \"min\": 0.0001,\n        \"max\": 0.001,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 129.32646029847618,\n        \"min\": 32.0,\n        \"max\": 256.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          256.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"encoder_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5275252316519468,\n        \"min\": 1.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"decoder_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.081665999466133,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "real_scores = pd.read_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/real_miwae_gridsearch.csv\")\n",
        "real_scores.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qHyfg7oKZQa_",
      "metadata": {
        "id": "qHyfg7oKZQa_"
      },
      "outputs": [],
      "source": [
        "def extract_best_scores(test_type=\"raw\"):\n",
        "    \"\"\"\n",
        "    Used to extract the best ROC-AUC scores from the complete grid search. A new csv is created\n",
        "    containing just the best runs with their hyperparameters and either complete ground truth scores\n",
        "    or the downstream scores.\n",
        "\n",
        "    This is used instead of generate best imputations as it is less intensive and more telling.\n",
        "\n",
        "    :param test_type: String representing the main cateogry of missing data (artificial or raw).\n",
        "    \"\"\"\n",
        "    if test_type == \"raw\":\n",
        "        # Only 3 raw files and tested in downstream.\n",
        "        file_references = [\"raw_2\", \"raw_5\", \"raw_10\"]\n",
        "        metric = \"mean_test_roc_auc\"\n",
        "    elif test_type == \"artificial\":\n",
        "        # 12 combinations of missing types and different missing levels\n",
        "        missing_types = [\"mcar\", \"mnar_central\", \"mnar_upper\", \"mnar_lower\"]\n",
        "        missing_levels = [\"0.2\", \"0.5\", \"0.7\"]\n",
        "        file_references = []\n",
        "\n",
        "        for missing_type in missing_types:\n",
        "            for missing_level in missing_levels:\n",
        "                file_references.append(\"{}_{}\".format(missing_type, missing_level))\n",
        "\n",
        "        # Tested in their ground truth reconstruction\n",
        "        metric = \"average_norm_mae\"\n",
        "    else:\n",
        "        raise ValueError(\"Invalid score type.\")\n",
        "    best_results = []\n",
        "\n",
        "    # Go through each  of the results files\n",
        "    for file in file_references:\n",
        "        file_dir = \"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/results/miwae_{}_individual_scores.csv\".format(file)\n",
        "\n",
        "        complete_scores = pd.read_csv(file_dir)\n",
        "        complete_scores = complete_scores.drop(columns=[\"timestamp\"])\n",
        "\n",
        "        # Extract result with the best result, maximising ROC-AUC and minimising nMAE\n",
        "        if test_type == \"raw\":\n",
        "           best_result = complete_scores.iloc[complete_scores[metric].idxmax()]\n",
        "        else:\n",
        "            best_result = complete_scores.iloc[complete_scores[metric].idxmin()]\n",
        "        # Save the best result to list\n",
        "        best_results.append(best_result)\n",
        "\n",
        "    # Convert best results to a dataframe and save\n",
        "    best_results_df = pd.DataFrame(best_results)\n",
        "    best_results_df.to_csv(\"/content/drive/MyDrive/Sheffield/6000 Dissertation/Imputing Health Care Data/Data/results/best_{}_miwae_scores.csv\".format(test_type), index=False)\n",
        "\n",
        "    return best_results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uGn8i07GaFKv",
      "metadata": {
        "id": "uGn8i07GaFKv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "02761a00-4281-4209-8c63-962fbdb1bb41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   reference  mean_test_accuracy  std_test_accuracy  mean_test_precision  \\\n",
              "29     raw_2            0.878350           0.003003             0.889660   \n",
              "34     raw_5            0.921607           0.002127             0.929915   \n",
              "17    raw_10            0.920271           0.001866             0.928430   \n",
              "\n",
              "    std_test_precision  mean_test_recall  std_test_recall  mean_test_f1  \\\n",
              "29            0.004150          0.978017         0.003676      0.931734   \n",
              "34            0.001612          0.988364         0.001091      0.958249   \n",
              "17            0.001380          0.988650         0.000964      0.957593   \n",
              "\n",
              "    std_test_f1  mean_test_roc_auc  std_test_roc_auc  learning_rate  \\\n",
              "29     0.001558           0.846831          0.008465           0.01   \n",
              "34     0.001114           0.846080          0.005431           0.01   \n",
              "17     0.000977           0.850313          0.005329           0.01   \n",
              "\n",
              "    latent_dimensions  n_latent_samples  dropout  l2_reg  batch_size  \\\n",
              "29                  4                40     0.05  0.0010          32   \n",
              "34                 16                40     0.05  0.0001         256   \n",
              "17                  8                50     0.30  0.0010          32   \n",
              "\n",
              "    encoder_layers_count  decoder_layers_count  \n",
              "29                     1                     2  \n",
              "34                     4                     1  \n",
              "17                     2                     5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29a3e54c-d574-432e-be7a-1140855c1111\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reference</th>\n",
              "      <th>mean_test_accuracy</th>\n",
              "      <th>std_test_accuracy</th>\n",
              "      <th>mean_test_precision</th>\n",
              "      <th>std_test_precision</th>\n",
              "      <th>mean_test_recall</th>\n",
              "      <th>std_test_recall</th>\n",
              "      <th>mean_test_f1</th>\n",
              "      <th>std_test_f1</th>\n",
              "      <th>mean_test_roc_auc</th>\n",
              "      <th>std_test_roc_auc</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>latent_dimensions</th>\n",
              "      <th>n_latent_samples</th>\n",
              "      <th>dropout</th>\n",
              "      <th>l2_reg</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>encoder_layers_count</th>\n",
              "      <th>decoder_layers_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>raw_2</td>\n",
              "      <td>0.878350</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.889660</td>\n",
              "      <td>0.004150</td>\n",
              "      <td>0.978017</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.931734</td>\n",
              "      <td>0.001558</td>\n",
              "      <td>0.846831</td>\n",
              "      <td>0.008465</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>raw_5</td>\n",
              "      <td>0.921607</td>\n",
              "      <td>0.002127</td>\n",
              "      <td>0.929915</td>\n",
              "      <td>0.001612</td>\n",
              "      <td>0.988364</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.958249</td>\n",
              "      <td>0.001114</td>\n",
              "      <td>0.846080</td>\n",
              "      <td>0.005431</td>\n",
              "      <td>0.01</td>\n",
              "      <td>16</td>\n",
              "      <td>40</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>256</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>raw_10</td>\n",
              "      <td>0.920271</td>\n",
              "      <td>0.001866</td>\n",
              "      <td>0.928430</td>\n",
              "      <td>0.001380</td>\n",
              "      <td>0.988650</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.957593</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.850313</td>\n",
              "      <td>0.005329</td>\n",
              "      <td>0.01</td>\n",
              "      <td>8</td>\n",
              "      <td>50</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29a3e54c-d574-432e-be7a-1140855c1111')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29a3e54c-d574-432e-be7a-1140855c1111 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29a3e54c-d574-432e-be7a-1140855c1111');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-582946f1-783e-4362-8fdd-ba1d5bfb5943\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-582946f1-783e-4362-8fdd-ba1d5bfb5943')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-582946f1-783e-4362-8fdd-ba1d5bfb5943 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"extract_best_scores()\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"raw_2\",\n          \"raw_5\",\n          \"raw_10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_test_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.024597861739847755,\n        \"min\": 0.8783500272549553,\n        \"max\": 0.9216072732277878,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.8783500272549553,\n          0.9216072732277878,\n          0.9202708263112857\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_test_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005956532661052312,\n        \"min\": 0.0018655711407086,\n        \"max\": 0.0030027084467918,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0030027084467918,\n          0.0021265727269162,\n          0.0018655711407086\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_test_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.022824599268481533,\n        \"min\": 0.8896601509837501,\n        \"max\": 0.92991530798782,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.8896601509837501,\n          0.92991530798782,\n          0.9284298523861836\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_test_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0015362191091996796,\n        \"min\": 0.0013804382990396,\n        \"max\": 0.0041495862164817,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0041495862164817,\n          0.0016122883910409,\n          0.0013804382990396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_test_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006057908713976018,\n        \"min\": 0.9780172413793105,\n        \"max\": 0.9886497064579256,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9780172413793105,\n          0.9883641586358928,\n          0.9886497064579256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_test_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0015306288694002393,\n        \"min\": 0.00096397204655,\n        \"max\": 0.0036764487304379,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0036764487304379,\n          0.0010912568144938,\n          0.00096397204655\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_test_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.015122372789357987,\n        \"min\": 0.9317344006496052,\n        \"max\": 0.9582485251121238,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9317344006496052,\n          0.9582485251121238,\n          0.957593422225764\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_test_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00030336698872593826,\n        \"min\": 0.0009772064356235,\n        \"max\": 0.0015575658613755,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0015575658613755,\n          0.001114148449967,\n          0.0009772064356235\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_test_roc_auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0022583915147094645,\n        \"min\": 0.8460802887061426,\n        \"max\": 0.8503127895120459,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.8468307245546411,\n          0.8460802887061426,\n          0.8503127895120459\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_test_roc_auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0017821431240375263,\n        \"min\": 0.0053287337040189,\n        \"max\": 0.0084654827613764,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0084654827613764,\n          0.005431261597801,\n          0.0053287337040189\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.01,\n        \"max\": 0.01,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.01\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latent_dimensions\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 4,\n        \"max\": 16,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_latent_samples\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 40,\n        \"max\": 50,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          50\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14433756729740643,\n        \"min\": 0.05,\n        \"max\": 0.3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"l2_reg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005196152422706631,\n        \"min\": 0.0001,\n        \"max\": 0.001,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 129,\n        \"min\": 32,\n        \"max\": 256,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"encoder_layers_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"decoder_layers_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "extract_best_scores()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}